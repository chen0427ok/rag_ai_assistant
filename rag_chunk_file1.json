{
    "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf": {
        "chunks": [
            "Published as a conference paper at ICLR 2021\nNEURAL PRUNING VIA GROWING REGULARIZATION\nHuan Wang, Can Qin, Yulun Zhang, Yun Fu\nNortheastern University, Boston, MA, USA\n{wang.huan, qin.ca}@northeastern.edu,\nyulun100@gmail.com, yunfu@ece.neu.edu\nABSTRACT\nRegularization has long been utilized to learn sparsity in deep neural network\npruning. However, its role is mainly explored in the small penalty strength regime.\nIn this work, we extend its application to a new scenario where the regularization\ngrows large gradually to tackle two central problems of pruning: pruning sched-\nule and weight importance scoring. (1) The former topic is newly brought up in\nthis work, which we ﬁnd critical to the pruning performance while receives lit-\ntle research attention. Speciﬁcally, we propose an L2 regularization variant with\nrising penalty factors and show it can bring signiﬁcant accuracy gains compared\nwith its one-shot counterpart, even when the same weights are removed. (2) The",
            "rising penalty factors and show it can bring signiﬁcant accuracy gains compared\nwith its one-shot counterpart, even when the same weights are removed. (2) The\ngrowing penalty scheme also brings us an approach to exploit the Hessian in-\nformation for more accurate pruning without knowing their speciﬁc values, thus\nnot bothered by the common Hessian approximation problems. Empirically, the\nproposed algorithms are easy to implement and scalable to large datasets and net-\nworks in both structured and unstructured pruning. Their effectiveness is demon-\nstrated with modern deep neural networks on the CIFAR and ImageNet datasets,\nachieving competitive results compared to many state-of-the-art algorithms. Our\ncode and trained models are publicly available at https://github.com/mingsun-\ntse/regularization-pruning.\n1\nINTRODUCTION\nAs deep neural networks advance in recent years LeCun et al. (2015); Schmidhuber (2015), their",
            "tse/regularization-pruning.\n1\nINTRODUCTION\nAs deep neural networks advance in recent years LeCun et al. (2015); Schmidhuber (2015), their\nremarkable effectiveness comes at a cost of rising storage, memory footprint, computing resources\nand energy consumption Cheng et al. (2017); Deng et al. (2020). Neural network pruning Han et al.\n(2015; 2016); Li et al. (2017); Wen et al. (2016); He et al. (2017); Gale et al. (2019) is deemed as a\npromising force to alleviate this problem. Since its early debut Mozer & Smolensky (1989); Reed\n(1993), the central problem of neural network pruning has been (arguably) how to choose weights\nto discard, i.e., the weight importance scoring problem LeCun et al. (1990); Hassibi & Stork (1993);\nMolchanov et al. (2017b; 2019); Wang et al. (2019a); He et al. (2020).\nThe approaches to the scoring problem generally fall into two groups: importance-based and\nregularization-based Reed (1993). The former focuses on directly proposing certain theoretically",
            "The approaches to the scoring problem generally fall into two groups: importance-based and\nregularization-based Reed (1993). The former focuses on directly proposing certain theoretically\nsound importance criterion so that we can prune the unimportant weights once for all. Thus, the\npruning process is typically one-shot. In contrast, regularization-based approaches typically select\nunimportant weights through training with a penalty term Han et al. (2015); Wen et al. (2016); Liu\net al. (2017). However, the penalty strength is usually maintained in a small regime to avoid dam-\naging the model expressivity. Whereas, a large penalty strength can be helpful, speciﬁcally in two\naspects. (1) A large penalty can push unimportant weights rather close to zero, then the pruning\nlater barely hurts the performance even if the simple weight magnitude is adopted as criterion. (2)\nIt is well-known that different weights of a neural network lie on the regions with different local",
            "later barely hurts the performance even if the simple weight magnitude is adopted as criterion. (2)\nIt is well-known that different weights of a neural network lie on the regions with different local\nquadratic structures, i.e., Hessian information. Many methods try to tap into this to build a more\naccurate scoring LeCun et al. (1990); Hassibi & Stork (1993); Wang et al. (2019a); Singh & Al-\nistarh (2020). However, for deep networks, it is especially hard to estimate Hessian. Sometimes,\neven the computing itself can be intractable without resorting to proper approximation Wang et al.\n(2019a). On this problem, we ask: Is it possible to exploit the Hessian information without knowing\ntheir speciﬁc values? This is the second scenario where a growing regularization can help. We will\n1\narXiv:2012.09243v2  [cs.CV]  5 Apr 2021",
            "Published as a conference paper at ICLR 2021\nshow under a growing regularization, the weight magnitude will naturally separate because of their\ndifferent underlying local quadratic structure, therein we can pick the unimportant weights more\nfaithfully even using the simple magnitude-based criterion. Corresponding to these two aspects,\nwe will present two algorithms based on a growing L2 regularization paradigm, in which the ﬁrst\nhighlights a better pruning schedule1 and the second explores a better pruning criterion.\nOur contributions. (1) We propose a simple yet effective growing regularization scheme, which\ncan help transfer the model expressivity to the remaining part during pruning. The encouraging\nperformance inspires us that the pruning schedule may be as critical as the weight importance cri-\nterion and deserve more research attention. (2) We further adopt growing regularization to exploit",
            "performance inspires us that the pruning schedule may be as critical as the weight importance cri-\nterion and deserve more research attention. (2) We further adopt growing regularization to exploit\nHessian implicitly, without knowing their speciﬁc values. The method can help choose the unim-\nportant weights more faithfully with a theoretically sound basis. In this regard, our paper is the ﬁrst\nto show the connection between magnitude-based pruning and Hessian-based pruning, pointing out\nthat the latter can be turned into the ﬁrst one through our proposed growing regularization scheme.\n(3) The proposed two algorithms are easy to implement and scalable to large-scale datasets and net-\nworks. We show their effectiveness compared with many state-of-the-arts. Especially, the methods\ncan work seamlessly for both ﬁlter pruning and unstructured pruning.\n2\nRELATED WORK\nRegularization-based pruning. The ﬁrst group of relevant works is those applying regularization",
            "can work seamlessly for both ﬁlter pruning and unstructured pruning.\n2\nRELATED WORK\nRegularization-based pruning. The ﬁrst group of relevant works is those applying regularization\nto learn sparsity. The most famous probably is to use L0 or L1 regularization Louizos et al. (2018);\nLiu et al. (2017); Ye et al. (2018) due to their sparsity-inducing nature. In addition, the common\nL2 regularization is also explored for approximated sparsity Han et al. (2015; 2016). The early\npapers focus more on unstructured pruning, which is beneﬁcial to model compression yet not to\nacceleration. For structured pruning in favor of acceleration, Group-wise Brain Damage Lebedev &\nLempitsky (2016) and SSL Wen et al. (2016) propose to use Group LASSO Yuan & Lin (2006) to\nlearn regular sparsity, where the penalty strength is still kept in small scale because the penalty is\nuniformly applied to all the weights. To resolve this, Ding et al. (2018) and Wang et al. (2019c)",
            "uniformly applied to all the weights. To resolve this, Ding et al. (2018) and Wang et al. (2019c)\npropose to employ different penalty factors for different weights, enabling large regularization.\nImportance-based pruning. Importance-based pruning tries to establish certain advanced impor-\ntance criteria that can reﬂect the true relative importance among weights as faithfully as possible.\nThe pruned weights are usually decided immediately by some proposed formula instead of by train-\ning (although the whole pruning process can involve training, e.g., iterative pruning). The most\nwidely used criterion is the magnitude-based: weight absolute value for unstructured pruningHan\net al. (2015; 2016) or L1/L2-norm for structured pruning Li et al. (2017). This heuristic criterion\nwas proposed a long time ago Reed (1993) and has been argued to be inaccurate. In this respect,\nimprovement mainly comes from using Hessian information to obtain a more accurate approxima-",
            "was proposed a long time ago Reed (1993) and has been argued to be inaccurate. In this respect,\nimprovement mainly comes from using Hessian information to obtain a more accurate approxima-\ntion of the increased loss when a weight is removed LeCun et al. (1990); Hassibi & Stork (1993).\nHessian is intractable to compute for large networks, so some methods (e.g., EigenDamage Wang\net al. (2019a), WoodFisher Singh & Alistarh (2020)) employ cheap approximation (such as K-FAC\nFisher Martens & Grosse (2015)) to make the 2nd-order criteria tractable on deep networks.\nNote that, there is no a hard boundary between the importance-based and regularization-based.\nMany papers present their schemes in the combination of the two Ding et al. (2018); Wang et al.\n(2019c). The difference mainly lies in their emphasis: Regularization-based method focuses more\non an advanced penalty scheme so that the subsequent pruning criterion can be simple; while the",
            "(2019c). The difference mainly lies in their emphasis: Regularization-based method focuses more\non an advanced penalty scheme so that the subsequent pruning criterion can be simple; while the\nimportance-based one focus more on an advanced importance criterion itself. Meanwhile, regular-\nization paradigm always involves iterative training, while the importance-based can be one-shot Le-\nCun et al. (1990); Hassibi & Stork (1993); Wang et al. (2019a) (no training for picking weights to\nprune) or involve iterative training Molchanov et al. (2017b; 2019); Ding et al. (2019a;b).\nOther model compression methods. Apart from pruning, there are also many other model com-\npression approaches, e.g., quantization Courbariaux & Bengio (2016); Courbariaux et al. (2016);\nRastegari et al. (2016), knowledge distillation Buciluˇa et al. (2006); Hinton et al. (2014), low-\nrank decomposition Denton et al. (2014); Jaderberg et al. (2014); Lebedev et al. (2014); Zhang",
            "Rastegari et al. (2016), knowledge distillation Buciluˇa et al. (2006); Hinton et al. (2014), low-\nrank decomposition Denton et al. (2014); Jaderberg et al. (2014); Lebedev et al. (2014); Zhang\n1By pruning schedule, we mean the way to remove weights (e.g., removing all weights in a single step or\nmulti-steps), not the training schedule such as learning rate settings, etc.\n2",
            "Published as a conference paper at ICLR 2021\net al. (2015), and efﬁcient architecture design or search Howard et al. (2017); Sandler et al. (2018);\nHoward et al. (2019); Zhang et al. (2018); Tan & Le (2019); Zoph & Le (2017); Elsken et al. (2019).\nThey are orthogonal to network pruning and can work with the proposed methods to compress more.\n3\nPROPOSED METHOD\n3.1\nPROBLEM FORMULATION\nPruning can be formulated as a transformation T(∗) that takes a pretrained big model w as input and\noutput a small model w1, typically followed by a ﬁne-tuning process F(∗), which gives us the ﬁnal\noutput w2 = F(w1). We do not focus on F(∗) since it is simply a standard neural network training\nprocess, but focus on the process of w1 = T(w). The effect of pruning can be further speciﬁed\ninto two sub-transformations: (1) M = T1(w), which obtains a binary mask vector M that decides\nwhich weights will be removed; (2) T2(w), which adjusts the values of remaining weights. That is,",
            "into two sub-transformations: (1) M = T1(w), which obtains a binary mask vector M that decides\nwhich weights will be removed; (2) T2(w), which adjusts the values of remaining weights. That is,\nw1 = T(w) = T1(w) ⊙T2(w) = M ⊙T2(w).\n(1)\nFor one-shot pruning, there is no iterative training at T1. It depends on a speciﬁc algorithm to\ndecide whether to adjust the remaining weights. For example, OBD LeCun et al. (1990) and L1-\nnorm pruning Li et al. (2017) do not adjust the kept weights (i.e., T2 is the identity function) while\nOBS Hassibi & Stork (1993) does. For learning-based pruning, both T1 and T2 involve iterative\ntraining and the kept weights will always be adjusted.\nIn the following, we will present our algorithms in the ﬁlter pruning scenario since we mainly\nfocus on model acceleration instead of compression in this work. Nevertheless, the methodology\ncan seamlessly translate to the unstructured pruning case. The difference lies in how we deﬁne the",
            "focus on model acceleration instead of compression in this work. Nevertheless, the methodology\ncan seamlessly translate to the unstructured pruning case. The difference lies in how we deﬁne the\nweight group: For ﬁlter pruning, a 4-d tensor convolutional ﬁlter (or 2-d tensor for fully-connected\nlayers) is regarded as a weight group, while for unstructured pruning, a single weight makes a group.\n3.2\nPRUNING SCHEDULE: GREG-1\nOur ﬁrst method (GReg-1) is a variant of L1-norm pruning Li et al. (2017). It obtains the mask M\nby L1-norm sorting but adjusts the kept weights via regularization. Speciﬁcally, given a pre-trained\nmodel w and layer pruning ratio rl, we sort the ﬁlters by L1-norm and set the mask to zero for\nthose with the least norms. Then, unlike Li et al. (2017) which removes the unimportant weights\nimmediately (i.e., one-shot fashion), we impose a growing L2 penalty to drive them to zero ﬁrst:\nλj = λj + δλ, j ∈{j | M[j] = 0},\n(2)",
            "immediately (i.e., one-shot fashion), we impose a growing L2 penalty to drive them to zero ﬁrst:\nλj = λj + δλ, j ∈{j | M[j] = 0},\n(2)\nwhere λj is the penalty factor for j-th weight; δλ is the granularity in which we add up the penalty.\nClearly, a smaller δλ means this regularization process smoother. Besides, λj is only updated\nevery Ku iterations, which is a buffer time to let the network adapt to the new regularization. This\nalgorithm is to explore whether the way we remove them (i.e., pruning schedule) leads to a difference\ngiven the same weights to prune. Simple as it is, the scheme can bring signiﬁcant accuracy gains\nespecially under a large pruning ratio (Tab. 1). Note that, we intentionally set δλ the same for all\nthe unimportant weights to keep the core idea simple. Natural extensions of using different penalty\nfactors for different weights (such as those in Ding et al. (2018); Wang et al. (2019c)) may be worth\nexploring but out of the scope of this work.",
            "factors for different weights (such as those in Ding et al. (2018); Wang et al. (2019c)) may be worth\nexploring but out of the scope of this work.\nWhen λj reaches a pre-set ceiling τ, we terminate the training and prune those with the least L1-\nnorms, then ﬁne-tune. Notably, the pruning will barely hurt the accuracy since the unimportant\nweights have been compressed to typically less than\n1\n1000 the magnitude of remaining weights.\n3.3\nIMPORTANCE CRITERION: GREG-2\nOur second algorithm is to further take advantage of the growing regularization scheme, not for\npruning schedule but scoring. The training of neural networks is prone to overﬁtting, so regulariza-\ntion is normally employed. L2 regularization (or referred to as weight decay) is a standard technique\nfor deep network training. Given a dataset D, model parameters w, the total loss will typically be\nE(w, D) = L(w, D) + 1\n2λ∥w∥2\n2,\n(3)\n3",
            "Published as a conference paper at ICLR 2021\nwhere L is the task loss function. When the training converges, there should be\nλw∗\ni + ∂L\n∂wi\n\f\f\nwi=w∗\ni = 0,\n(4)\nwhere w∗\ni indicates the i-th weight at its local minimum. Eq. (4) shows that, for each speciﬁc weight\nelement, its equilibrium position is determined by two forces: loss gradient (i.e., guidance from the\ntask) and regularization gradient (i.e., guidance from our prior). Our idea is to slightly increase the\nλ to break the equilibrium and see how it results in a new one. A general impression is: If λ goes a\nlittle higher, the penalty force will drive the weights further towards origin and it will not stop unless\nproper loss gradient comes to halt it and then a new equilibrium is reached at ˆw∗\ni . Considering\ndifferent weights have different scales, we deﬁne a ratio ri = ˆw∗\ni /w∗\ni to describe how much the\nweight magnitude changes after increasing the penalty factor. Our interest lies in how the ri differs",
            "i /w∗\ni to describe how much the\nweight magnitude changes after increasing the penalty factor. Our interest lies in how the ri differs\nfrom one another and how it relates to the underlying Hessian information.\nDeep neural networks are well-known over-parameterized and highly non-convex. To obtain a fea-\nsible analysis, we adopt a local quadratic approximation of the loss function based on Taylor se-\nries expansion Strang (1991) following common practices LeCun et al. (1990); Hassibi & Stork\n(1993); Wang et al. (2019a). Then when the model is converged, the error E can be described by\nthe converged weights w∗and the underlying Hessian matrix H (note H is p.s.d. since the model is\nconverged). After increasing the penalty λ by δλ, the new converged weights can be proved to be\nˆw∗= (H + δλ I)−1Hw∗,\n(5)\nwhere I stands for the identity matrix. Here we meet with the common problem of estimating\nHessian and its inverse, which are well-known to be intractable for deep neural networks. We",
            "(5)\nwhere I stands for the identity matrix. Here we meet with the common problem of estimating\nHessian and its inverse, which are well-known to be intractable for deep neural networks. We\nexplore two simpliﬁed cases to help us move forward.\n(1) H is diagonal, which is a common simpliﬁcation for Hessian LeCun et al. (1990), implying that\nthe weights are independent of each other. For w∗\ni with second derivative hii. With L2 penalty\nincreased by δλ (δλ > 0), the new converged weights can be proved to be\nˆw∗\ni =\nhii\nhii + δλw∗\ni , ⇒ri = ˆw∗\ni\nw∗\ni\n=\n1\nδλ/hii + 1,\n(6)\nwhere ri ∈[0, 1) since hii ≥0 and δλ > 0. As seen, larger hii results in larger ri (closer to 1),\nmeaning that the weight is relatively less moved towards the origin. Our second algorithm pri-\nmarily builds upon this ﬁnding, which implies when we add a penalty perturbation to the converged\nnetwork, the way that different weights respond can reﬂect their underlying Hessian information.",
            "marily builds upon this ﬁnding, which implies when we add a penalty perturbation to the converged\nnetwork, the way that different weights respond can reﬂect their underlying Hessian information.\n(2) In practice, we know H is rarely diagonal. How the dependency among weights affects the\nﬁnding above is of interest. To have a closed form of inverse Hessian in Eq. (5), we explore the 2-d\ncase, namely, w∗=\n\u0000 w∗\n1\nw∗\n2\n\u0001\n, H =\n\u0000 h11 h12\nh12 h22\n\u0001\n, ˆH =\n\u0000 h11+δλ\nh12\nh12\nh22+δλ\n\u0001\n. The new converged weights can\nbe analytically solved below, where the approximation equality is because that δλ is rather small,\n\u001a\nˆw∗\n1\nˆw∗\n2\n\u001b\n=\n1\n| ˆH|\n\u001a\n(h11h22 + h11δλ −h2\n12)w∗\n1 + δλh12w∗\n2\n(h11h22 + h22δλ −h2\n12)w∗\n2 + δλh12w∗\n1\n\u001b\n≈\n1\n| ˆH|\n\u001a\n(h11h22 + h11δλ −h2\n12)w∗\n1\n(h11h22 + h22δλ −h2\n12)w∗\n2\n\u001b\n,\n(7)\n⇒r1 =\n1\n| ˆH|\n(h11h22 + h11δλ −h2\n12), r2 =\n1\n| ˆH|\n(h11h22 + h22δλ −h2\n12).\n(8)\nAs seen, h11 > h22 also leads to r1 > r2, in line with the ﬁnding above. The existence of weight",
            "12)w∗\n2\n\u001b\n,\n(7)\n⇒r1 =\n1\n| ˆH|\n(h11h22 + h11δλ −h2\n12), r2 =\n1\n| ˆH|\n(h11h22 + h22δλ −h2\n12).\n(8)\nAs seen, h11 > h22 also leads to r1 > r2, in line with the ﬁnding above. The existence of weight\ndependency (i.e., the h12) actually does not affect the conclusion since it is included in both ratios.\nThese theoretical analyses show us that when the penalty is increased at the same pace, because of\ndifferent local curvature structures, the weights actually respond differently – weights with larger\ncurvature will be less moved. As such, the magnitude discrepancy among weights will be magniﬁed\nas λ grows. Ultimately, the weights will naturally separate (see Fig. 1 for an empirical validation).\nWhen the discrepancy is large enough, even the simple L1-norm can make an accurate criterion.\nNotably, the whole process happens itself with the uniformly rising L2 penalty, no need to know the\nHessian values, thus not bothered by any issue arising from Hessian approximation in relevant prior",
            "Notably, the whole process happens itself with the uniformly rising L2 penalty, no need to know the\nHessian values, thus not bothered by any issue arising from Hessian approximation in relevant prior\narts LeCun et al. (1990); Hassibi & Stork (1993); Wang et al. (2019a); Singh & Alistarh (2020).\nIn terms of the speciﬁc algorithm, all the penalty factor is increased at the same pace,\nλj = λj + δλ, for all j.\n(9)\n4",
            "Published as a conference paper at ICLR 2021\nAlgorithm 1 GReg-1 and GReg-2 Algorithms\n1: Input: Pre-trained model w, pruning ratio for l-th layer rl, l = 1 ∼L, original weight decay γ.\n2: Input: Regularization ceiling τ, ceiling for picking τ ′, interval Ku, Ks, granularity δλ.\n3: Init: Iteration i = 0. λj = 0 for all ﬁlter j. Set kept ﬁlter indexes Sk\nl to ∅for each layer l.\n4: Init: Set pruned ﬁlter indexes Sp\nl by L1-norm sorting, set Sp\nl to full set, for each layer l.\n5: while λj ≤τ, j ∈Sp\nl do\n6:\nif i % Ku = 0 then\n7:\nif Sk\nl = ∅and λj > τ ′, j ∈Sp\nl then\n8:\nSet Sp\nl by L1-norm scoring, Sk\nl as the complementary set of Sp\nl , for each layer l.\n9:\nend if\n10:\nλj = λj + δλ for j ∈Sp\nl , λj = −γ for j ∈Sk\nl , for each layer l\n11:\nend if\n12:\nWeight update by stochastic gradient descent (where the regularization is enforced).\n13:\ni = i + 1.\n14: end while\n15: Train for another Ks iterations to stabilize. Then prune by L1-norms and get model w1.\n16: Fine-tune w1 to regain accuracy.",
            "13:\ni = i + 1.\n14: end while\n15: Train for another Ks iterations to stabilize. Then prune by L1-norms and get model w1.\n16: Fine-tune w1 to regain accuracy.\n17: Output: Pruned model w2.\nWhen λj reaches some ceiling τ ′, the magnitude gap turns large enough to let L1-norm do scor-\ning faithfully. After this, the procedures are similar to those in GReg-1: λ for the unimportant\nweights are further increased. One extra step is to bring back the kept weights to the normal mag-\nnitude. Although they are the “survivors” during the previous competition under a large penalty,\ntheir expressivity are also hurt. To be exact, we adopt negative penalty factor for the kept weights\nto encourage them to recover. When the λ for unimportant weights reaches the threshold τ (akin\nto that of GReg-1), the training is terminated. L1-pruning is conducted and then ﬁne-tune to regain\naccuracy. To this end, the proposed two algorithms can be summarized in Algorithm 1.",
            "to that of GReg-1), the training is terminated. L1-pruning is conducted and then ﬁne-tune to regain\naccuracy. To this end, the proposed two algorithms can be summarized in Algorithm 1.\nPruning ratios. We employ pre-speciﬁed pruning ratios in this work to keep the core method neat\n(see Appendix for more discussion). Exploring layer-wise sensitivity is out of the scope of this work,\nbut clearly any method that ﬁnds more proper pruning ratios can readily work with our approaches.\nDiscussion: differences from IncReg. Although our work shares a general spirit of growing reg-\nularization with IncReg Wang et al. (2019c;b), our work is actually starkly different from theirs in\nmany axes:\n• Motivation. The motivations for using the growing regularization are different. Wang\net al. (2019c;b) adopt growing regularization to select the unimportant weights by train-\ning. Namely, they focus on the importance criterion problem. In contrast, we use growing",
            "et al. (2019c;b) adopt growing regularization to select the unimportant weights by train-\ning. Namely, they focus on the importance criterion problem. In contrast, we use growing\nregularization to investigate the pruning schedule problem (for GReg-1) or exploit the un-\nderlying Hessian information (for GReg-2). The importance criterion is simply L1-norm.\n• Algorithm design. Wang et al. (2019c;b) assign different regularization factors to different\nweight groups based on their relative importance, while we assign them with the same fac-\ntors. For GReg-1, this may not be a substantial difference, while for GReg-2, the difference\nis fundamental because the theoretical analysis of GReg-2 (Sec. 3.3) relies on the fact that\nregularization factors are kept the same for different weights.\n• Theoretical analysis. The algorithm in Wang et al. (2019c;b) is generally heuristic-based,\nwhile our work provides rigorous theoretical analyses (Sec. 3.3) to support the proposed\nalgorithm GReg-2.",
            "• Theoretical analysis. The algorithm in Wang et al. (2019c;b) is generally heuristic-based,\nwhile our work provides rigorous theoretical analyses (Sec. 3.3) to support the proposed\nalgorithm GReg-2.\n• Empirical performance.\nBoth our methods are signiﬁcantly better than Wang et al.\n(2019c;b) on the large-scale ImageNet dataset, which will be shown in the experiment\nsection (Tab. 3).\nDiscussion: other regularization forms. The proposed methods in this work adopts L2 regulariza-\ntion. Here we discuss the possibility to generalize the method to other regularization forms (L1 and\nL0). (1) For GReg-1, it can be easily generalized to other regularization forms like L1. For GReg-2,\nsince the theoretical basis in Sec. 3.3 relies on the local quadratic approximation, L2 regularization\nmeets this requirement while L1 does not. Therefore, GReg-2 cannot be (easily) generalized to the\n5",
            "Published as a conference paper at ICLR 2021\nTable 1: Comparison between pruning schedules: one-shot pruning vs. our proposed GReg-1. Each\nsetting is randomly run for 3 times, mean and std accuracies reported.\nResNet56 + CIFAR10: Baseline accuracy 93.36%, #Params: 0.85M, FLOPs: 0.25G\nPruning ratio r (%)\n50\n70\n90\n92.5\n95\nSparsity (%) / Speedup\n49.82/1.99×\n70.57/3.59×\n90.39/11.41×\n93.43/14.76×\n95.19/19.31×\nAcc. (%, L1+one-shot)\n92.97±0.15\n91.88±0.09\n87.34±0.21\n87.31±0.28\n82.79±0.22\nAcc. (%, GReg-1, ours)\n93.06±0.09\n92.23±0.21\n89.49±0.23\n88.39±0.15\n85.97±0.16\nAcc. gain (%)\n0.09\n0.35\n2.15\n1.08\n3.18\nVGG19 + CIFAR100: Baseline accuracy 74.02%, #Params: 20.08M, FLOPs: 0.80G\nPruning ratio r (%)\n50\n60\n70\n80\n90\nSparsity (%) / Speedup\n74.87/3.60×\n84.00/5.41×\n90.98/8.84×\n95.95/17.30×\n98.96/44.22×\nAcc. (%, L1+one-shot)\n71.49±0.14\n70.27±0.12\n66.05±0.04\n61.59±0.03\n51.36±0.11\nAcc. (%, GReg-1, ours)\n71.50±0.12\n70.33±0.12\n67.35±0.15\n63.55±0.29\n57.09±0.03\nAcc. gain (%)\n0.01\n0.06\n1.30\n1.96\n5.73",
            "Acc. (%, L1+one-shot)\n71.49±0.14\n70.27±0.12\n66.05±0.04\n61.59±0.03\n51.36±0.11\nAcc. (%, GReg-1, ours)\n71.50±0.12\n70.33±0.12\n67.35±0.15\n63.55±0.29\n57.09±0.03\nAcc. gain (%)\n0.01\n0.06\n1.30\n1.96\n5.73\nL1 regularization as far as we can see currently. (2) For L0 regularization, it is well-known NP-hard.\nIn practice, it is typically converted to the L1 regularization case, which we just discussed.\n4\nEXPERIMENTAL RESULTS\nDatasets and networks. We ﬁrst conduct analyses on the CIFAR10/100 datasets Krizhevsky (2009)\nwith ResNet56 He et al. (2016)/VGG19 Simonyan & Zisserman (2015). Then we evaluate our\nmethods on the large-scale ImageNet dataset Deng et al. (2009) with ResNet34 and 50 He et al.\n(2016). For CIFAR datasets, we train our baseline models with accuracies comparable to those in\nthe original papers. For ImageNet, we take the ofﬁcial PyTorch Paszke et al. (2019) pre-trained\nmodels2 as baseline to maintain comparability with other methods.",
            "the original papers. For ImageNet, we take the ofﬁcial PyTorch Paszke et al. (2019) pre-trained\nmodels2 as baseline to maintain comparability with other methods.\nTraining settings. To control the irrelevant factors as we can, for comparison methods that release\ntheir pruning ratios, we will adopt their ratios; otherwise, we will use our speciﬁed ones. We com-\npare the speedup (measured by FLOPs reduction) since we mainly target model acceleration rather\nthan compression. Detailed training settings (e.g., hyper-parameters and layer pruning ratios) are\nsummarized in the Appendix.\n4.1\nRESNET56/VGG19 ON CIFAR-10/100\nPruning schedule: GReg-1. First, we explore the effect of different pruning schedules on the\nperformance of pruning. Speciﬁcally, we conduct two sets of experiments for comparison: (1)\nprune by L1-norm sorting and ﬁne-tune Li et al. (2017) (shorted as “L1+one-shot”); (2) employ the\nproposed growing regularization scheme (“GReg-1”) and ﬁne-tune. We use a uniform pruning ratio",
            "prune by L1-norm sorting and ﬁne-tune Li et al. (2017) (shorted as “L1+one-shot”); (2) employ the\nproposed growing regularization scheme (“GReg-1”) and ﬁne-tune. We use a uniform pruning ratio\nscheme here: Pruning ratio r is the same for all l-th conv layer (the ﬁrst layer is not pruned following\ncommon practice Gale et al. (2019)). For ResNet56, since it has the residual addition restriction, we\nonly prune the ﬁrst conv layer in a block as previous works do Li et al. (2017). For comprehensive\ncomparisons, the pruning ratios vary in a large spectrum, covering acceleration ratios from around\n2× to 44×. Note that we do not intend to obtain the best performance here but systematically explore\nthe effect of different pruning schedules, so we employ relatively simple settings (e.g., the uniform\npruning ratios). For fair comparisons, the ﬁne-tuning scheme (e.g., number of epochs, learning rate",
            "pruning ratios). For fair comparisons, the ﬁne-tuning scheme (e.g., number of epochs, learning rate\nschedule, etc.) is the same for different methods. Therefore, the key comparison here is to see which\nmethod can deliver a better base model before ﬁne-tuning.\nThe results are shown in Tab. 1. We have the following observations: (1) On the whole, the proposed\nGReg-1 consistently outperforms L1+one-shot. It is important to reiterate that the two settings have\nexactly the same pruned weights, so the only difference is how they are removed. The accuracy\ngaps show that apart from importance scoring, pruning schedule is also a critical factor. In the\nAppendix D, we present more results to demonstrate this ﬁnding actually is general, not merely\nlimited to the case of L1-norm criterion. The proposed regularization-based pruning schedule is\nconsistently more favorable than the one-shot counterpart. (2) The larger pruning ratio, the more\n2https://pytorch.org/docs/stable/torchvision/models.html\n6",
            "consistently more favorable than the one-shot counterpart. (2) The larger pruning ratio, the more\n2https://pytorch.org/docs/stable/torchvision/models.html\n6",
            "Published as a conference paper at ICLR 2021\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRegularization factor \n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm stddev\nlayer1.0.conv1\nlayer1.2.conv1\nlayer1.4.conv1\nlayer1.6.conv1\nlayer2.0.conv1\nlayer2.2.conv1\nlayer2.4.conv1\nlayer2.6.conv1\nlayer3.0.conv1\nlayer3.2.conv1\nlayer3.4.conv1\nlayer3.6.conv1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRegularization factor \n0\n2\n4\n6\n8\nNormalized L1-norm stddev\nlayer  1\nlayer  2\nlayer  3\nlayer  4\nlayer  5\nlayer  6\nlayer  7\nlayer  8\nlayer  9\nlayer 10\nlayer 11\nlayer 12\nlayer 13\nlayer 14\nlayer 15\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRegularization factor \n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nNormalized L1-norm stddev\nlayer1.0.conv1\nlayer1.2.conv1\nlayer2.0.conv1\nlayer2.2.conv1\nlayer3.0.conv1\nlayer3.2.conv1\nlayer3.4.conv1\nlayer4.0.conv1\nlayer4.2.conv1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRegularization factor \n0\n1\n2\n3\n4\n5\n6\nNormalized L1-norm stddev\nlayer1.0.conv1\nlayer1.0.conv2\nlayer1.2.conv1\nlayer1.2.conv2\nlayer2.0.conv1\nlayer2.0.conv2\nlayer2.2.conv1\nlayer2.2.conv2\nlayer3.0.conv1",
            "Regularization factor \n0\n1\n2\n3\n4\n5\n6\nNormalized L1-norm stddev\nlayer1.0.conv1\nlayer1.0.conv2\nlayer1.2.conv1\nlayer1.2.conv2\nlayer2.0.conv1\nlayer2.0.conv2\nlayer2.2.conv1\nlayer2.2.conv2\nlayer3.0.conv1\nlayer3.0.conv2\nlayer3.2.conv1\nlayer3.2.conv2\nlayer4.0.conv1\nlayer4.0.conv2\nlayer4.2.conv1\nlayer4.2.conv2\n(a) ResNet56\n(b) VGG19\n(c) ResNet34\n(d) ResNet50\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n(a) Iter 0\n(b) Iter 5,000\n(c) Iter 10,000\n(d) Iter 15,000\nFigure 1: Row 1: Illustration of weight separation as L2 penalty grows. Row 2: Normalized ﬁlter\nL1-norm over iterations for ResNet50 layer2.3.conv1 (please see the Appendix for VGG19 plots).\nTable 2: Comparison of different methods on the CIFAR10 and CIFAR100 datasets.\nMethod",
            "L1-norm over iterations for ResNet50 layer2.3.conv1 (please see the Appendix for VGG19 plots).\nTable 2: Comparison of different methods on the CIFAR10 and CIFAR100 datasets.\nMethod\nNetwork/Dataset\nBase acc. (%) Pruned acc. (%) Acc. drop Speedup\nCP He et al. (2017)\nResNet56/CIFAR10\n92.80\n91.80\n1.00\n2.00×\nAMC He et al. (2018b)\n92.80\n91.90\n0.90\n2.00×\nSFP He et al. (2018a)\n93.59\n93.36\n0.23\n2.11×\nAFP Ding et al. (2018)\n93.93\n92.94\n0.99\n2.56×\nC-SGD Ding et al. (2019a)\n93.39\n93.44\n-0.05\n2.55×\nGReg-1 (ours)\n93.36\n93.18\n0.18\n2.55×\nGReg-2 (ours)\n93.36\n93.36\n0.00\n2.55×\nKron-OBD Wang et al. (2019a)\nVGG19/CIFAR100\n73.34\n60.70\n12.64\n5.73×\nKron-OBS Wang et al. (2019a)\n73.34\n60.66\n12.68\n6.09×\nEigenDamage Wang et al. (2019a)\n73.34\n65.18\n8.16\n8.80×\nGReg-1 (ours)\n74.02\n67.55\n6.67\n8.84×\nGReg-2 (ours)\n74.02\n67.75\n6.47\n8.84×\npronounced of the gain. This is reasonable since when more weights are pruned, the network cannot",
            "73.34\n65.18\n8.16\n8.80×\nGReg-1 (ours)\n74.02\n67.55\n6.67\n8.84×\nGReg-2 (ours)\n74.02\n67.75\n6.47\n8.84×\npronounced of the gain. This is reasonable since when more weights are pruned, the network cannot\nrecover by its inherent plasticity Mittal et al. (2018), then the regularization-based way is more\nhelpful because it helps the model transfer its expressive power to the remaining part. When the\npruning ratio is relatively small (such as ResNet56, r = 50%) , the plasticity of the model is enough\nto heal, so the beneﬁt from GReg-1 is less signiﬁcant compared with the one-shot counterpart.\nImportance criterion: GReg-2. Here we empirically validate our ﬁnding in Sec. 3.3, that is,\nwith uniformly rising L2 penalty, the weights should naturally separate. We claim, if h11 > h22,\nthere should be r1 > r2, where r1 =\nˆ\nw1\nw1 , r2 =\nˆ\nw2\nw2 (the * mark indicating the local minimum\nis omitted here for readability). r1 > r2 leads to\nˆ\nw1\nw1 >\nˆ\nw2\nw2 , namely, r1 =\nˆ\nw1\nˆ\nw2 >\nw1\nw2 . This",
            "ˆ\nw1\nw1 , r2 =\nˆ\nw2\nw2 (the * mark indicating the local minimum\nis omitted here for readability). r1 > r2 leads to\nˆ\nw1\nw1 >\nˆ\nw2\nw2 , namely, r1 =\nˆ\nw1\nˆ\nw2 >\nw1\nw2 . This\nshows that, after the L2 penalty grows a little, the new magnitude ratio of weight 1 over weight\n2 will be magniﬁed if h11 > h22 (w1, w2 are positive in the analysis here, while the conclusion\nstill holds if either of them is negative). In Fig. 1 (Row 1), we plot the standard deviation (divided\nby the means for normalization since the magnitude varies over iterations) of ﬁlter L1-norms as\nthe regularization grows. As seen, the normalized L1-norm stddev grows larger and larger as λ\ngrows. This phenomenon consistently appears across different models and datasets. To ﬁguratively\nunderstand how the increasing penalty affects the relative magnitude over time, in Fig. 1 (Row 2),\nwe plot the relative L1-norms (divided by the max L1-norm for normalization) at different iterations.",
            "we plot the relative L1-norms (divided by the max L1-norm for normalization) at different iterations.\nAs shown, it is hard to tell which ﬁlters are really important by the initial ﬁlter magnitude (Iter 0),\nbut under a large penalty later, their discrepancy turns more and more obvious and ﬁnally it is very\neasy to identify which ﬁlters are more important. Since the magnitude gap is so large, the simple\nL1-norm can make a sufﬁciently faithful criterion.\nCIFAR benchmarks. Finally, we compare the proposed algorithms with existing methods on the\nCIFAR datasets (Tab. 2). Here we adopt non-uniform pruning ratios (see the Appendix for speciﬁc\nnumbers) for the best accuracy-FLOPs trade-off. On CIFAR10, compared with AMC He et al.\n7",
            "Published as a conference paper at ICLR 2021\nTable 3: Acceleration comparison on ImageNet. FLOPs: ResNet34: 3.66G, ResNet50: 4.09G.\nMethod\nNetwork\nBase top-1 (%) Pruned top-1 (%) Top-1 drop Speedup\nL1 (pruned-B) Li et al. (2017)\nResNet34\n73.23\n72.17\n1.06\n1.32×\nTaylor-FO Molchanov et al. (2019)\n73.31\n72.83\n0.48\n1.29×\nGReg-1 (ours)\n73.31\n73.54\n-0.23\n1.32×\nGReg-2 (ours)\n73.31\n73.61\n-0.30\n1.32×\nProvableFP Liebenwein et al. (2020) ResNet50\n76.13\n75.21\n0.92\n1.43×\nGReg-1 (ours)\n76.13\n76.27\n-0.14\n1.49×\nAOFP Ding et al. (2019b)\nResNet50\n75.34\n75.63\n-0.29\n1.49×\nGReg-2 (ours)∗\n75.40\n76.13\n-0.73\n1.49×\nIncReg Wang et al. (2019b)\nResNet50\n75.60\n72.47\n3.13\n2.00×\nSFP He et al. (2018a)\n76.15\n74.61\n1.54\n1.72×\nHRank Lin et al. (2020a)\n76.15\n74.98\n1.17\n1.78×\nTaylor-FO Molchanov et al. (2019)\n76.18\n74.50\n1.68\n1.82×\nFactorized Li et al. (2019)\n76.15\n74.55\n1.60\n2.33×\nDCP Zhuang et al. (2018)\n76.01\n74.95\n1.06\n2.25×\nCCP-AC Peng et al. (2019)\n76.15\n75.32\n0.83\n2.18×\nGReg-1 (ours)\n76.13\n75.16\n0.97\n2.31×",
            "1.68\n1.82×\nFactorized Li et al. (2019)\n76.15\n74.55\n1.60\n2.33×\nDCP Zhuang et al. (2018)\n76.01\n74.95\n1.06\n2.25×\nCCP-AC Peng et al. (2019)\n76.15\n75.32\n0.83\n2.18×\nGReg-1 (ours)\n76.13\n75.16\n0.97\n2.31×\nGReg-2 (ours)\n76.13\n75.36\n0.77\n2.31×\nC-SGD-50 Ding et al. (2019a)\nResNet50\n75.34\n74.54\n0.80\n2.26×\nAOFP Ding et al. (2019b)\n75.34\n75.11\n0.23\n2.31×\nGReg-2 (ours)∗\n75.40\n75.22\n0.18\n2.31×\nLFPC He et al. (2020)\nResNet50\n76.15\n74.46\n1.69\n2.55×\nGReg-1 (ours)\n76.13\n74.85\n1.28\n2.56×\nGReg-2 (ours)\n76.13\n74.93\n1.20\n2.56×\nIncReg Wang et al. (2019b)\nResNet50\n75.60\n71.07\n4.53\n3.00×\nTaylor-FO Molchanov et al. (2019)\n76.18\n71.69\n4.49\n3.05×\nGReg-1 (ours)\n76.13\n73.75\n2.38\n3.06×\nGReg-2 (ours)\n76.13\n73.90\n2.23\n3.06×\n* Since the base models of C-SGD and AOFP have a much lower accuracy than ours, for fair comparison, we\ntrain our own base models with similar accuracy.\nTable 4: Compression comparison on ImageNet with ResNet50. #Parameters: 25.56M.\nMethod\nBase top-1 (%) Pruned top-1 (%) Top-1 drop Sparsity (%)",
            "train our own base models with similar accuracy.\nTable 4: Compression comparison on ImageNet with ResNet50. #Parameters: 25.56M.\nMethod\nBase top-1 (%) Pruned top-1 (%) Top-1 drop Sparsity (%)\nGSM Ding et al. (2019c)\n75.72\n74.30\n1.42\n80.00\nVariational Dropout Molchanov et al. (2017a)\n76.69\n75.28\n1.41\n80.00\nDPF Lin et al. (2020b)\n75.95\n74.55\n1.40\n82.60\nWoodFisher Singh & Alistarh (2020)\n75.98\n75.20\n0.78\n82.70\nGReg-1 (ours)\n76.13\n75.45\n0.68\n82.70\nGReg-2 (ours)\n76.13\n75.27\n0.86\n82.70\n(2018b), though it adopts better layer-wise pruning ratios via reinforcement-learning, our algorithms\ncan still deliver more favorable performance using sub-optimal human-speciﬁed ratios. AFP Ding\net al. (2018) is another work exploring large regularization, while they do not adopt the growing\nscheme as we do. Its performance is also less favorable on CIFAR10 as shown in the table. Although\nour methods perform a little worse than C-SGD Ding et al. (2019a) on CIFAR10, on the large-scale",
            "scheme as we do. Its performance is also less favorable on CIFAR10 as shown in the table. Although\nour methods perform a little worse than C-SGD Ding et al. (2019a) on CIFAR10, on the large-scale\nImageNet dataset, we will show our methods are signiﬁcantly better than C-SGD.\nNotably, on CIFAR100, Kron-OBD/OBS (an extension by Wang et al. (2019a) of the original\nOBD/OBS from unstructured pruning to structured pruning) are believed to be more accurate than\nL1-norm in terms of capturing relative weight importance LeCun et al. (1990); Hassibi & Stork\n(1993); Wang et al. (2019a). Yet, they are signiﬁcantly outperformed by our GReg-1 based on the\nsimple L1-norm scoring. This may inspire us that an average pruning schedule (like the one-shot\nfashion) can offset the gain from a more advanced importance scoring scheme.\n4.2\nRESNET34/50 ON IMAGENET\nThen we evaluate our methods on the standard large-scale ImageNet benchmarks with ResNets He",
            "fashion) can offset the gain from a more advanced importance scoring scheme.\n4.2\nRESNET34/50 ON IMAGENET\nThen we evaluate our methods on the standard large-scale ImageNet benchmarks with ResNets He\net al. (2016). We refer to the ofﬁcial PyTorch ImageNet training example3 to make sure the imple-\n3https://github.com/pytorch/examples/tree/master/imagenet\n8",
            "Published as a conference paper at ICLR 2021\nmentation (such as data augmentation, weight decay, momentum, etc.) is standard. Please refer to\nthe summarized training setting in the Appendix for details.\nThe results are shown in Tab. 3. Methods with similar speedup are grouped together for easy com-\nparison. In general, our method achieves comparable or better performance across various speedups\non ResNet34 and 50. Concretely, (1) On both ResNet34 and 50, when the speedup is small (less\nthan 2×), only our methods (and AOFP Ding et al. (2019b) for ResNet50) can even improve the\ntop-1 accuracy. This phenomenon is broadly found by previous works Wen et al. (2016); Wang et al.\n(2018); He et al. (2017) but mainly on small datasets like CIFAR, while we make it on the much\nchallenging ImageNet benchmark. (2) Similar to the results on CIFAR (Tab. 1), when the speedup\nis larger, the advantage of our method is more obvious. For example, ours GReg-2 only outperforms",
            "challenging ImageNet benchmark. (2) Similar to the results on CIFAR (Tab. 1), when the speedup\nis larger, the advantage of our method is more obvious. For example, ours GReg-2 only outperforms\nTaylor-FO Molchanov et al. (2019) by 0.86% top-1 accuracy at the ∼2× setting, while at ∼3×,\nGReg-2 is better by 2.21% top-1 accuracy. (3) Many methods work on the weight importance crite-\nrion problem, including some very recent ones (ProvableFP Liebenwein et al. (2020), LFPC He et al.\n(2020)). Yet as shown, our simple variant of L1-norm pruning can still be a strong competitor in\nterms of accuracy-FLOPs trade-off. This reiterates one of our key ideas in this work that the pruning\nschedule may be as important as weight importance scoring and worth more research attention.\nUnstructured pruning. Although we mainly target ﬁlter pruning in this work, the proposed meth-\nods actually can be applied to unstructured pruning as effectively. In Tab. 4, we present the results",
            "Unstructured pruning. Although we mainly target ﬁlter pruning in this work, the proposed meth-\nods actually can be applied to unstructured pruning as effectively. In Tab. 4, we present the results\nof unstructured pruning on ResNet50. WoodFisher Singh & Alistarh (2020) is the state-of-the-art\nHessian-based unstructured pruning approach. Notably, without any Hessian approximation, our\nGReg-2 can achieve comparable performance with it (better absolute accuracy, yet slightly worse\naccuracy drop). Besides, the simple magnitude pruning variant GReg-1 delivers more favorable\nresult, implying that a better pruning schedule also matters in the unstructured pruning case.\n5\nCONCLUSION\nRegularization is long deemed as a sparsity-learning tool in neural network pruning, which usually\nworks in the small strength regime. In this work, we present two algorithms that exploit regulariza-\ntion in a new fashion that the penalty factor is uniformly raised to a large level. Two central problems",
            "tion in a new fashion that the penalty factor is uniformly raised to a large level. Two central problems\nregarding deep neural pruning are tackled by the proposed methods, pruning schedule and weight\nimportance criterion. The proposed approaches rely on few impractical assumptions, have a sound\ntheoretical basis, and are scalable to large datasets and networks. Apart from the methodology it-\nself, the encouraging results on CIFAR and ImageNet also justify our general ideas in this paper: (1)\nIn addition to weight importance scoring, pruning schedule is another pivotal factor in deep neural\npruning which may deserve more research attention. (2) Without any Hessian approximation, we\ncan still tap into its power for pruning with the help of growing L2 regularization.\nACKNOWLEDGEMENTS\nThe work is supported by the National Science Foundation Award ECCS-1916839 and the\nU.S. Army Research Ofﬁce Award W911NF-17-1-0367.\nREFERENCES",
            "ACKNOWLEDGEMENTS\nThe work is supported by the National Science Foundation Award ECCS-1916839 and the\nU.S. Army Research Ofﬁce Award W911NF-17-1-0367.\nREFERENCES\nCristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In SIGKDD,\n2006. 2\nYu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration\nfor deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 1\nM. Courbariaux and Y. Bengio. BinaryNet: Training deep neural networks with weights and activa-\ntions constrained to +1 or −1. arXiv preprint arXiv:1602.02830, 2016. 2\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized\nneural networks: Training deep neural networks with weights and activations constrained to +1\nor -1. arXiv preprint arXiv:1602.02830, 2016. 2\n9",
            "Published as a conference paper at ICLR 2021\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009. 6\nLei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware\nacceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):\n485–532, 2020. 1\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear\nstructure within convolutional networks for efﬁcient evaluation. In NeurIPS, 2014. 2\nXiaohan Ding, Guiguang Ding, Jungong Han, and Sheng Tang. Auto-balanced ﬁlter pruning for\nefﬁcient convolutional neural networks. In AAAI, 2018. 2, 3, 7, 8\nXiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han. Centripetal sgd for pruning very\ndeep convolutional networks with complicated structure. In CVPR, 2019a. 2, 7, 8\nXiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan. Approximated",
            "deep convolutional networks with complicated structure. In CVPR, 2019a. 2, 7, 8\nXiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han, and Chenggang Yan. Approximated\noracle ﬁlter pruning for destructive cnn width optimization. In ICML, 2019b. 2, 8, 9, 13\nXiaohan Ding, Xiangxin Zhou, Yuchen Guo, Jungong Han, Ji Liu, et al. Global sparse momentum\nsgd for pruning very deep neural networks. In NeurIPS, 2019c. 8\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter.\nNeural architecture search: A survey.\nJMLR, 20(55):1–21, 2019. 3\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv\npreprint arXiv:1902.09574, 2019. 1, 6\nSong Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for\nefﬁcient neural network. In NeurIPS, 2015. 1, 2\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding. In ICLR, 2016. 1, 2",
            "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding. In ICLR, 2016. 1, 2\nB. Hassibi and D. G. Stork. Second order derivatives for network pruning: Optimal brain surgeon.\nIn NeurIPS, 1993. 1, 2, 3, 4, 8\nK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n6, 8\nYang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft ﬁlter pruning for accelerating\ndeep convolutional neural networks. In IJCAI, 2018a. 7, 8\nYang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, and Yi Yang. Learning ﬁlter\npruning criteria for deep convolutional neural networks acceleration. In CVPR, 2020. 1, 8, 9\nYihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural net-\nworks. In ICCV, 2017. 1, 7, 9, 13\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: Automl for model",
            "works. In ICCV, 2017. 1, 7, 9, 13\nYihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: Automl for model\ncompression and acceleration on mobile devices. In ECCV, 2018b. 7\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In\nNeurIPS Workshop, 2014. 2\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun\nWang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. arXiv\npreprint arXiv:1905.02244, 2019. 3\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 3\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions. In BMVC, 2014. 2\n10",
            "Published as a conference paper at ICLR 2021\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,\n2009. 6\nV. Lebedev and V. Lempitsky. Fast convnets using group-wise brain damage. In CVPR, 2016. 2\nVadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.\nSpeeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arXiv preprint\narXiv:1412.6553, 2014. 2\nY. LeCun, J. S. Denker, and S. A. Solla. Optimal brain damage. In NeurIPS, 1990. 1, 2, 3, 4, 8, 13,\n14, 15\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436, 2015.\n1\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for\nefﬁcient convnets. In ICLR, 2017. 1, 2, 3, 6, 8, 14\nTuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convo-\nlutional neural networks via factorized convolutional ﬁlters. In CVPR, 2019. 8",
            "Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu. Compressing convo-\nlutional neural networks via factorized convolutional ﬁlters. In CVPR, 2019. 8\nLucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.\nProvable ﬁlter\npruning for efﬁcient neural networks. In ICLR, 2020. 8, 9\nMingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling\nShao. Hrank: Filter pruning using high-rank feature map. In CVPR, 2020a. 8\nTao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning\nwith feedback. In ICLR, 2020b. 8\nZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learn-\ning efﬁcient convolutional networks through network slimming. In ICCV, 2017. 1, 2\nZhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\nnetwork pruning. In ICLR, 2019. 14",
            "Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of\nnetwork pruning. In ICLR, 2019. 14\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\nl 0 regularization. In ICLR, 2018. 2\nJames Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate\ncurvature. In ICML, 2015. 2\nDeepak Mittal, Shweta Bhardwaj, Mitesh M Khapra, and Balaraman Ravindran. Recovering from\nrandom pruning: On the plasticity of deep convolutional neural networks. In WACV, 2018. 7\nDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural\nnetworks. In ICML, 2017a. 8\nP. Molchanov, S. Tyree, and T. Karras. Pruning convolutional neural networks for resource efﬁcient\ninference. In ICLR, 2017b. 1, 2\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation\nfor neural network pruning. In CVPR, 2019. 1, 2, 8, 9",
            "inference. In ICLR, 2017b. 1, 2\nPavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation\nfor neural network pruning. In CVPR, 2019. 1, 2, 8, 9\nMichael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a\nnetwork via relevance assessment. In NeurIPS, 1989. 1\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, 2019. 6\nHanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang. Collaborative channel pruning for\ndeep networks. In ICML, 2019. 8\n11",
            "Published as a conference paper at ICLR 2021\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet\nclassiﬁcation using binary convolutional neural networks. In ECCV, 2016. 2\nR. Reed. Pruning algorithms – a survey. IEEE Transactions on Neural Networks, 4(5):740–747,\n1993. 1, 2\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-\nbilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. 3\nJ¨urgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117,\n2015. 1\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. In ICLR, 2015. 6\nSidak Pal Singh and Dan Alistarh. Woodﬁsher: Efﬁcient second-order approximations for model\ncompression. In NeurIPS, 2020. 1, 2, 4, 8, 9, 13\nGilbert Strang. Calculus. Wellesley-Cambridge Press, 1991. 4\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-",
            "Gilbert Strang. Calculus. Wellesley-Cambridge Press, 1991. 4\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural net-\nworks. In ICML, 2019. 3\nChaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning\nin the kronecker-factored eigenbasis. In ICML, 2019a. 1, 2, 4, 7, 8\nHuan Wang, Qiming Zhang, Yuehai Wang, and Haoji Hu.\nStructured probabilistic pruning for\nconvolutional neural network acceleration. In BMVC, 2018. 9\nHuan Wang, Xinyi Hu, Qiming Zhang, Yuehai Wang, Lu Yu, and Haoji Hu. Structured pruning for\nefﬁcient convolutional neural networks via incremental regularization. IEEE Journal of Selected\nTopics in Signal Processing, 14(4):775–788, 2019b. 5, 8\nHuan Wang, Qiming Zhang, Yuehai Wang, Lu Yu, and Haoji Hu. Structured pruning for efﬁcient\nconvnets via incremental regularization. In IJCNN, 2019c. 2, 3, 5\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in",
            "convnets via incremental regularization. In IJCNN, 2019c. 2, 3, 5\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\ndeep neural networks. In NeurIPS, 2016. 1, 2, 9, 13\nJianbo Ye, Xin Lu, Zhe Lin, and James Z Wang. Rethinking the smaller-norm-less-informative\nassumption in channel pruning of convolution layers. In ICLR, 2018. 2\nMing Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal\nof the Royal Statistical Society, 68(1):49–67, 2006. 2\nXiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun.\nEfﬁcient and accurate\napproximations of nonlinear convolutional networks. In CVPR, 2015. 2\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient\nconvolutional neural network for mobile devices. In CVPR, 2018. 3\nZhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou",
            "convolutional neural network for mobile devices. In CVPR, 2018. 3\nZhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou\nHuang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In\nNeurIPS, 2018. 8\nBarret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In ICLR, 2017.\n3\n12",
            "Published as a conference paper at ICLR 2021\nA\nAPPENDIX\nA.1\nEXPERIMENTAL SETTING DETAILS\nTraining setting summary. About the networks evaluated, we intentionally avoid AlexNet and\nVGG on the ImageNet benchmark because the single-branch architecture is no longer representa-\ntive of the modern deep network architectures with residuals (but still keep VGG19 on the CIFAR\nanalysis to make sure the ﬁndings are not limited to one speciﬁc architecture). Apart from some key\nsettings stated in the paper, a more detailed training setting summary is shown as Tab. 5.\nTable 5: Training setting summary. For the SGD solver, in the parentheses are the momentum and\nweight decay. For ImageNet, batch size 64 is used for pruning instead of the standard 256, which is\nbecause we want to save the training time.\nDataset\nCIFAR\nImageNet\nSolver\nSGD (0.9, 5e-4)\nSGD (0.9, 1e-4)\nLR policy (prune)\nFixed (1e-3)\nLR policy (ﬁnetune) Multi-step (0:1e-2, 60:1e-3, 90:1e-4) Multi-step (0:1e-2, 30:1e-3, 60:1e-4, 75:1e-5)",
            "Dataset\nCIFAR\nImageNet\nSolver\nSGD (0.9, 5e-4)\nSGD (0.9, 1e-4)\nLR policy (prune)\nFixed (1e-3)\nLR policy (ﬁnetune) Multi-step (0:1e-2, 60:1e-3, 90:1e-4) Multi-step (0:1e-2, 30:1e-3, 60:1e-4, 75:1e-5)\nTotal epoch (ﬁnetune)\n120\n90\nBatch size (prune)\n256\n64\nBatch size (ﬁnetune)\n256\nPruning ratios. Although several recent methods Ding et al. (2019b); Singh & Alistarh (2020) can\nautomatically decide pruning ratios, in this paper we opt to consider pruning independent with the\npruning ratio choosing. The main consideration is that pruning ratio is broadly believed to reﬂect\nthe redundancy of different layers LeCun et al. (1990); Wen et al. (2016); He et al. (2017), which\nis an inherent characteristic of the model, thus should not be coupled with the subsequent pruning\nalgorithms.\nBefore we list the speciﬁc pruning ratios, we explain how we set them. (1) For a ResNet, if it has N\nstages, we will use a list of N ﬂoats to represent its pruning ratios for the N stages. For example,",
            "stages, we will use a list of N ﬂoats to represent its pruning ratios for the N stages. For example,\nResNet56 has 4 stages in conv layers, then “[0, 0.5, 0.5, 0.5]” means “for the ﬁrst stage (which is also\nthe ﬁrst conv layer), the pruning ratio is 0; the other three stages have pruning ratio of 0.5”. Besides,\nsince we do not prune the last conv in a residual block, which means for a two-layer residual block\n(for ResNet56), we only prune the ﬁrst layer; for a three-layer bottleneck block (for ResNet34 and\n50), we only prune the ﬁrst and second layers. (2) For VGG19, we use the following pruning ratio\nsetting. For example, “[0:0, 1-9:0.3, 10-15:0.5]” means “for the ﬁrst layer (index starting from 0),\nthe pruning ratio is 0; for layer 1 to 9, the pruning ratio is 0.3; for layer 10 to 15, the pruning ratio is\n0.5”.\nWith these, the speciﬁc pruning ratio for each of our experiments in the paper are listed in Tab. 6.",
            "0.5”.\nWith these, the speciﬁc pruning ratio for each of our experiments in the paper are listed in Tab. 6.\nWe do not have strong rules to set them, except one, which is setting the pruning ratios of higher\nstages smaller, because the FLOPs of higher layers are relatively smaller (due to the fact that the\nspatial feature map sizes are smaller) and we are targeting more acceleration than compression. Of\ncourse, this scheme only is quite crude, yet as our results (Tab. 3 and 4) show, even with these crude\nsettings, the performances are still competitive.\nB\nPROOF OF EQ. 5\nWhen a quadratic function E converges at w∗with Hessian matrix H, it can be formulated as\nE = (w −w∗)T H(w −w∗) + C,\n(10)\nwhere C is a constant. Now a new function is made by increasing the L2 penalty by small amount\nδλ, namely,\nˆE = E + δλwT Iw.\n(11)\nLet the new converged values be ˆw∗, then similar to Eq. 10, ˆE can be formulated as\nˆE = (w −ˆw∗)T ˆH(w −ˆw∗) + ˆC, where ˆH = H + δλI.\n(12)\n13",
            "Published as a conference paper at ICLR 2021\nTable 6: Pruning ratio summary.\nDataset\nNetwork\nSpeedup\nPruned top-1 accuracy (%)\nPruning ratio\nCIFAR10\nResNet56\n2.55×\n93.36\n[0, 0.75, 0.75, 0.32]\nCIFAR100\nVGG19\n8.84×\n67.56\n[0:0, 1-15:0.70]\nImageNet\nResNet34\n1.32×\n73.44\n[0, 0.50, 0.60, 0.40, 0]∗\nImageNet\nResNet50\n1.49×\n76.24\n[0, 0.30, 0.30, 0.30, 0.14]\nImageNet\nResNet50\n2.31×\n75.16\n[0, 0.60, 0.60, 0.60, 0.21]\nImageNet\nResNet50\n2.56×\n74.75\n[0, 0.74, 0.74, 0.60, 0.21]\nImageNet\nResNet50\n3.06×\n73.50\n[0, 0.68, 0.68, 0.68, 0.50]\n* In addition to the pruning ratios, several layers are skipped, following the setting of L1 (pruned-B) Li\net al. (2017). Speciﬁcally, we refer to the implementation of Liu et al. (2019) at https://github.com/Eric-\nmingjie/rethinking-network-pruning/tree/master/imagenet/l1-norm-pruning.\nMeanwhile, combine Eq. 10 and Eq. 11, we can obtain\nˆE = (w −w∗)T H(w −w∗) + δλwT Iw + C.\n(13)\nCompare Eq. 13 with Eq. 12, we have\n(H + δλI)ˆw∗= Hw∗⇒ˆw∗= (H + δλI)−1Hw∗.\n(14)\nC",
            "Meanwhile, combine Eq. 10 and Eq. 11, we can obtain\nˆE = (w −w∗)T H(w −w∗) + δλwT Iw + C.\n(13)\nCompare Eq. 13 with Eq. 12, we have\n(H + δλI)ˆw∗= Hw∗⇒ˆw∗= (H + δλI)−1Hw∗.\n(14)\nC\nPROOF OF EQ. 7\nˆH =\n\u001a\nh11 + δλ\nh12\nh12\nh22 + δλ\n\u001b\n⇒ˆH−1 =\n1\n| ˆH|\n\u001a\nh22 + δλ\n−h12\n−h12\nh11 + δλ\n\u001b\n(15)\nTherefore, ˆw∗= ˆH−1Hw∗⇒\n\u001a\nˆw∗\n1\nˆw∗\n2\n\u001b\n= ˆH−1H\n\u001a\nw∗\n1\nw∗\n2\n\u001b\n=\n1\n| ˆH|\n\u001a\nh22 + δλ\n−h12\n−h12\nh11 + δλ\n\u001b \u001a\nh11\nh12\nh12\nh22\n\u001b \u001a\nw∗\n1\nw∗\n2\n\u001b\n=\n1\n| ˆH|\n\u001a\n(h11h22 + h11δλ −h2\n12)w∗\n1 + δλh12w∗\n2\n(h11h22 + h22δλ −h2\n12)w∗\n2 + δλh12w∗\n1\n\u001b\n.\n(16)\nD\nGREG-1 + OBD\nIn Sec. 4.1, we show when pruning the same weights, GReg-1 is signiﬁcantly better than the one-\nshot counterpart, where the pruned weights are selected by the L1-norm criterion. Here we conduct\nthe same comparison just with a different pruning criterion introduced in OBD LeCun et al. (1990).\nOBD is also an one-shot pruning method, using a Hessian-based criterion which is believed to be\nmore advanced than L1-norm.",
            "OBD is also an one-shot pruning method, using a Hessian-based criterion which is believed to be\nmore advanced than L1-norm.\nResults are shown in Tab. 7. As seen, using this more advanced importance criterion, our pruning\nscheme based on growing regularization is still consistently better than the one-shot counterpart.\nBesides, it is also veriﬁed here that a better pruning schedule can bring more accuracy gain when\nthe speedup is larger.\nE\nFILTER L1-NORM CHANGE OF VGG19\nIn Fig. 1 (Row 2), we plot the ﬁlter L1-norm change over time for ResNet50 on ImageNet. Here we\nplot the case of VGG19 on CIFAR100 to show the weight separation phenomenon under growing\nregularization is a general one across different datasets and networks.\nF\nHYPER-PARAMETERS AND SENSITIVITY ANALYSIS\nThere are ﬁve introduced values in our methods: regularization ceiling τ, ceiling for picking τ ′,\ninterval Ku, Ks, granularity δλ. Their settings are summarized in Tab. 8. Among them, the ceilings\n14",
            "Published as a conference paper at ICLR 2021\nTable 7: Comparison between pruning schedules: one-shot pruning vs. our proposed GReg-1 using\nthe Hessian-based criterion introduced in OBD LeCun et al. (1990). Each setting is randomly run\nfor 3 times, mean and std accuracies reported. We vary the global pruning ratio from 0.7 to 0.95\nso as to cover the major speedup spectrum of interest. Same as Tab. 1, the pruned weights here are\nexactly the same for the two methods under each speedup ratio. The ﬁnetuning processes (number\nof epochs, LR schedules, etc.) are also the same to keep fair comparison.\nResNet56 + CIFAR10: Baseline accuracy 93.36%, #Params: 0.85M, FLOPs: 0.25G\nSpeedup\n2.15×\n3.00×\n4.86×\n5.80×\n6.87×\nAcc. (%, OBD)\n92.90 (0.05)\n91.90 (0.04)\n89.82 (0.11)\n88.56 (0.11)\n86.90 (0.03)\nAcc. (%, Ours)\n92.94 (0.12)\n92.27 (0.14)\n90.37 (0.17)\n89.78 (0.06)\n88.69 (0.06)\nAcc. gain (%)\n0.04\n0.37\n0.55\n1.22\n1.79\nVGG19 + CIFAR100: Baseline accuracy 74.02%, #Params: 20.08M, FLOPs: 0.80G\nSpeedup\n1.92×",
            "92.94 (0.12)\n92.27 (0.14)\n90.37 (0.17)\n89.78 (0.06)\n88.69 (0.06)\nAcc. gain (%)\n0.04\n0.37\n0.55\n1.22\n1.79\nVGG19 + CIFAR100: Baseline accuracy 74.02%, #Params: 20.08M, FLOPs: 0.80G\nSpeedup\n1.92×\n2.96×\n5.89×\n7.69×\n11.75×\nAcc. (%, OBD)\n72.68 (0.08)\n70.42 (0.16)\n62.54 (0.13)\n59.18 (0.32)\n54.19 (0.57)\nAcc. (%, Ours)\n73.08 (0.11)\n71.30 (0.28)\n65.83 (0.13)\n62.87 (0.20)\n59.53 (0.10)\nAcc. gain (%)\n0.40\n0.88\n3.29\n3.69\n5.34\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n0\n20\n40\n60\n80\n100\n120\nFilter index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized L1-norm\n(a) Iter 0\n(b) Iter 10,000\n(c) Iter 20,000\n(d) Iter 30,000\nFigure 2: Normalized ﬁlter L1-norm over iterations for VGG19 layer3.\nare set through validation: τ = 1 is set to make sure the unimportant weights are pushed down",
            "(c) Iter 20,000\n(d) Iter 30,000\nFigure 2: Normalized ﬁlter L1-norm over iterations for VGG19 layer3.\nare set through validation: τ = 1 is set to make sure the unimportant weights are pushed down\nenough (as stated in the main paper, normally after the regularization training, their magnitudes are\ntoo small to cause signiﬁcant accuracy degradation if they are completely removed). τ ′ = 0.01 is\nset generally for the same goal as τ, but since it is applied to all the weight (not just the unimportant\nones), we only expect it to be moderately large (thus smaller than τ) so that the important and unim-\nportant can be differentiated with a clear boundary. For the δλ, we use a very small regularization\ngranularity δλ, which our theoretical analysis is based on. We set its value to 1e-4 for GReg-1 and\n1e-5 for GReg-2 with reference to the original weight decay value 5×10−4 (for CIFAR models) and\n10−4 (for ImageNet models). Note that, these values come from our methods per se, not directly",
            "1e-5 for GReg-2 with reference to the original weight decay value 5×10−4 (for CIFAR models) and\n10−4 (for ImageNet models). Note that, these values come from our methods per se, not directly\nrelated to datasets and networks, thus are invariant to them. This is why we can employ the same set-\nting of these three hyper-parameters in all our experiments, freeing practitioners from heavy tuning\nwhen dealing with different networks or datasets.\nTable 8: Hyper-parameters of our methods.\nNotation\nDefault value (CIFAR)\nDefault value (ImageNet)\nδλ\nGReg-1: 1e-4, GReg-2: 1e-5\nτ\n1\nτ ′\n0.01\nKu\n10 iterations\n5 iterations\nKs\n5k iterations\n40k iterations\nA little bit of change is for Ku, Ks. Both are generally to let the network have enough time to\nconverge to the new equilibrium. Generally, we prefer large update intervals, yet we also need\nto consider the time complexity: Too large of them will bring too many iterations, which may",
            "converge to the new equilibrium. Generally, we prefer large update intervals, yet we also need\nto consider the time complexity: Too large of them will bring too many iterations, which may\nbe unnecessary. Among them, Ks is less important since it is to stabilize the large regularization\n(τ = 1). We introduce it simply to make sure the training is fully converged. Therefore, the possibly\nmore sensitive hyper-parameter is the Ku (set to 5 for ImageNet and 10 for CIFAR). Here we will\nshow the performance is insensitive to the varying Ku. As shown in Tab. 9, the peak performance\nappears at around Ku = 15 for ResNet56 and Ku = 10 for VGG19. We simply adopt 10 for a\nuniform setting in our paper. We did not heavily tune these hyper-parameters, yet as seen, they work\npretty well across different networks and datasets. Notably, even for the worst cases in Tab. 9 (in\n15",
            "Published as a conference paper at ICLR 2021\nblue color), they are still signiﬁcantly better than those of the “L1+one-shot” scheme, demonstrating\nthe robustness of the proposed algorithm.\nTable 9: Sensitivity analysis of Ku on CIFAR10/100 datasets with the proposed GReg-1 algorithm.\nKu = 10 is the default setting. Pruning ratio 90% (ResNet56) and 70% (VGG19) are explored here.\nExperiments are randomly run for 3 times with mean accuracy and standard deviation reported. The\nbest is highlighted with bold and the worst is highlighted with blue color.\nKu\n1\n5\n10\n15\n20\nL1+one-shot\nAcc. (%, ResNet56)\n89.40±0.04\n89.38±0.13\n89.49±0.23\n89.69±0.05\n89.62±0.13\n87.34±0.21\nAcc. (%, VGG19)\n67.22±0.33\n67.32±0.24\n67.35±0.15\n67.06±0.40\n66.93±0.22\n66.05±0.04\nG\nMORE RESULTS OF PRUNING SCHEDULE COMPARISON\nIn Tab. 1, we show using L1-norm sorting, our proposed GReg-1 can consistently surpass the one-\nshot schedule even pruning the same weights. Here we ask a more general question: Can the beneﬁts",
            "In Tab. 1, we show using L1-norm sorting, our proposed GReg-1 can consistently surpass the one-\nshot schedule even pruning the same weights. Here we ask a more general question: Can the beneﬁts\nfrom a regularization-based schedule consistently appear, agnostic to the weight importance scoring\ncriterion? This question is important because it will show if the gain from a better pruning schedule\nis only a bonus concurrent with the L1 criterion or a really universal phenomenon. Since there are\nliterally so many weight importance criteria, we cannot ablate them one by one. Nevertheless, given\na pre-trained model and a pruning ratio r, no matter what criterion, its role is to select a ﬁlter subset.\nFor example, if there are 100 ﬁlters in a layer and r = 0.5, then they are at most\n\u0000100\n50\n\u0001\nimportance\ncriteria in theory for this layer. We can simply randomly pick a subset of ﬁlters (which corresponds",
            "\u0000100\n50\n\u0001\nimportance\ncriteria in theory for this layer. We can simply randomly pick a subset of ﬁlters (which corresponds\nto certain criterion, albeit unknown) and compare the one-shot way with regularization-based way\non the subset. Based on this idea, we conduct ﬁve random runs on the ResNet56 and VGG19 to\nexplore this. The pruning ratio is chosen as 90% for ResNet56 and 70% for VGG19 because under\nthis ratio the compression (or acceleration) ratio is about 10 times, neither too large nor too small\n(where the network can heal itself regardless of pruning methods).\nThe results are shown in Tab. 10. Here is a sanity check: Compared with Tab. 1, the mean accuracy\nof pruning randomly picked ﬁlters should be less than pruning those picked by L1-norm, conﬁrmed\nby 86.85% vs. 87.34% for ResNet56 and 65.04% vs. 66.05% for VGG19. As seen, in each run,\nthe regularization-based way also signiﬁcantly surpasses its one-shot counterpart. Although ﬁve",
            "by 86.85% vs. 87.34% for ResNet56 and 65.04% vs. 66.05% for VGG19. As seen, in each run,\nthe regularization-based way also signiﬁcantly surpasses its one-shot counterpart. Although ﬁve\nrandom runs are still too few given the exploding potential combinations, yet as shown by the ac-\ncuracy standard deviations, the results are stable and thus qualiﬁed to support our ﬁnding that the\nregularization-based pruning schedule is better to the one-shot counterpart.\nTable 10: Comparison between pruning schedules: one-shot vs. GReg-1. Pruning ratio is 90% for\nResNet56 and 70% for VGG19. In each run, the weights to prune are picked randomly before the\ntraining starts.\nResNet56 + CIFAR10\nRun #1\nRun #2\nRun #3\nRun #4\nRun #5\nMean±std\nAcc. (%, one-shot)\n87.57\n87.00\n86.27\n86.75\n86.67\n86.85±0.43\nAcc. (%, GReg-1, ours)\n89.26\n88.98\n88.78\n89.42\n88.96\n89.08±0.23\nVGG19 + CIFAR100\nRun #1\nRun #2\nRun #3\nRun #4\nRun #5\nMean±std\nAcc. (%, one-shot)\n64.56\n65.06\n65.07\n65.05\n65.48\n65.04±0.29\nAcc. (%, GReg-1, ours)",
            "89.26\n88.98\n88.78\n89.42\n88.96\n89.08±0.23\nVGG19 + CIFAR100\nRun #1\nRun #2\nRun #3\nRun #4\nRun #5\nMean±std\nAcc. (%, one-shot)\n64.56\n65.06\n65.07\n65.05\n65.48\n65.04±0.29\nAcc. (%, GReg-1, ours)\n66.63\n66.57\n66.80\n66.80\n67.16\n66.79±0.21\n16"
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 1,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 1,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 1,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 1,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 2,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 3,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 3,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 3,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 3,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 3,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 4,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 5,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 5,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 5,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 5,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 5,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 6,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 7,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 8,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 8,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 8,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 8,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 8,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 9,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 9,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 9,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 9,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 9,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 10,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 10,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 10,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 10,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 11,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 11,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 11,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 11,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 12,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 12,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 12,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 12,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 13,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 13,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 13,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 13,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 14,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 14,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 14,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 15,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 15,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 15,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 15,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 15,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 16,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 16,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 16,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 16,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            },
            {
                "page": 16,
                "source": "NEURAL PRUNING VIA GROWING REGULARIZATION.pdf",
                "title": "Published as a conference paper at ICLR 2021",
                "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu"
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25",
            "chunk_26",
            "chunk_27",
            "chunk_28",
            "chunk_29",
            "chunk_30",
            "chunk_31",
            "chunk_32",
            "chunk_33",
            "chunk_34",
            "chunk_35",
            "chunk_36",
            "chunk_37",
            "chunk_38",
            "chunk_39",
            "chunk_40",
            "chunk_41",
            "chunk_42",
            "chunk_43",
            "chunk_44",
            "chunk_45",
            "chunk_46",
            "chunk_47",
            "chunk_48",
            "chunk_49",
            "chunk_50",
            "chunk_51",
            "chunk_52",
            "chunk_53",
            "chunk_54",
            "chunk_55",
            "chunk_56",
            "chunk_57",
            "chunk_58",
            "chunk_59",
            "chunk_60",
            "chunk_61",
            "chunk_62",
            "chunk_63",
            "chunk_64",
            "chunk_65",
            "chunk_66",
            "chunk_67",
            "chunk_68",
            "chunk_69",
            "chunk_70",
            "chunk_71",
            "chunk_72",
            "chunk_73",
            "chunk_74",
            "chunk_75",
            "chunk_76",
            "chunk_77",
            "chunk_78"
        ]
    },
    "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf": {
        "chunks": [
            "DepGraph: Towards Any Structural Pruning\nGongfan Fang1\nXinyin Ma1\nMingli Song2\nMichael Bi Mi3\nXinchao Wang1*\nNational University of Singapore1\nZhejiang University2\nHuawei Technologies Ltd.3\ngongfan@u.nus.edu, maxinyin@u.nus.edu, xinchao@nus.edu.sg\nhttps://github.com/VainF/Torch-Pruning\nAbstract\nStructural pruning enables model acceleration by re-\nmoving structurally-grouped parameters from neural net-\nworks.\nHowever, the parameter-grouping patterns vary\nwidely across different models,\nmaking architecture-\nspecific pruners, which rely on manually-designed grouping\nschemes, non-generalizable to new architectures. In this\nwork, we study a highly-challenging yet barely-explored\ntask, any structural pruning, to tackle general structural\npruning of arbitrary architecture like CNNs, RNNs, GNNs\nand Transformers. The most prominent obstacle towards\nthis goal lies in the structural coupling, which not only\nforces different layers to be pruned simultaneously, but also",
            "and Transformers. The most prominent obstacle towards\nthis goal lies in the structural coupling, which not only\nforces different layers to be pruned simultaneously, but also\nexpects all removed parameters to be consistently unimpor-\ntant, thereby avoiding structural issues and significant per-\nformance degradation after pruning. To address this prob-\nlem, we propose a general and fully automatic method, De-\npendency Graph (DepGraph), to explicitly model the depen-\ndency between layers and comprehensively group coupled\nparameters for pruning. In this work, we extensively evalu-\nate our method on several architectures and tasks, including\nResNe(X)t, DenseNet, MobileNet and Vision transformer\nfor images, GAT for graph, DGCNN for 3D point cloud,\nalongside LSTM for language, and demonstrate that, even\nwith a simple norm-based criterion, the proposed method\nconsistently yields gratifying performances.\n1. Introduction\nThe recent emergence of edge computing applications",
            "with a simple norm-based criterion, the proposed method\nconsistently yields gratifying performances.\n1. Introduction\nThe recent emergence of edge computing applications\ncalls for the necessity for deep neural compression [16,22,\n25,33,34,61,65–67,69,75]. Among the many network com-\npression paradigms, pruning has proven itself to be highly\neffective and practical [7, 11, 30, 31, 44, 58, 59, 74]. The\ngoal of network pruning is to remove redundant parame-\nters from a given network to lighten its size and potentially\nspeed up the inference. Mainstream pruning approaches can\n*Corresponding author\nNorm\nMulti-Head\nAttention\nNorm\nMLP\n+\n+\nConv1\nBN1\nReLU\n+\nConv2\nBN2\nReLU\n(a) CNNs\n(b) Transformers\n𝜎\n𝜎\ntanh\n𝜎\n×\n×\n×\n+\ntanh\n(c) RNNs\n(d) GNNs\nGNN\nLayer\nFigure 1. Parameters from different layers are inherently depen-\ndent on each other across network architectures, which forces sev-\neral layers to be pruned simultaneously. For instance, to prune",
            "Figure 1. Parameters from different layers are inherently depen-\ndent on each other across network architectures, which forces sev-\neral layers to be pruned simultaneously. For instance, to prune\nthe Conv2 in (a), all other layers {Conv1, BN1, BN2} within the\nblock must be pruned as well. In this work, we introduce a generic\nscheme, termed as Dependency Graph, to explicitly account for\nsuch dependencies and execute the pruning of arbitrary architec-\nture in a fully automatic manner.\nbe roughly categorized into two schemes, structurual prun-\ning [4, 29, 71] and unstructurual pruning [8, 13, 44]. The\ncore difference between the two lies in that, structural prun-\ning changes the structure of neural networks by physically\nremoving grouped parameters, while unstructural pruning\nconducts zeroing on partial weights without modification\nto the network structure. Compared to unstructural ones,\nstructural pruning does not rely on specific AI accelerators",
            "conducts zeroing on partial weights without modification\nto the network structure. Compared to unstructural ones,\nstructural pruning does not rely on specific AI accelerators\nor software to reduce memory consumption and computa-\ntional costs, thereby finding a wider domain of applications\nin practice [38,68].\nNevertheless, the nature of structural pruning per se\nmakes itself a challenging task, especially for modern deep\nneural networks with coupled and complex internal struc-\ntures. The rationale lies in that, deep neural networks are\nbuilt upon a large number of basic modules like convolu-\ntion, normalization, or activation, yet these modules, either\nparameterized or not, are intrinsically coupled through the\nintricate connections [17, 23]. As a result, even when we\nseek to remove only one channel from a CNN illustrated in\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;",
            "This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n16091",
            "Figure 1(a), we have to take care of its inter-dependencies\nto all layers simultaneously, otherwise we will eventually\nget a broken network. To be exact, the residual connection\nrequires the output of two convolutional layers to share the\nsame number of channels and thus forces them to be pruned\ntogether [20, 40, 71]. The same goes for structural pruning\non other architectures like Transformers, RNNs and GNNs\nas illustrated in Figs. 1(b-d).\nUnfortunately, dependency does not only emerge in\nresidual structures, which can be infinitely complex in mod-\nern models [23, 46]. Existing structural approaches have\nlargely relied on case-by-case analyses to handle depen-\ndencies in networks [29, 40].\nDespite the promising re-\nsults achieved, such a network-specific pruning approach is\neffort-consuming. Moreover, these methods are not directly\ngeneralizable, meaning that the manually-designed group-\ning scheme is not transferable to other network families or",
            "effort-consuming. Moreover, these methods are not directly\ngeneralizable, meaning that the manually-designed group-\ning scheme is not transferable to other network families or\neven the network architectures in the same family, which in\nturn, greatly limit their industrial applications.\nIn this paper, we strive for a generic scheme towards any\nstructural pruning, where structural pruning over arbitrary\nnetwork architectures is executed in an automatic fashion,\nAt the heart of our approach is to estimate the Dependency\nGraph (DepGraph), which explicitly models the interdepen-\ndency between paired layers in neural networks. Our moti-\nvation to introduce DepGraph for structural pruning stems\nfrom the observation that, structural pruning at one layer ef-\nfectively “triggers” pruning at adjacent layers, which further\nleads to a chain effect like {BN2←Conv2→BN1→Conv1}\nas shown in Figure 1(a). As such, to trace the dependen-\ncies across different layers, we may decompose and model",
            "leads to a chain effect like {BN2←Conv2→BN1→Conv1}\nas shown in Figure 1(a). As such, to trace the dependen-\ncies across different layers, we may decompose and model\nthe dependency chain as a recursive process, which natu-\nrally boils down to the problem of finding the maximum\nconnected components in the graph, and can be solved in\nO(N) complexity via graph traversal.\nIt is also worth noting that in structural pruning, grouped\nlayers are pruned simultaneously, which expects all re-\nmoved parameters in the same group to be consistently\nunimportant. This brings certain difficulties to existing im-\nportance criteria designed for a single layer [20,27,29,42].\nTo be exact, the parameter importance in a single layer\nno longer reveals correct importance due to the entangle-\nment with other parameterized layers. To address this prob-\nlem, we fully leverage the comprehensive ability of depen-\ndency modeling powered by DepGraph to design a “group-",
            "ment with other parameterized layers. To address this prob-\nlem, we fully leverage the comprehensive ability of depen-\ndency modeling powered by DepGraph to design a “group-\nlevel” importance criterion, which learns consistent sparsity\nwithin groups, so that those zeroized groups can be safely\nremoved without too much performance degradation.\nTo validate the effectiveness of DepGraph, we apply the\nproposed method to several popular architectures includ-\ning CNNs [23, 40], Transformers [10], RNNs [12, 53], and\nGNNs [55], where competitive performance is achieved\ncompared to state-of-the-art methods [7, 32, 58, 71]. For\nCNN pruning, our method obtains a 2.57× accelerated\nResNet-56 model with 93.64% accuracy on CIFAR, which\nis even superior to the unpruned model with 93.53% ac-\ncuracy.\nAnd on ImageNet-1k, our algorithm achieves\nmore than 2× speed-up on ResNet-50, with only 0.32%\nperformance lost. More importantly, our method can be\nreadily transferred to various popular networks, includ-",
            "more than 2× speed-up on ResNet-50, with only 0.32%\nperformance lost. More importantly, our method can be\nreadily transferred to various popular networks, includ-\ning ResNe(X)t [40, 63], DenseNet [23], VGG [51], Mo-\nbileNet [48], GoogleNet [54] and Vision Transformer [10],\nand demonstrate gratifying results. Besides, we also con-\nduct further experiments on non-image neural networks, in-\ncluding LSTM [12] for text classification, DGCNN [60] for\n3D point cloud, and GAT [55] for graph data, where our\nmethod achieves from 8× to 16× acceleration without a\nsignificant performance drop.\nIn sum, our contribution is a generic pruning scheme to-\nwards any structural pruning, termed as Dependency Graph\n(DepGraph), which allows for automatic parameter group-\ning and effectively improves the generalizability of struc-\ntural pruning over various network architectures, including\nCNNs, RNNs, GNNs and Vision Transformers.\n2. Related Work\nStructural and Unstructural Pruning.\nPruning has",
            "tural pruning over various network architectures, including\nCNNs, RNNs, GNNs and Vision Transformers.\n2. Related Work\nStructural and Unstructural Pruning.\nPruning has\nmade tremendous progress in the field of network ac-\nceleration, as evidenced by various studies in the litera-\nture [2, 19–21, 29, 35, 39]. Mainstream pruning methods\ncan be broadly categorized into two types: structural prun-\ning [4,29,32,71,71] and unstructural pruning [8,28,44,49].\nStructural pruning aims to physically remove a group of pa-\nrameters, thereby reducing the size of neural networks. In\ncontrast, unstructured pruning involves zeroing out specific\nweights without altering the network structure.\nIn prac-\ntice, unstructural pruning, in particular, is straightforward\nto implement and inherently adaptable to various networks.\nhowever, it often necessitates specialized AI accelerators or\nsoftware for model acceleration [15].\nConversely, struc-\ntural pruning improves the inference overhead by physi-",
            "however, it often necessitates specialized AI accelerators or\nsoftware for model acceleration [15].\nConversely, struc-\ntural pruning improves the inference overhead by physi-\ncally removing parameters from networks, thereby finding a\nwider domain of applications [29,38]. In the literature, The\ndesign space of pruning algorithms encompasses a range\nof aspects, including pruning schemes [21, 39], parameter\nselection [20, 43, 44], layer sparsity [27, 49] and training\ntechniques [47, 58]. In recent years, numerous robust cri-\nteria have been introduced, such as magnitude-based cri-\nteria [20, 27, 70] and gradient-based criteria [32, 37] and\nlearned sparsity [7, 35]. Recently, a comprehensive study\nhas also been conducted to assess the efficacy of various\ncriteria and provide a fair benchmark [57].\nPruning Grouped Parameters.\nIn complex network\nstructures [29,32,38,71,77], dependencies can arise among\ngroups of parameters, necessitating their simultaneous\n16092",
            "Residual Connection\n(a) Basic dependency\n(b) Residual dependency\n(c) Concatenation dependency\nConcatenation\n(d) Reduction dependency\n∑\n𝑤!\n𝑤!\"#\n𝑤!\"$\n𝑤!\n𝑤!\"#\n𝑤!\"$\n𝑤!\n𝑤\"#\n𝑤\"$\n𝑤!\nFigure 2. Grouped parameters with inter-dependency in different structures. All highlighted parameters must be pruned simultaneously.\npruning. The pruning of coupled parameters has been a\nfocus of research since the early days of structural prun-\ning [29, 35, 39]. For instance, when pruning two succes-\nsive convolutional layers, removing a filter from the first\nlayer necessitates the pruning of associated kernels in the\nsubsequent layer [29]. Although it is feasible to analyze pa-\nrameter dependencies manually, this process can be exceed-\ningly labor-intensive when applied to complicated networks\nas illustrated in many prior studies [29, 71, 77]. Further-\nmore, such manual schemes are inherently non-transferable\nto novel architectures, which severely restricts the applica-",
            "as illustrated in many prior studies [29, 71, 77]. Further-\nmore, such manual schemes are inherently non-transferable\nto novel architectures, which severely restricts the applica-\ntions of pruning. Recently, some pilot works have been pro-\nposed to decipher the complex relationships between lay-\ners [32,71]. Unfortunately, existing techniques still depend\non empirical rules or predefined architectural patterns, ren-\ndering them insufficiently versatile for all structural pruning\napplications. In this study, we present a general approach\nto resolve this challenge, demonstrating that addressing pa-\nrameter dependency effectively generalizes structural prun-\ning across a wide array of networks, resulting in satisfactory\nperformance on several tasks.\n3. Method\n3.1. Dependency in Neural Networks\nIn this work, we focus on structural pruning of any neu-\nral networks under the restriction of parameter dependency.\nWithout loss of generality, we develop our method upon",
            "In this work, we focus on structural pruning of any neu-\nral networks under the restriction of parameter dependency.\nWithout loss of generality, we develop our method upon\nfully-connected (FC) layers. Let’s begin with a linear neural\nnetwork composed of three consecutive layers as illustrated\nin Figure 2 (a), parameterized by 2-D weight matrices wl,\nwl+1 and wl+2 respectively. This simple neural network can\nbe made slim by structural pruning via the removal of neu-\nrons. In this case, it is easy to find that some dependencies\nemerge between parameters, denoted as wl ⇔wl+1, which\nforces wl and wl+1 to be simultaneously pruned. Specif-\nically, to prune the k-th neuron that bridges wl and wl+1,\nboth wl[k, :] and wl+1[:, k] will be removed.\nIn the lit-\nerature, researchers handle layer dependencies and enable\nstructural pruning on deep neural networks with manually-\ndesigned and model-specific schemes [21, 29]. Neverthe-\nless, there are many kinds of dependencies just as illustrated",
            "structural pruning on deep neural networks with manually-\ndesigned and model-specific schemes [21, 29]. Neverthe-\nless, there are many kinds of dependencies just as illustrated\nin Figure 2 (b-d). It is intractable to manually analyze all\ndependencies in a case-by-case manner, let alone that sim-\nple dependencies can be nested or composed to form more\ncomplex patterns. To address the dependency issue in struc-\ntural pruning, we introduce Dependency Graph in this work,\nwhich provides a general and fully-automatic mechanism\nfor dependency modeling.\n3.2. Dependency Graph\nGrouping.\nTo enable structural pruning, we first need to\ngroup layers according to their inter-dependency. Formally,\nour goal is to find a grouping matrix G ∈RL×L where L\nrefers to the number of layers in a to-be-pruned network,\nand Gij = 1 indicates the presence of dependency between\ni-th layer and j-th layer. We let Diag(G) = 11×L to en-\nable self-dependency for convenience. With the grouping",
            "and Gij = 1 indicates the presence of dependency between\ni-th layer and j-th layer. We let Diag(G) = 11×L to en-\nable self-dependency for convenience. With the grouping\nmatrix, it is straightforward to find all coupled layers with\ninter-dependency to the i-th layer, denoted as g(i):\ng(i) = {j|Gij = 1}\n(1)\nNevertheless, it is non-trivial to estimate the grouping pat-\nterns from a neural network due to the fact that modern deep\nnetworks may consist of thousands of layers with compli-\ncated connections, resulting in a large and intricate group-\ning matrix G. In this matrix, Gij is not only determined by\nthe i-th and j-th layers but also affected by those intermedi-\nate layers between them. Thus, such non-local and implicit\nrelations can not be handled with simple rules in most cases.\nTo overcome this challenge, we do not directly estimate the\ngrouping matrix G, and propose an equivalent but easy-to-\nestimate method for dependency modeling, namely the De-",
            "To overcome this challenge, we do not directly estimate the\ngrouping matrix G, and propose an equivalent but easy-to-\nestimate method for dependency modeling, namely the De-\npendency Graph, from which G can be efficiently derived.\nDependency Graph.\nLet us begin by considering a group\ng = {w1, w2, w3}, which has dependencies w1 ⇔w2,\nw2 ⇔w3, and w1 ⇔w3. Upon closer inspection of this\ndependency modeling, we can observe that there is some re-\ndundancy present. For example, the dependency w1 ⇔w3\ncan be derived from w1 ⇔w2 and w2 ⇔w3 through\na recursive process. Initially, we take w1 as the starting\npoint and examine its dependency on other layers, such as\n16093",
            "Conv 𝑓!\nBN 𝑓\"\nReLU 𝑓#\n+\nConv 𝑓$\nBN 𝑓%\nReLU 𝑓&\n(a) CNNs\nAdd 𝑓'\n𝑓!\n(\n𝑓!\n)\n𝑓\"\n(\n𝑓\"\n)\n𝑓#\n(\n𝑓#\n)\n𝑓$\n(\n𝑓$\n)\n𝑓%\n(\n𝑓%\n)\n𝑓&\n(\n𝑓&\n)\n𝑓'\n(\n𝑓'\n)\nSucceeding\nLayers\nPreceding\nLayers\n(b) Propagation on Dependency Graph\n𝑠𝑐ℎ𝑓!\" ≠𝑠𝑐ℎ(𝑓!#)\n𝑠𝑐ℎ𝑓$\" = 𝑠𝑐ℎ(𝑓$#)\n1\n0\n2\n3\n0 1 2 3\n1\n0\n2\n3\n1\n0\n2\n3\n1\n0\n2\n3\n1\n0\n2\n3\n0 1 2 3\n1\n0\n2\n3\nFigure 3. Layer grouping is achieved via a recursive propagation\non DepGraph, starting from the f +\n4 . In this example, there is no\nIntra-layer Dependency between convolutional input f −\n4 and out-\nput f +\n4 due to the diverged pruning schemes illustrated above.\nw1 ⇔w2. Then, w2 provides a new starting point for recur-\nsively expanding the dependency, which in turn ”triggers”\nw2 ⇔w3. This recursive process ultimately ends with a\ntransitive relation, w1 ⇔w2 ⇔w3. In this case, we only\nneed two dependencies to describe the relations in group g.\nSimilarly, the grouping matrix discussed in Section 3.2 is\nalso redundant for dependency modeling and thus can be",
            "need two dependencies to describe the relations in group g.\nSimilarly, the grouping matrix discussed in Section 3.2 is\nalso redundant for dependency modeling and thus can be\ncompressed into a more compact form with fewer edges\nwhile retaining the same information. We demonstrate that\na new graph D that measures the local inter-dependency be-\ntween adjacent layers, named Dependency Graph, can be\nan effective reduction for the grouping matrix G. The De-\npendency Graph differs from G in that it only records the\ndependencies between adjacent layers with direct connec-\ntions. The Graph D can be viewed as the transitive reduc-\ntion [1] of G, which contains the same vertices as G but\nwith as few edges as possible. Formally, D is constructed\nsuch that, for all Gij = 1, there exists a path in D between\nvertex i and j. Therefore, Gij can be derived by examing\nthe presence of a path between vertices i and j in D.\nNetwork Decomposition.\nHowever, we find that building",
            "vertex i and j. Therefore, Gij can be derived by examing\nthe presence of a path between vertices i and j in D.\nNetwork Decomposition.\nHowever, we find that building\nthe dependency graph at the layer level can be problematic\nin practice, since some basic layers such as fully-connected\nlayers may have two different pruning schemes like w[k, :]\nand w[:, k] as discussed in Section 3.1, which compress the\ndimensions of inputs and outputs respectively. Besides, net-\nworks also contain non-parameterized operations such as\nskip connections, which also affect the dependency between\nlayers [40].\nTo remedy these issues, we propose a new\nnotation to decompose a network F(x; w) into finer and\nmore basic components, denoted as F = {f1, f2, ..., fL},\nwhere each component f refers to either a parameterized\nlayer such as convolution or a non-parameterized operation\nsuch as residual adding. Instead of modeling relationships\nat the layer level, we concentrate on the dependencies be-",
            "layer such as convolution or a non-parameterized operation\nsuch as residual adding. Instead of modeling relationships\nat the layer level, we concentrate on the dependencies be-\ntween the inputs and outputs of the layers. Specifically, we\ndenote the input and output of component fi as f −i and\nf +i, respectively. For any network, the final decomposi-\ntion can be formalized as F = {f −\n1 , f +\n1 , ..., f −\nL , f +\nL }. This\nnotation facilitates easier dependency modeling and allows\ndifferent pruning schemes for the same layer.\nDependency Modeling.\nLeveraging this notation, we re-\nsketch the neural network as Equation 2, where two princi-\npal types of dependencies can be discerned, namely inter-\nlayer dependency and intra-layer dependency, as demon-\nstrated below:\n(f −\n1 , f +\n1 ) ↔(f −\n2\n|\n{z\n}\nInter-layer Dep\n, f +\n2 ) · · · ↔(f −\nL , f +\nL )\n|\n{z\n}\nIntra-layer Dep\n(2)\nThe symbol ↔signifies the connectivity between two adja-\ncent layers. Examination of these two dependencies yields",
            "|\n{z\n}\nInter-layer Dep\n, f +\n2 ) · · · ↔(f −\nL , f +\nL )\n|\n{z\n}\nIntra-layer Dep\n(2)\nThe symbol ↔signifies the connectivity between two adja-\ncent layers. Examination of these two dependencies yields\nstraightforward but general rules for dependency modeling:\n• Inter-layer Dependency: A dependency f −\ni\n⇔f +\nj con-\nsistently arises in connected layers where f −\ni ↔f +\nj .\n• Intra-layer Dependency: A dependency f −\ni\n⇔f +\ni exists\nif f −\ni\nand f +\ni\nshare identical pruning schemes, denoted\nby sch(f −\ni ) = sch(f +\ni ).\nFirst, the inter-layer dependency can be readily estimated\nif the topological structure of the network is known. For\nconnected layers with f −\ni ↔f +\nj , a dependency consistently\nexists, because f −\ni and f +\nj correspond to the same interme-\ndiate features of the network. The subsequent step involves\nelucidating the intra-layer dependency. An intra-layer de-\npendency necessitates that both the input and output of a\nsingle layer should be pruned simultaneously. Numerous",
            "elucidating the intra-layer dependency. An intra-layer de-\npendency necessitates that both the input and output of a\nsingle layer should be pruned simultaneously. Numerous\nnetwork layers satisfy this condition, such as batch normal-\nization, whose inputs and outputs share the same pruning\nscheme, denoted as sch(f −\ni ) = sch(f +\ni ), and thus will be\npruned simultaneously as illustrated in Figure 3. In con-\ntrast, layers like convolutions have distinct pruning schemes\nfor their inputs and outputs, i.e., w[:, k, :, :] ̸= w[k, :, :, :] as\nshown in Figure 3, resulting in sch(f −\ni ) ̸= sch(f +\ni ). In\nsuch instances, there is no dependency between the input\nand output of a convolution layer. Given the aforementioned\nrules, we can formally establish the dependency modeling\nas follows:\nD(f −\ni , f +\nj ) = 1\n\u0002\nf −\ni ↔f +\nj\n\u0003\n|\n{z\n}\nInter-layer Dep\n∨1\n\u0002\ni = j ∧sch(f −\ni ) = sch(f +\nj )\n\u0003\n|\n{z\n}\nIntra-layer Dep\n(3)\nwhere ∨and ∧refer to the logical “OR” and “AND” op-",
            "as follows:\nD(f −\ni , f +\nj ) = 1\n\u0002\nf −\ni ↔f +\nj\n\u0003\n|\n{z\n}\nInter-layer Dep\n∨1\n\u0002\ni = j ∧sch(f −\ni ) = sch(f +\nj )\n\u0003\n|\n{z\n}\nIntra-layer Dep\n(3)\nwhere ∨and ∧refer to the logical “OR” and “AND” op-\nerations, and 1 is a indicator function returning “True” is\n16094",
            "𝑤\n𝑤!\n(a) Unstructural Sparsity\n𝑤\n𝑤!\n(b) Structural but Inconsistent Sparsity\n𝑤\n𝑤!\n(c) Consistent Structural Sparsity\ndep\n||𝒘𝒊𝒋\n# ||𝒑≈𝟎\n||𝒘𝒊\n#||𝒑≈𝟎\n||𝒘𝒊||𝒑≠𝟎\ndep\n||𝒘𝒊\n#||𝒑≈𝟎\n||𝒘𝒊||𝒑≈𝟎\nFigure 4. Learning different sparsity schemes to estimate the importance of grouped parameters. Method (a) is used in unstructural pruning\nwhich only focuses on the importance of single weight. Method (b) learns structurally sparse layers [35], but ignores coupled weights in\nother layers. Our method as shown in (c) learns group sparsity which forces all coupled parameters to zero, so that they can be easily\ndistinguished by a simple magnitude method.\nAlgorithm 1: Dependency Graph\nInput: A neural network F(x; w)\nOutput: DepGraph D(F, E)\nf −= {f −\n1 , f −\n2 , ..., f −\nL } decomposed from the F\nf + = {f +\n1 , f +\n2 , ..., f +\nL } decomposed from the F\nInitialize DepGraph D = 02L×2L\nfor i = {0, 1, .., L} do\nfor j = {0, 1, .., L} do\nD(f −\ni , f +\nj ) = D(f +\nj , f −\ni ) =\n1\n\u0002\nf −\ni ↔f +\nj\n\u0003\n|\n{z\n}",
            "1 , f +\n2 , ..., f +\nL } decomposed from the F\nInitialize DepGraph D = 02L×2L\nfor i = {0, 1, .., L} do\nfor j = {0, 1, .., L} do\nD(f −\ni , f +\nj ) = D(f +\nj , f −\ni ) =\n1\n\u0002\nf −\ni ↔f +\nj\n\u0003\n|\n{z\n}\nInter-layer Dep\n∨1\n\u0002\ni = j ∧sch(f −\ni ) = sch(f +\nj )\n\u0003\n|\n{z\n}\nIntra-layer Dep\nreturn D\nthe condition holds. The first term examines the Inter-layer\nDependency caused by network connectivity, while the sec-\nond term examines the intra-layer dependency introduced\nby shared pruning schemes between layer inputs and out-\nputs. It is worth noting that, DepGraph is a symmetric ma-\ntrix with D(f −\ni , f +\nj ) = D(f +\nj , f −\ni ). As such, we can exam-\nine all input-output pairs to estimate the dependency graph.\nIn Figure 3, we visualize the DepGraph of a CNN block\nwith residual connections. Algorithm 1 and 2 summarize\nthe algorithms for dependency modeling and grouping.\n3.3. Group-level Pruning\nIn previous sections, we have developed a general\nmethodology for analyzing dependencies within neural net-",
            "the algorithms for dependency modeling and grouping.\n3.3. Group-level Pruning\nIn previous sections, we have developed a general\nmethodology for analyzing dependencies within neural net-\nworks, which naturally leads to a group-level pruning prob-\nlem.\nAssessing the importance of grouped parameters\nposes a significant challenge to pruning as it involves sev-\neral coupled layers. In this section, we leverage a simple\nnorm-based criterion [29] to establish a practical method\nfor group-level pruning.\nGiven a parameter group g =\n{w1, w2, ..., w|g|}, existing criteria like L2-norm impor-\ntance I(w) = ∥w∥2 can produce independent scores for\neach w ∈g.\nA natural way to estimate the group im-\nportance would be computing an aggregated score I(g) =\nP\nw∈g I(w).\nUnfortunately, importance scores indepen-\nAlgorithm 2: Grouping\nInput: DepGraph D(F, E)\nOutput: Groups G\nG = {}\nfor i = {1, 2, ..., 2 ∗∥F∥} do\ng = {i}\nrepeat\nUNSEEN = {1, 2, ..., 2 ∗∥F∥} −g\ng′ = {j ∈UNSEEN|∃k ∈g, Dkj = 1}\ng = g ∪g′",
            "Algorithm 2: Grouping\nInput: DepGraph D(F, E)\nOutput: Groups G\nG = {}\nfor i = {1, 2, ..., 2 ∗∥F∥} do\ng = {i}\nrepeat\nUNSEEN = {1, 2, ..., 2 ∗∥F∥} −g\ng′ = {j ∈UNSEEN|∃k ∈g, Dkj = 1}\ng = g ∪g′\nuntil g′ = ∅;\nG = G ∪{g}\nreturn G\ndently estimated on different layers are likely to be non-\nadditive and thus meaningless due to the divergence of dis-\ntributions and magnitudes. To make this simple aggrega-\ntion works for importance estimation, we propose a sparse\ntraining method to sparsify parameters at the group level\nas illustrated in Figure 4 (c), so that those zeroized groups\ncan be safely removed from the network. Specifically, for\neach parameters w with K prunable dimensions indexed by\nw[k], we introduce a simple regularization term for sparse\ntraining, defined as:\nR(g, k) =\nK\nX\nk=1\nγk · Ig,k =\nK\nX\nk=1\nX\nw∈g\nγk∥w[k]∥2\n2\n(4)\nwhere Ig,k = P\nw∈g ∥w[k]∥2\n2 represents the importance of\nthe k-th prunable dimensions, and γk refers to the shrinkage",
            "R(g, k) =\nK\nX\nk=1\nγk · Ig,k =\nK\nX\nk=1\nX\nw∈g\nγk∥w[k]∥2\n2\n(4)\nwhere Ig,k = P\nw∈g ∥w[k]∥2\n2 represents the importance of\nthe k-th prunable dimensions, and γk refers to the shrinkage\nstrength applied to those parameters. We use a controllable\nexponential strategy to determine the γk as follows:\nγk = 2α(Imax\ng\n−Ig,k)/(Imax\ng\n−Imin\ng )\n(5)\nwhere a normalized score is used to control the shrinkage\nstrength αk, varying within the range of\n\u0002\n20, 2α\u0003\n. In this\nwork, we use a constant hyper-parameter α = 4 for all ex-\nperiments. After sparse training, we further use a simple\n16095",
            "Model / Data\nMethod\nBase\nPruned\n∆Acc.\nSpeed Up\nResNet56\nCIFAR10\nNISP [74]\n-\n-\n-0.03\n1.76×\nGeometric [20]\n93.59\n93.26\n-0.33\n1.70×\nPolar [78]\n93.80\n93.83\n+0.03\n1.88×\nCP [29]\n92.80\n91.80\n-1.00\n2.00×\nAMC [19]\n92.80\n91.90\n-0.90\n2.00×\nHRank [31]\n93.26\n92.17\n-0.09\n2.00×\nSFP [18]\n93.59\n93.36\n-0.23\n2.11×\nResRep [7]\n93.71\n93.71\n+0.00\n2.12×\nOurs w/o SL\n93.53\n93.46\n-0.07\n2.11×\nOurs\n93.53\n93.77\n+0.24\n2.11×\nGBN ( [71])\n93.10\n92.77\n-0.33\n2.51×\nAFP ( [6])\n93.93\n92.94\n-0.99\n2.56×\nC-SGD ( [4])\n93.39\n93.44\n+0.05\n2.55×\nGReg-1 ( [58])\n93.36\n93.18\n-0.18\n2.55×\nGReg-2 ( [58])\n93.36\n93.36\n-0.00\n2.55×\nOurs w/o SL\n93.53\n93.36\n-0.17\n2.51×\nOurs\n93.53\n93.64\n+0.11\n2.57×\nVGG19\nCIFAR100\nOBD ( [56])\n73.34\n60.70\n-12.64\n5.73×\nOBD ( [56])\n73.34\n60.66\n-12.68\n6.09×\nEigenD ( [56])\n73.34\n65.18\n-8.16\n8.80×\nGReg-1 ( [58])\n74.02\n67.55\n-6.67\n8.84×\nGReg-2 ( [58])\n74.02\n67.75\n-6.27\n8.84×\nOurs w/o SL\n73.50\n67.60\n-5.44\n8.87×\nOurs\n73.50\n70.39\n-3.11\n8.92×\nTable 1. Pruning results on CIFAR-10 and CIFAR-100.",
            "GReg-1 ( [58])\n74.02\n67.55\n-6.67\n8.84×\nGReg-2 ( [58])\n74.02\n67.75\n-6.27\n8.84×\nOurs w/o SL\n73.50\n67.60\n-5.44\n8.87×\nOurs\n73.50\n70.39\n-3.11\n8.92×\nTable 1. Pruning results on CIFAR-10 and CIFAR-100.\nrelative score ˆIg,k = N · Ig,k/ P{TopN(Ig)} to identify\nand remove unimportant parameters. In the experiments\nsection, we show that such a simple pruning method, when\ncombined with consistent sparse training, can achieve com-\nparable performance to modern approaches.\n4. Experiments\n4.1. Settings\nThis paper focuses on classification tasks and conducts\nextensive experiments on a variety of datasets, such as\nCIFAR [26] and ImageNet [3] for image classification,\nPPI [14] for graph classification, ModelNet [62] for 3D\nclassification, and AGNews [76] for text classification. For\neach dataset, we evaluated our method on several popu-\nlar architectures, including ResNe(X)t [40, 63], VGG [51],\nDenseNet [23], MobileNet [48], GoogleNet [54], Vision\nTransformers [10], LSTM [12], DGCNNs [60], and Graph",
            "lar architectures, including ResNe(X)t [40, 63], VGG [51],\nDenseNet [23], MobileNet [48], GoogleNet [54], Vision\nTransformers [10], LSTM [12], DGCNNs [60], and Graph\nAttention Networks [55].\nTo conduct ImageNet experi-\nments, we use off-the-shelf models from Torchvision [41]\nas the original models. After pruning, All models will be\nfine-tuned following a similar protocol as the pre-training\nstage, with a smaller learning rate and fewer iterations.\n4.2. Results on CIFAR\nPerformance.\nCIFAR [26] is a tiny image dataset, which\nis widely used to verify the effectiveness of pruning al-\ngorithms.\nWe follow existing works [7, 58] to prune a\nResNet-56 on CIFAR-10, and a VGG network on CIFAR-\n100. As shown in Table 1. We report the accuracy of the\npruned models as well as their theoretical speedup ratios,\n0\n1\n2\n3\nGroup Norm\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nCount\n#0 w/ grouping\nOriginal\nSparse\n0.00\n0.25\n0.50\n0.75\n1.00\nGroup Norm\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nCount\n#22 w/ grouping\nOriginal\nSparse\n0.0\n0.5",
            "0\n1\n2\n3\nGroup Norm\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nCount\n#0 w/ grouping\nOriginal\nSparse\n0.00\n0.25\n0.50\n0.75\n1.00\nGroup Norm\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nCount\n#22 w/ grouping\nOriginal\nSparse\n0.0\n0.5\n1.0\nGroup Norm\n0\n2\n4\n6\n8\n10\nCount\n#29 w/ grouping\nOriginal\nSparse\n0\n1\n2\n3\nGroup Norm\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nCount\n#0 w/o grouping\nOriginal\nSparse\n0.00\n0.25\n0.50\n0.75\n1.00\nGroup Norm\n0\n2\n4\n6\n8\nCount\n#22 w/o grouping\nOriginal\nSparse\n0.00\n0.25\n0.50\n0.75\n1.00\nGroup Norm\n0\n2\n4\n6\n8\n10\nCount\n#29 w/o grouping\nOriginal\nSparse\nFigure 5. Histogram of group-level sparsity obtained by sparse\nlearning w/ and w/o grouping, which respectively correspond to\nthe strategy (c) and (b) in Figure.4.\ndefined as Speed Up =\nMACs(base)\nMACs(pruned). Note that baselines\nlike ResRep [7], GReg [58] also deploy sparse training for\npruning. A key difference between our algorithm and exist-\ning sparsity-based algorithms is that our pruner consistently\npromotes sparsity across all grouped layers, convering con-",
            "pruning. A key difference between our algorithm and exist-\ning sparsity-based algorithms is that our pruner consistently\npromotes sparsity across all grouped layers, convering con-\nvolutions, batch normalizations and fully-connected layers.\nWith this improvement, we are able to take full advantage\nof the group structures to learn better sparsity, and thus im-\nprove the accuracy of pruned models.\nDistribution of Group Sparsity.\nAs previously stated,\nconsistent sparsity are important for structural pruning, as it\nforces all pruned parameters to be consistently unimportant.\nIn Figure 5, we visualize the norm of grouped parameters\nlearned by the consistent and inconsistent strategies in Fig-\nure 4 (c) and (b). It is easy to find that our method produces\nstrong sparsity at the group level, which would be benefi-\ncial for identifying unimportant parameters. However, the\ninconsistent method that works on different layers indepen-\ndently fails to produce consistent importance across layers,",
            "cial for identifying unimportant parameters. However, the\ninconsistent method that works on different layers indepen-\ndently fails to produce consistent importance across layers,\nwhich could result in non-sparse norm at the group level.\n4.3. Ablation Study\nGrouping Strategy.\nTo further verify the effectiveness of\ngrouping, we evaluate different strategies on several convo-\nlutional networks. Strategies mainly include: 1) No group-\ning: sparse learning and importance evaluation are per-\nformed independently on a single convolutional layer; 2)\nConv-only Grouping: all convolutional layers within the\ngroup are sparsified in a consistent manner. 3) Full Group-\ning: All parameterized layers within a group, such as con-\nvolutions, batch normalizations, and fully-connected lay-\ners, are sparsified consistently. As shown in Table 2, When\nwe ignore the grouping information in neural networks and\nsparsify each layer in isolation, the performance of our ap-",
            "ers, are sparsified consistently. As shown in Table 2, When\nwe ignore the grouping information in neural networks and\nsparsify each layer in isolation, the performance of our ap-\nproach will degrade significantly, and in some cases even\ncollapse due to over-pruning. The results of Conv-only set-\n16096",
            "Architecture\nStrategy\nPruned Accuracy with Uniform / Learned Sparsity\n1.5×\n3.0×\n6.0×\n12×\nAvg.\nResNet-56\n(72.58)\nRandom\n71.49 / 72.07\n68.52 / 68.16\n60.35 / 60.25\n53.21 / 48.01\n63.39 / 62.15\nNo grouping\n71.96 / 72.07\n67.85 / 67.89\n62.64 / 63.18\n54.52 / 53.65\n64.24 / 64.20\nConv-only\n71.64 / 71.94\n68.30 / 69.07\n62.44 / 62.63\n53.89 / 54.94\n64.07 / 64.65\nFull Grouping\n71.68 / 72.57\n68.70 / 70.38\n63.72 / 65.33\n55.23 / 55.92\n64.83 / 66.09\nVGG-19\n(73.50)\nRandom\n72.63 / 72.77\n71.27 / 70.83\n68.97 / 69.16\n62.45 / 63.42\n63.83 / 69.05\nNo Grouping\n73.83 / 55.13\n71.40 / 53.21\n69.19 / 50.10\n65.12 /\n†3.87\n69.14 / 40.58\nConv-Only\n73.32 / 73.22\n71.38 / 71.80\n69.66 / 69.85\n64.69 / 65.95\n69.76 / 70.21\nFull Grouping\n73.11 / 74.00\n71.57 / 72.46\n69.72 / 70.38\n65.74 / 66.20\n70.03 / 70.58\nDenseNet-121\n(78.73)\nRandom\n79.04 / 79.43\n77.86 / 78.62\n75.47 / 74.52\n69.26 / 69.64\n75.41 / 75.80\nNo Grouping\n79.31 / 78.91\n78.08 / 78.62\n78.62 / 68.57\n72.93 / 57.17\n77.24 / 70.82\nConv-Only\n79.18 / 79.74\n77.98 / 78.85",
            "Random\n79.04 / 79.43\n77.86 / 78.62\n75.47 / 74.52\n69.26 / 69.64\n75.41 / 75.80\nNo Grouping\n79.31 / 78.91\n78.08 / 78.62\n78.62 / 68.57\n72.93 / 57.17\n77.24 / 70.82\nConv-Only\n79.18 / 79.74\n77.98 / 78.85\n76.61 / 77.22\n73.30 / 73.95\n76.77 / 77.44\nFull Grouping\n79.34 / 79.74\n77.97 / 79.19\n77.08 / 77.78\n74.77 / 75.29\n77.29 / 77.77\nMobileNetv2\n(70.80)\nRandom\n70.90 / 70.69\n67.75 / 67.54\n61.32 / 62.26\n53.41 / 53.97\n63.35 / 63.62\nNo Grouping\n71.16 / 71.28\n69.93 / 68.59\n66.76 / 37.38\n60.28 / 28.24\n67.03 / 51.37\nConv-Only\n71.22 / 71.51\n70.33 / 70.15\n66.16 / 66.49\n61.35 / 63.24\n67.27 / 67.85\nFull Grouping\n71.11 / 71.67\n70.06 / 70.81\n66.48 / 68.02\n60.32 / 63.37\n66.99 / 68.67\nGoogleNet\n(77.56)\nRandom\n77.52 / 77.72\n76.47 / 76.15\n74.92 / 74.19\n69.37 / 69.69\n74.57 / 74.44\nNo Grouping\n77.44 / 77.23\n76.84 / 74.95\n75.60 / 63.78\n71.92 / 63.72\n75.45 / 69.92\nConv-Only\n77.33 / 77.62\n76.68 / 76.92\n75.66 / 74.98\n71.90 / 71.87\n75.49 / 75.35\nFull Grouping\n77.91 / 77.76\n76.90 / 77.00\n75.42 / 75.44\n71.98 / 72.88",
            "75.60 / 63.78\n71.92 / 63.72\n75.45 / 69.92\nConv-Only\n77.33 / 77.62\n76.68 / 76.92\n75.66 / 74.98\n71.90 / 71.87\n75.49 / 75.35\nFull Grouping\n77.91 / 77.76\n76.90 / 77.00\n75.42 / 75.44\n71.98 / 72.88\n75.53 / 75.57\nTable 2. Ablation study on CIFAR-100 for different grouping strategies and sparsity configurations. The proposed strategy, full grouping,\ntakes all parameterized layers into account during sparse training, while other strategies only leverage partial layers. Accuracy (%) of\npruned models with uniform layer sparsity or learned layer sparsity is reported. †:In some cases, our method over-prunes some dimension\nto 1, which severely damages the final accuracy.\nting show that grouping partial parameters is beneficial to\nthe final performance, but some useful information in the\ngroup is still ignored. Therefore, it is feasible to further im-\nprove the pruning accuracy with the full grouping strategy.\nLearned Sparsity.\nLayer sparsity is also an important",
            "group is still ignored. Therefore, it is feasible to further im-\nprove the pruning accuracy with the full grouping strategy.\nLearned Sparsity.\nLayer sparsity is also an important\nfactor for pruning, which determines the final structure of\npruned neural networks.\nTable 2 provides some results\nabout layer sparsity. This work primarily focuses on two\ntypes of sparsity, namely uniform sparsity and learned spar-\nsity. With uniform sparsity, the same pruning ratio will be\napplied to different layers, assuming that redundancy is dis-\ntributed uniformly through the network. However, previous\nexperiments in Figure 5 have shown that different layers are\nnot equally prunable. In most cases, the learned sparsity\noutperforms the uniform one, although sometimes it may\nover-prune some layers, leading to degraded accuracy.\nGeneralizability of DepGraph.\nResults in Table 2 also\ndemonstrate the generalizability of our framework, which\nis able to handle various convolutional neural networks.",
            "Generalizability of DepGraph.\nResults in Table 2 also\ndemonstrate the generalizability of our framework, which\nis able to handle various convolutional neural networks.\nMoreover, we emphasize that our method is compatible\nwith DenseNet and GoogleNet, which contains dense con-\nnections and parallel structures. In the following sections,\nwe will further demonstrate the capability of our framework\nto more architectures.\n4.4. Towards Any Structural Pruning\nVisualization of DepGraph.\nPruning large neural net-\nworks presents a considerable challenge due to the intricate\nprocess of grouping parameters. However, by employing\nthe DepGraph, all coupled groups can be effortlessly ob-\ntained. We provide visualizations of the DepGraph D and\nthe derived grouping matrices G for DenseNet-121 [23],\nResNet-18, and Vision Transformers [10] in Figure 6. The\ngrouping matrices are derived from the DepGraph as out-\nlined in Algorithm 2, where G[i, j] = 1 signifies that",
            "ResNet-18, and Vision Transformers [10] in Figure 6. The\ngrouping matrices are derived from the DepGraph as out-\nlined in Algorithm 2, where G[i, j] = 1 signifies that\nthe i-th layer belongs to the same group as the j-th layer.\nDenseNet-121 demonstrates a strong correlation between\nlayers within the same dense block, leading to large groups\nduring the structural pruning. The proposed Dependency\nGraph proves to be helpful when dealing with complex net-\nworks, as manually analyzing all dependencies in such net-\nworks is indeed an intractable task.\nImageNet.\nTable 3 presents pruning results on ImageNet\nfor several architectures, including ResNet, DenseNet, Mo-\nbileNet, ResNeXt, and Vision Transformers. The target of\n16097",
            "Arch.\nMethod\nBase\nPruned\n∆Acc.\nMACs\nResNet-50\nResNet-50\n76.15\n-\n-\n4.13\nThiNet [39]\n72.88\n72.04\n-0.84\n2.44\nSSS [24]\n76.12\n74.18\n-1.94\n2.82\nSFP [18]\n76.15\n74.61\n-1.54\n2.40\nAutoSlim [73]\n76.10\n75.60\n-0.50\n2.00\nFPGM [20]\n76.15\n75.50\n-0.65\n2.38\nTaylor [42]\n76.18\n74.50\n-1.68\n2.25\nSlimable [72]\n76.10\n74.90\n-1.20\n2.30\nCCP [45]\n76.15\n75.50\n-0.65\n2.11\nAOFP-C1 [5]\n75.34\n75.63\n+0.29\n2.58\nTAS [9]\n77.46\n76.20\n-1.26\n2.31\nGFP [32]\n76.79\n76.42\n-0.37\n2.04\nGReg-2 [58]\n76.13\n75.36\n-0.77\n2.77\nOurs\n76.15\n75.83\n-0.32\n1.99\nDenseNet-121\nDenseNet-121\n74.44\n-\n-\n2.86\nPSP-1.38G [50]\n74.35\n74.05\n-0.30\n1.38\nPSP-0.58G [50]\n74.35\n70.34\n-4.01\n0.58\nOurs-1.38G\n74.44\n73.98\n-0.46\n1.37\nOurs-0.58G\n74.44\n70.13\n-4.31\n0.57\nMob-v2\nMob-v2\n71.87\n-\n-\n0.33\nNetAdapt [64]\n-\n70.00\n-\n0.24\nMeta [36]\n74.70\n68.20\n-6.50\n0.14\nGFP [32]\n75.74\n69.16\n-6.58\n0.15\nOurs\n71.87\n68.46\n-3.41\n0.15\nNeXt-50\nResNeXt-50\n77.62\n-\n-\n4.27\nSSS [24]\n77.57\n74.98\n-2.59\n2.43\nGFP [32]\n77.97\n77.53\n-0.44\n2.11\nOurs\n77.62\n76.48\n-1.14\n2.09\nViT-B/16\nVIT-B/16\n81.07\n-\n-\n17.6",
            "-6.58\n0.15\nOurs\n71.87\n68.46\n-3.41\n0.15\nNeXt-50\nResNeXt-50\n77.62\n-\n-\n4.27\nSSS [24]\n77.57\n74.98\n-2.59\n2.43\nGFP [32]\n77.97\n77.53\n-0.44\n2.11\nOurs\n77.62\n76.48\n-1.14\n2.09\nViT-B/16\nVIT-B/16\n81.07\n-\n-\n17.6\nCP-ViT [52]\n77.91\n77.36\n-0.55\n11.7\nOurs+EMA\n81.07\n79.58\n-1.39\n10.4\nOurs\n81.07\n79.17\n-1.90\n10.4\nTable 3. Pruning results on ImageNet.\nthis work is not to provide state-of-the-art results for var-\nious models, thus we only use the most basic importance\ncriterion in this work. We show that a simple norm-based\ncriterion, when combined with dependency modeling, can\nachieve comparable performance to modern approaches that\nuse powerful criteria [32,71] and training techniques [58].\nText, 3D Point Cloud, Graph and More.\nIn addition to\nCNNs and Transformers, our method is easily applicable\nto other architectures as well. This part consists of experi-\nments on a variety of data, including texts, graphs, and 3D\npoint clouds, as shown in Table 4. We utilize LSTM for text",
            "to other architectures as well. This part consists of experi-\nments on a variety of data, including texts, graphs, and 3D\npoint clouds, as shown in Table 4. We utilize LSTM for text\nclassification by studying the effectiveness of DepGraph on\nrecursive structures in which parameterized layers are cou-\npled due to the element-wise operations. DepGraph is also\ntested on Dynamic Graph CNNs that contain aggregation\noperations for 3D point clouds. Furthermore, we conduct\nexperiments with graph data, which require entirely differ-\nent architectures from those used for other tasks. In this ex-\nArch. & Data\nMethod\nBase\nPruned\n∆\nSpeedup\nLSTM\n(AGNews)\nDepGraph+Random\n92.10\n91.23\n-0.87\n16.28×\nDepGraph+CP [29]\n92.10\n91.50\n-0.60\n16.28×\nOurs w/o SL\n92.10\n91.53\n-0.57\n16.28×\nOurs\n92.10\n91.75\n-0.35\n16.28×\nDGCNN\n(ModelNet40)\nDepGraph+Random\n92.10\n91.05\n-1.05\n10.05×\nDepGraph+CP [29]\n92.10\n91.00\n-1.10\n10.05×\nDepGraph+Slim [35]\n92.10\n91.74\n-0.36\n10.35×\nOurs w/o SL\n92.10\n91.86\n-0.24\n11.46×\nOurs\n92.10\n92.02",
            "DGCNN\n(ModelNet40)\nDepGraph+Random\n92.10\n91.05\n-1.05\n10.05×\nDepGraph+CP [29]\n92.10\n91.00\n-1.10\n10.05×\nDepGraph+Slim [35]\n92.10\n91.74\n-0.36\n10.35×\nOurs w/o SL\n92.10\n91.86\n-0.24\n11.46×\nOurs\n92.10\n92.02\n-0.08\n11.98×\nGAT\n(PPI)\nDepGraph+Random\n0.986\n0.951\n-0.035\n8.05×\nDepGraph+CP [29]\n0.986\n0.957\n-0.029\n8.05×\nOurs w/o SL\n0.986\n0.953\n-0.033\n8.26×\nOurs\n0.986\n0.961\n-0.025\n8.43×\nTable 4. Pruning neural networks for non-image data, including\nAGNews (text), ModelNet (3D Point Cloud) and PPI (Graph). We\nreport the classification accuracy (%) of pruned model for AG-\nNews and ModelNet and micro-F1 score for PPI.\n(a) DenseNet-121\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n100\n200\n300\n400\n500\n600\n700\n800\n(b) ResNet-18\n0\n20\n40\n60\n80\n100\n120\n0\n20\n40\n60\n80\n100\n120\n(c) ViT-Base\n0\n200\n400\n600\n800\n0\n200\n400\n600\n800\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n20\n40\n60\n80\n100\n120\n0\n20\n40\n60\n80\n100\n120\n0\n200\n400\n600\n800\n0\n200\n400\n600\n800\nFigure 6.",
            "0\n200\n400\n600\n800\n0\n200\n400\n600\n800\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n100\n200\n300\n400\n500\n600\n700\n800\n0\n20\n40\n60\n80\n100\n120\n0\n20\n40\n60\n80\n100\n120\n0\n200\n400\n600\n800\n0\n200\n400\n600\n800\nFigure 6.\nDependency graphs (top) and the derived grouping\nschemes (bottom) for DenseNet-121, ResNet-18 and ViT-Base.\nperiment, we concentrate on the acceleration of Graph At-\ntention Networks, which have several coupled layers within\neach GNN layer. Considering the lack of works concern-\ning pruning on these datasets, we combine DepGraph with\nsome classic pruning methods in CNNs to establish our\nbaselines. The results indicate that our method can be in-\ndeed generalized to a wide variety of architectures.\n5. Conclusion\nIn this work, we introduce Dependency Graph to enable\nany structural pruning on a wide variety of neural networks.\nOur work is the first attempt, to our knowledge, to develop\na general algorithm that can be applied to architectures, in-\ncluding CNNs, RNNs, GNNs, and Transformers.",
            "Our work is the first attempt, to our knowledge, to develop\na general algorithm that can be applied to architectures, in-\ncluding CNNs, RNNs, GNNs, and Transformers.\nAcknowledgment\nThis research is supported by the National Research\nFoundation Singapore under its AI Singapore Programme\n(Award Number: AISG2-RP-2021-023).\n16098",
            "References\n[1] Alfred V. Aho, Michael R Garey, and Jeffrey D. Ullman. The\ntransitive reduction of a directed graph. SIAM Journal on\nComputing, 1(2):131–137, 1972. 4\n[2] Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Mar-\nculescu. Towards efficient model compression via learned\nglobal ranking. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 1518–\n1528, 2020. 2\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248–255. Ieee, 2009. 6\n[4] Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong\nHan. Centripetal sgd for pruning very deep convolutional\nnetworks with complicated structure.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4943–4953, 2019. 1, 2, 6\n[5] Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han,",
            "In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4943–4953, 2019. 1, 2, 6\n[5] Xiaohan Ding, Guiguang Ding, Yuchen Guo, Jungong Han,\nand Chenggang Yan. Approximated oracle filter pruning for\ndestructive cnn width optimization. In International Confer-\nence on Machine Learning, pages 1607–1616. PMLR, 2019.\n8\n[6] Xiaohan Ding, Guiguang Ding, Jungong Han, and Sheng\nTang.\nAuto-balanced filter pruning for efficient convolu-\ntional neural networks. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 32, 2018. 6\n[7] Xiaohan Ding, Tianxiang Hao, Jianchao Tan, Ji Liu, Jungong\nHan, Yuchen Guo, and Guiguang Ding. Resrep: Lossless\ncnn pruning via decoupling remembering and forgetting. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 4510–4520, 2021. 1, 2, 6\n[8] Xin Dong, Shangyu Chen, and Sinno Pan.\nLearning to\nprune deep neural networks via layer-wise optimal brain sur-",
            "Computer Vision, pages 4510–4520, 2021. 1, 2, 6\n[8] Xin Dong, Shangyu Chen, and Sinno Pan.\nLearning to\nprune deep neural networks via layer-wise optimal brain sur-\ngeon. Advances in Neural Information Processing Systems,\n30, 2017. 1, 2\n[9] Xuanyi Dong and Yi Yang.\nNetwork pruning via trans-\nformable architecture search. Advances in Neural Informa-\ntion Processing Systems, 32, 2019. 8\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 2, 6, 7\n[11] Shangqian Gao, Feihu Huang, Weidong Cai, and Heng\nHuang. Network pruning via performance maximization. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9270–9280, 2021. 1\n[12] Alex Graves. Long short-term memory. Supervised sequence",
            "Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9270–9280, 2021. 1\n[12] Alex Graves. Long short-term memory. Supervised sequence\nlabelling with recurrent neural networks, pages 37–45, 2012.\n2, 6\n[13] Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic net-\nwork surgery for efficient dnns. Advances in neural informa-\ntion processing systems, 29, 2016. 1\n[14] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive\nrepresentation learning on large graphs. Advances in neural\ninformation processing systems, 30, 2017. 6\n[15] Song Han, Huizi Mao, and William J Dally.\nDeep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding.\narXiv preprint\narXiv:1510.00149, 2015. 2\n[16] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\ning both weights and connections for efficient neural net-\nwork. Advances in neural information processing systems,\n28, 2015. 1",
            "[16] Song Han, Jeff Pool, John Tran, and William Dally. Learn-\ning both weights and connections for efficient neural net-\nwork. Advances in neural information processing systems,\n28, 2015. 1\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1\n[18] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi\nYang. Soft filter pruning for accelerating deep convolutional\nneural networks. arXiv preprint arXiv:1808.06866, 2018. 6,\n8\n[19] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and\nSong Han. Amc: Automl for model compression and ac-\nceleration on mobile devices. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages 784–\n800, 2018. 2, 6\n[20] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi\nYang.\nFilter pruning via geometric median for deep con-\nvolutional neural networks acceleration. In Proceedings of",
            "800, 2018. 2, 6\n[20] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi\nYang.\nFilter pruning via geometric median for deep con-\nvolutional neural networks acceleration. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4340–4349, 2019. 2, 6, 8\n[21] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning\nfor accelerating very deep neural networks. In Proceedings\nof the IEEE international conference on computer vision,\npages 1389–1397, 2017. 2, 3\n[22] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2(7), 2015. 1\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger.\nDensely connected convolutional net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 4700–4708, 2017. 1,\n2, 6, 7\n[24] Zehao Huang and Naiyan Wang. Data-driven sparse struc-\nture selection for deep neural networks. In Proceedings of",
            "vision and pattern recognition, pages 4700–4708, 2017. 1,\n2, 6, 7\n[24] Zehao Huang and Naiyan Wang. Data-driven sparse struc-\nture selection for deep neural networks. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n304–320, 2018. 8\n[25] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song,\nand Dacheng Tao. Meta-aggregator: Learning to aggregate\nfor 1-bit graph neural networks. In ICCV, 2021. 1\n[26] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6\n[27] Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jin-\nwoo Shin. Layer-adaptive sparsity for the magnitude-based\npruning. arXiv preprint arXiv:2010.07611, 2020. 2\n[28] Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould,\nand Philip HS Torr.\nA signal propagation perspective for\npruning neural networks at initialization.\narXiv preprint\narXiv:1906.06307, 2019. 2\n16099",
            "[29] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and\nHans Peter Graf. Pruning filters for efficient convnets. arXiv\npreprint arXiv:1608.08710, 2016. 1, 2, 3, 5, 6, 8\n[30] Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xi-\naotong Zhang.\nPruning and quantization for deep neural\nnetwork acceleration: A survey. Neurocomputing, 461:370–\n403, 2021. 1\n[31] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang,\nBaochang Zhang, Yonghong Tian, and Ling Shao. Hrank:\nFilter pruning using high-rank feature map. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 1529–1538, 2020. 1, 6\n[32] Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou,\nJing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang,\nQingmin Liao, and Wayne Zhang. Group fisher pruning for\npractical network compression. In International Conference\non Machine Learning, pages 7021–7032. PMLR, 2021. 2, 3,\n8\n[33] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xin-",
            "practical network compression. In International Conference\non Machine Learning, pages 7021–7032. PMLR, 2021. 2, 3,\n8\n[33] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xin-\nchao Wang. Dataset distillation via factorization. In Confer-\nence on Neural Information Processing Systems, 2022. 1\n[34] Songhua Liu, Jingwen Ye, Runpeng Yu, and Xinchao Wang.\nSlimmable dataset condensation. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2023. 1\n[35] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang,\nShoumeng Yan, and Changshui Zhang. Learning efficient\nconvolutional networks through network slimming. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2736–2744, 2017. 2, 3, 5, 8\n[36] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin\nYang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta\nlearning for automatic neural network channel pruning. In\nProceedings of the IEEE/CVF international conference on",
            "Yang, Kwang-Ting Cheng, and Jian Sun. Metapruning: Meta\nlearning for automatic neural network channel pruning. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 3296–3305, 2019. 8\n[37] Ekdeep Singh Lubana and Robert P Dick. A gradient flow\nframework for analyzing network pruning. arXiv preprint\narXiv:2009.11839, 2020. 2\n[38] Jian-Hao Luo and Jianxin Wu. Neural network pruning with\nresidual-connections and limited-data.\nIn The IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 1458–1467, June 2020. 1, 2\n[39] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter\nlevel pruning method for deep neural network compression.\nIn Proceedings of the IEEE international conference on com-\nputer vision, pages 5058–5066, 2017. 2, 3, 8\n[40] Xiaolong Ma, Geng Yuan, Sheng Lin, Zhengang Li, Hao\nSun, and Yanzhi Wang. Resnet can be pruned 60×: Intro-\nducing network purification and unused path removal (p-rm)",
            "[40] Xiaolong Ma, Geng Yuan, Sheng Lin, Zhengang Li, Hao\nSun, and Yanzhi Wang. Resnet can be pruned 60×: Intro-\nducing network purification and unused path removal (p-rm)\nafter weight pruning. In 2019 IEEE/ACM International Sym-\nposium on Nanoscale Architectures (NANOARCH), pages 1–\n2. IEEE, 2019. 2, 4, 6\n[41] S´ebastien Marcel and Yann Rodriguez.\nTorchvision the\nmachine-vision package of torch. In Proceedings of the 18th\nACM international conference on Multimedia, pages 1485–\n1488, 2010. 6\n[42] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Fro-\nsio, and Jan Kautz. Importance estimation for neural net-\nwork pruning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11264–\n11272, 2019. 2, 8\n[43] Laurent Orseau, Marcus Hutter, and Omar Rivasplata. Log-\narithmic pruning is all you need. Advances in Neural Infor-\nmation Processing Systems, 33:2925–2934, 2020. 2\n[44] Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin.\nLookahead:",
            "arithmic pruning is all you need. Advances in Neural Infor-\nmation Processing Systems, 33:2925–2934, 2020. 2\n[44] Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin.\nLookahead:\na far-sighted alternative of magnitude-based\npruning. arXiv preprint arXiv:2002.04809, 2020. 1, 2\n[45] Hanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou\nHuang. Collaborative channel pruning for deep networks.\nIn International Conference on Machine Learning, pages\n5113–5122. PMLR, 2019. 8\n[46] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll´ar. Designing network design\nspaces. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 10428–10436,\n2020. 2\n[47] Alex Renda, Jonathan Frankle, and Michael Carbin. Com-\nparing rewinding and fine-tuning in neural network pruning.\narXiv preprint arXiv:2003.02389, 2020. 2\n[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted",
            "arXiv preprint arXiv:2003.02389, 2020. 2\n[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510–4520, 2018. 2, 6\n[49] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement\npruning: Adaptive sparsity by fine-tuning. Advances in Neu-\nral Information Processing Systems, 33:20378–20389, 2020.\n2\n[50] G¨unther Schindler, Wolfgang Roth, Franz Pernkopf, and\nHolger Fr¨oning. Parameterized structured pruning for deep\nneural networks. In International Conference on Machine\nLearning, Optimization, and Data Science, pages 16–27.\nSpringer, 2020. 8\n[51] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2, 6\n[52] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng",
            "lutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2, 6\n[52] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng\nJing, and Xiaoyao Liang. Cp-vit: Cascade vision transformer\npruning via progressive sparsity prediction. arXiv preprint\narXiv:2203.04570, 2022. 8\n[53] Ralf C Staudemeyer and Eric Rothstein Morris. Understand-\ning lstm–a tutorial into long short-term memory recurrent\nneural networks. arXiv preprint arXiv:1909.09586, 2019. 2\n[54] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1–9, 2015.\n2, 6\n[55] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. arXiv preprint arXiv:1710.10903, 2017. 2,\n6",
            "2, 6\n[55] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. arXiv preprint arXiv:1710.10903, 2017. 2,\n6\n[56] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong\nZhang. Eigendamage: Structured pruning in the kronecker-\nfactored eigenbasis. In International Conference on Machine\nLearning, pages 6566–6575. PMLR, 2019. 6\n16100",
            "[57] Huan Wang, Can Qin, Yue Bai, and Yun Fu. Why is the\nstate of neural network pruning so confusing? on the fair-\nness, comparison setup, and trainability in network pruning.\narXiv preprint arXiv:2301.05219, 2023. 2\n[58] Huan Wang, Can Qin, Yulun Zhang, and Yun Fu.\nNeu-\nral pruning via growing regularization.\narXiv preprint\narXiv:2012.09243, 2020. 1, 2, 6, 8\n[59] Wenxiao Wang, Minghao Chen, Shuai Zhao, Long Chen,\nJinming Hu, Haifeng Liu, Deng Cai, Xiaofei He, and Wei\nLiu. Accelerate cnns from three dimensions: a comprehen-\nsive pruning framework. In International Conference on Ma-\nchine Learning, pages 10717–10726. PMLR, 2021. 1\n[60] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon.\nDynamic\ngraph cnn for learning on point clouds. Acm Transactions\nOn Graphics (tog), 38(5):1–12, 2019. 2, 6\n[61] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and\nJian Cheng.\nQuantized convolutional neural networks for",
            "On Graphics (tog), 38(5):1–12, 2019. 2, 6\n[61] Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and\nJian Cheng.\nQuantized convolutional neural networks for\nmobile devices. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4820–4828,\n2016. 1\n[62] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 1912–1920, 2015. 6\n[63] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017. 2, 6\n[64] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec\nGo, Mark Sandler, Vivienne Sze, and Hartwig Adam. Ne-\ntadapt: Platform-aware neural network adaptation for mobile",
            "2017. 2, 6\n[64] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec\nGo, Mark Sandler, Vivienne Sze, and Hartwig Adam. Ne-\ntadapt: Platform-aware neural network adaptation for mobile\napplications. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 285–300, 2018. 8\n[65] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing\nknowledge in neural networks. In European Conference on\nComputer Vision, 2022. 1\n[66] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and\nXinchao Wang. Deep model reassembly. In Conference on\nNeural Information Processing Systems, 2022. 1\n[67] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and\nXinchao Wang. Distilling knowledge from graph convolu-\ntional networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2020. 1\n[68] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li,\nand Tong Zhang. Joint-detnas: upgrade your detector with\nnas, pruning and dynamic distillation.\nIn Proceedings of",
            "[68] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li,\nand Tong Zhang. Joint-detnas: upgrade your detector with\nnas, pruning and dynamic distillation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10175–10184, 2021. 1\n[69] Jingwen Ye, Songhua Liu, and Xinchao Wang. Partial net-\nwork cloning. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2023. 1\n[70] Jianbo Ye, Xin Lu, Zhe Lin, and James Z Wang.\nRe-\nthinking the smaller-norm-less-informative assumption in\nchannel pruning of convolution layers.\narXiv preprint\narXiv:1802.00124, 2018. 2\n[71] Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping\nWang. Gate decorator: Global filter pruning method for ac-\ncelerating deep convolutional neural networks. Advances in\nneural information processing systems, 32, 2019. 1, 2, 3, 6,\n8\n[72] Jiahui Yu.\nSlimmable neural networks for edge devices.\n2019. 8\n[73] Jiahui Yu and Thomas Huang.\nAutoslim: Towards one-",
            "neural information processing systems, 32, 2019. 1, 2, 3, 6,\n8\n[72] Jiahui Yu.\nSlimmable neural networks for edge devices.\n2019. 8\n[73] Jiahui Yu and Thomas Huang.\nAutoslim: Towards one-\nshot architecture search for channel numbers. arXiv preprint\narXiv:1903.11728, 2019. 8\n[74] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I\nMorariu, Xintong Han, Mingfei Gao, Ching-Yung Lin, and\nLarry S Davis. Nisp: Pruning networks using neuron impor-\ntance score propagation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n9194–9203, 2018. 1, 6\n[75] Ruonan Yu, Songhua Liu, and Xinchao Wang.\nDataset\ndistillation:\nA comprehensive review.\narXiv preprint\narXiv:2301.07014, 2023. 1\n[76] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level\nconvolutional networks for text classification. Advances in\nneural information processing systems, 28, 2015. 6\n[77] Yulun Zhang, Huan Wang, Can Qin, and Yun Fu. Aligned",
            "convolutional networks for text classification. Advances in\nneural information processing systems, 28, 2015. 6\n[77] Yulun Zhang, Huan Wang, Can Qin, and Yun Fu. Aligned\nstructured sparsity learning for efficient image super-\nresolution. Advances in Neural Information Processing Sys-\ntems, 34:2695–2706, 2021. 2, 3\n[78] Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng,\nKai Shuang, and Xiang Li. Neuron-level structured pruning\nusing polarization regularizer. Advances in neural informa-\ntion processing systems, 33:9865–9877, 2020. 6\n16101"
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 1,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 2,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 3,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 4,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 5,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 5,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 5,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 5,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 5,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 6,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 7,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 8,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 9,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 10,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            },
            {
                "page": 11,
                "source": "Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf",
                "title": "DepGraph: Towards Any Structural Pruning",
                "authors": "Gongfan Fang; Xinyin Ma; Mingli Song; Michael Bi Mi; Xinchao Wang"
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25",
            "chunk_26",
            "chunk_27",
            "chunk_28",
            "chunk_29",
            "chunk_30",
            "chunk_31",
            "chunk_32",
            "chunk_33",
            "chunk_34",
            "chunk_35",
            "chunk_36",
            "chunk_37",
            "chunk_38",
            "chunk_39",
            "chunk_40",
            "chunk_41",
            "chunk_42",
            "chunk_43",
            "chunk_44",
            "chunk_45",
            "chunk_46",
            "chunk_47",
            "chunk_48",
            "chunk_49",
            "chunk_50",
            "chunk_51",
            "chunk_52",
            "chunk_53",
            "chunk_54",
            "chunk_55",
            "chunk_56",
            "chunk_57",
            "chunk_58",
            "chunk_59",
            "chunk_60",
            "chunk_61",
            "chunk_62",
            "chunk_63",
            "chunk_64",
            "chunk_65",
            "chunk_66",
            "chunk_67",
            "chunk_68",
            "chunk_69",
            "chunk_70"
        ]
    },
    "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf": {
        "chunks": [
            "Published as a conference paper at ICLR 2017\nPRUNING CONVOLUTIONAL NEURAL NETWORKS\nFOR RESOURCE EFFICIENT INFERENCE\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz\nNVIDIA\n{pmolchanov, styree, tkarras, taila, jkautz}@nvidia.com\nABSTRACT\nWe propose a new formulation for pruning convolutional kernels in neural networks\nto enable efﬁcient inference. We interleave greedy criteria-based pruning with ﬁne-\ntuning by backpropagation—a computationally efﬁcient procedure that maintains\ngood generalization in the pruned network. We propose a new criterion based on\nTaylor expansion that approximates the change in the cost function induced by\npruning network parameters. We focus on transfer learning, where large pretrained\nnetworks are adapted to specialized tasks. The proposed criterion demonstrates\nsuperior performance compared to other criteria, e.g. the norm of kernel weights\nor feature map activation, for pruning large CNNs after adaptation to ﬁne-grained",
            "superior performance compared to other criteria, e.g. the norm of kernel weights\nor feature map activation, for pruning large CNNs after adaptation to ﬁne-grained\nclassiﬁcation tasks (Birds-200 and Flowers-102) relaying only on the ﬁrst order\ngradient information. We also show that pruning can lead to more than 10×\ntheoretical reduction in adapted 3D-convolutional ﬁlters with a small drop in\naccuracy in a recurrent gesture classiﬁer. Finally, we show results for the large-\nscale ImageNet dataset to emphasize the ﬂexibility of our approach.\n1\nINTRODUCTION\nConvolutional neural networks (CNN) are used extensively in computer vision applications, including\nobject classiﬁcation and localization, pedestrian and car detection, and video classiﬁcation. Many\nproblems like these focus on specialized domains for which there are only small amounts of care-\nfully curated training data. In these cases, accuracy may be improved by ﬁne-tuning an existing",
            "problems like these focus on specialized domains for which there are only small amounts of care-\nfully curated training data. In these cases, accuracy may be improved by ﬁne-tuning an existing\ndeep network previously trained on a much larger labeled vision dataset, such as images from Ima-\ngeNet (Russakovsky et al., 2015) or videos from Sports-1M (Karpathy et al., 2014). While transfer\nlearning of this form supports state of the art accuracy, inference is expensive due to the time, power,\nand memory demanded by the heavyweight architecture of the ﬁne-tuned network.\nWhile modern deep CNNs are composed of a variety of layer types, runtime during prediction is\ndominated by the evaluation of convolutional layers. With the goal of speeding up inference, we\nprune entire feature maps so the resulting networks may be run efﬁciently even on embedded devices.\nWe interleave greedy criteria-based pruning with ﬁne-tuning by backpropagation, a computationally",
            "prune entire feature maps so the resulting networks may be run efﬁciently even on embedded devices.\nWe interleave greedy criteria-based pruning with ﬁne-tuning by backpropagation, a computationally\nefﬁcient procedure that maintains good generalization in the pruned network.\nNeural network pruning was pioneered in the early development of neural networks (Reed, 1993).\nOptimal Brain Damage (LeCun et al., 1990) and Optimal Brain Surgeon (Hassibi & Stork, 1993)\nleverage a second-order Taylor expansion to select parameters for deletion, using pruning as regu-\nlarization to improve training and generalization. This method requires computation of the Hessian\nmatrix partially or completely, which adds memory and computation costs to standard ﬁne-tuning.\nIn line with our work, Anwar et al. (2015) describe structured pruning in convolutional layers at the\nlevel of feature maps and kernels, as well as strided sparsity to prune with regularity within kernels.",
            "level of feature maps and kernels, as well as strided sparsity to prune with regularity within kernels.\nPruning is accomplished by particle ﬁltering wherein conﬁgurations are weighted by misclassiﬁcation\nrate. The method demonstrates good results on small CNNs, but larger CNNs are not addressed.\nHan et al. (2015) introduce a simpler approach by ﬁne-tuning with a strong ℓ2 regularization term\nand dropping parameters with values below a predeﬁned threshold. Such unstructured pruning is very\neffective for network compression, and this approach demonstrates good performance for intra-kernel\npruning. But compression may not translate directly to faster inference since modern hardware\n1\narXiv:1611.06440v2  [cs.LG]  8 Jun 2017",
            "Published as a conference paper at ICLR 2017\nexploits regularities in computation for high throughput. So specialized hardware may be needed\nfor efﬁcient inference of a network with intra-kernel sparsity (Han et al., 2016). This approach\nalso requires long ﬁne-tuning times that may exceed the original network training by a factor of\n3 or larger. Group sparsity based regularization of network parameters was proposed to penalize\nunimportant parameters (Wen et al., 2016; Zhou et al., 2016; Alvarez & Salzmann, 2016; Lebedev\n& Lempitsky, 2016). Regularization-based pruning techniques require per layer sensitivity analysis\nwhich adds extra computations. In contrast, our approach relies on global rescaling of criteria for all\nlayers and does not require sensitivity estimation. Moreover, our approach is faster as we directly\nprune unimportant parameters instead of waiting for their values to be made sufﬁciently small by\noptimization under regularization.",
            "prune unimportant parameters instead of waiting for their values to be made sufﬁciently small by\noptimization under regularization.\nOther approaches include combining parameters with correlated weights (Srinivas & Babu, 2015),\nreducing precision (Gupta et al., 2015; Rastegari et al., 2016) or tensor decomposition (Kim et al.,\n2015). These approaches usually require a separate training procedure or signiﬁcant ﬁne-tuning, but\npotentially may be combined with our method for additional speedups.\n2\nMETHOD\nRemove the least \nimportant neuron\nFine-tuning\nEvaluate importance \nof neurons\nContinue pruning?\nNetwork\nStop pruning\nno\nyes\nFigure 1: Network pruning as\na backward ﬁlter.\nThe proposed method for pruning consists of the following steps:\n1) Fine-tune the network until convergence on the target task; 2)\nAlternate iterations of pruning and further ﬁne-tuning; 3) Stop prun-\ning after reaching the target trade-off between accuracy and pruning",
            "Alternate iterations of pruning and further ﬁne-tuning; 3) Stop prun-\ning after reaching the target trade-off between accuracy and pruning\nobjective, e.g. ﬂoating point operations (FLOPs) or memory utiliza-\ntion.\nThe procedure is simple, but its success hinges on employing the\nright pruning criterion. In this section, we introduce several efﬁcient\npruning criteria and related technical considerations.\nConsider\na\nset\nof\ntraining\nexamples\nD\n=\n\b\nX\n=\n{x0, x1, ..., xN}, Y\n=\n{y0, y1, ..., yN}\n\t\n, where x and y rep-\nresent an input and a target output, respectively. The network’s\nparameters1 W = {(w1\n1, b1\n1), (w2\n1, b2\n1), ...(wCℓ\nL , bCℓ\nL )} are optimized\nto minimize a cost value C(D|W). The most common choice for\na cost function C(·) is a negative log-likelihood function. A cost\nfunction is selected independently of pruning and depends only on\nthe task to be solved by the original network. In the case of transfer\nlearning, we adapt a large network initialized with parameters W0",
            "the task to be solved by the original network. In the case of transfer\nlearning, we adapt a large network initialized with parameters W0\npretrained on a related but distinct dataset.\nDuring pruning, we reﬁne a subset of parameters which preserves\nthe accuracy of the adapted network, C(D|W′) ≈C(D|W). This corresponds to a combinatorial\noptimization:\nmin\nW′\n\f\f\f\fC(D|W′) −C(D|W)\n\f\f\f\f\ns.t.\n||W′||0 ≤B,\n(1)\nwhere the ℓ0 norm in ||W′||0 bounds the number of non-zero parameters B in W ′. Intuitively, if\nW′ = W we reach the global minimum of the error function, however ||W′||0 will also have its\nmaximum.\nFinding a good subset of parameters while maintaining a cost value as close as possible to the\noriginal is a combinatorial problem. It will require 2|W| evaluations of the cost function for a selected\nsubset of data. For current networks it would be impossible to compute: for example, VGG-16 has\n|W| = 4224 convolutional feature maps. While it is impossible to solve this optimization exactly for",
            "subset of data. For current networks it would be impossible to compute: for example, VGG-16 has\n|W| = 4224 convolutional feature maps. While it is impossible to solve this optimization exactly for\nnetworks of any reasonable size, in this work we investigate a class of greedy methods.\nStarting with a full set of parameters W, we iteratively identify and remove the least important\nparameters, as illustrated in Figure 1. By removing parameters at each iteration, we ensure the\neventual satisfaction of the ℓ0 bound on W′.\n1A “parameter” (w, b) ∈W might represent an individual weight, a convolutional kernel, or the entire set of\nkernels that compute a feature map; our experiments operate at the level of feature maps.\n2",
            "Published as a conference paper at ICLR 2017\nSince we focus our analysis on pruning feature maps from convolutional layers, let us denote a\nset of image feature maps by zℓ∈RHℓ×Wℓ×Cℓwith dimensionality Hℓ× Wℓand Cℓindividual\nmaps (or channels).2 The feature maps can either be the input to the network, z0, or the output\nfrom a convolutional layer, zℓwith ℓ∈[1, 2, ..., L]. Individual feature maps are denoted z(k)\nℓ\nfor\nk ∈[1, 2, ..., Cℓ]. A convolutional layer ℓapplies the convolution operation (∗) to a set of input\nfeature maps zℓ−1 with kernels parameterized by w(k)\nℓ\n∈RCℓ−1×p×p:\nz(k)\nℓ\n= g(k)\nℓR\n\u0000zℓ−1 ∗w(k)\nℓ\n+ b(k)\nℓ\n\u0001\n,\n(2)\nwhere z(k)\nℓ\n∈RHℓ×Wℓis the result of convolving each of Cℓ−1 kernels of size p×p with its respective\ninput feature map and adding bias b(k)\nℓ. We introduce a pruning gate gl ∈{0, 1}Cl, an external switch\nwhich determines if a particular feature map is included or pruned during feed-forward propagation,\nsuch that when g is vectorized: W′ = gW.\n2.1\nORACLE PRUNING",
            "which determines if a particular feature map is included or pruned during feed-forward propagation,\nsuch that when g is vectorized: W′ = gW.\n2.1\nORACLE PRUNING\nMinimizing the difference in accuracy between the full and pruned models depends on the criterion for\nidentifying the “least important” parameters, called saliency, at each step. The best criterion would be\nan exact empirical evaluation of each parameter, which we denote the oracle criterion, accomplished\nby ablating each non-zero parameter w ∈W′ in turn and recording the cost’s difference.\nWe distinguish two ways of using this oracle estimation of importance: 1) oracle-loss quantiﬁes\nimportance as the signed change in loss, C(D|W′) −C(D|W), and 2) oracle-abs adopts the absolute\ndifference, |C(D|W′) −C(D|W)|. While both discourage pruning which increases the loss, the\noracle-loss version encourages pruning which may decrease the loss, while oracle-abs penalizes any",
            "difference, |C(D|W′) −C(D|W)|. While both discourage pruning which increases the loss, the\noracle-loss version encourages pruning which may decrease the loss, while oracle-abs penalizes any\npruning in proportion to its change in loss, regardless of the direction of change.\nWhile the oracle is optimal for this greedy procedure, it is prohibitively costly to compute, requiring\n||W ′||0 evaluations on a training dataset, one evaluation for each remaining non-zero parameter. Since\nestimation of parameter importance is key to both the accuracy and the efﬁciency of this pruning\napproach, we propose and evaluate several criteria in terms of performance and estimation cost.\n2.2\nCRITERIA FOR PRUNING\nThere are many heuristic criteria which are much more computationally efﬁcient than the oracle. For\nthe speciﬁc case of evaluating the importance of a feature map (and implicitly the set of convolutional\nkernels from which it is computed), reasonable criteria include: the combined ℓ2-norm of the",
            "the speciﬁc case of evaluating the importance of a feature map (and implicitly the set of convolutional\nkernels from which it is computed), reasonable criteria include: the combined ℓ2-norm of the\nkernel weights, the mean, standard deviation or percentage of the feature map’s activation, and\nmutual information between activations and predictions. We describe these criteria in the following\nparagraphs and propose a new criterion which is based on the Taylor expansion.\nMinimum weight.\nPruning by magnitude of kernel weights is perhaps the simplest possible crite-\nrion, and it does not require any additional computation during the ﬁne-tuning process. In case of prun-\ning according to the norm of a set of weights, the criterion is evaluated as: ΘMW : RCℓ−1×p×p →R,\nwith ΘMW (w) =\n1\n|w|\nP\ni w2\ni , where |w| is dimensionality of the set of weights after vectorization.\nThe motivation to apply this type of pruning is that a convolutional kernel with low ℓ2 norm detects",
            "1\n|w|\nP\ni w2\ni , where |w| is dimensionality of the set of weights after vectorization.\nThe motivation to apply this type of pruning is that a convolutional kernel with low ℓ2 norm detects\nless important features than those with a high norm. This can be aided during training by applying ℓ1\nor ℓ2 regularization, which will push unimportant kernels to have smaller values.\nActivation.\nOne of the reasons for the popularity of the ReLU activation is the sparsity in activation\nthat is induced, allowing convolutional layers to act as feature detectors. Therefore it is reasonable\nto assume that if an activation value (an output feature map) is small then this feature detector\nis not important for prediction task at hand. We may evaluate this by mean activation, ΘMA :\nRHl×Wℓ×Cℓ→R, with ΘMA(a) =\n1\n|a|\nP\ni ai for activation a = z(k)\nl\n, or by the standard deviation\nof the activation, ΘMA_std(a) =\nq\n1\n|a|\nP\ni(ai −µa)2.",
            "RHl×Wℓ×Cℓ→R, with ΘMA(a) =\n1\n|a|\nP\ni ai for activation a = z(k)\nl\n, or by the standard deviation\nof the activation, ΘMA_std(a) =\nq\n1\n|a|\nP\ni(ai −µa)2.\n2While our notation is at times speciﬁc to 2D convolutions, the methods are applicable to 3D convolutions,\nas well as fully connected layers.\n3",
            "Published as a conference paper at ICLR 2017\nMutual information.\nMutual information (MI) is a measure of how much information is present in\none variable about another variable. We apply MI as a criterion for pruning, ΘMI : RHl×Wℓ×Cℓ→R,\nwith ΘMI(a) = MI(a, y), where y is the target of neural network. MI is deﬁned for continuous\nvariables, so to simplify computation, we exchange it with information gain (IG), which is deﬁned\nfor quantized variables IG(y|x) = H(x) + H(y) −H(x, y), where H(x) is the entropy of variable\nx. We accumulate statistics on activations and ground truth for a number of updates, then quantize\nthe values and compute IG.\nTaylor expansion.\nWe phrase pruning as an optimization problem, trying to ﬁnd W′ with bounded\nnumber of non-zero elements that minimize\n\f\f∆C(hi)\n\f\f = |C(D|W′) −C(D|W)|. With this approach\nbased on the Taylor expansion, we directly approximate change in the loss function from removing a",
            "number of non-zero elements that minimize\n\f\f∆C(hi)\n\f\f = |C(D|W′) −C(D|W)|. With this approach\nbased on the Taylor expansion, we directly approximate change in the loss function from removing a\nparticular parameter. Let hi be the output produced from parameter i. In the case of feature maps,\nh = {z(1)\n0 , z(2)\n0 , ..., z(Cℓ)\nL\n}. For notational convenience, we consider the cost function equally depen-\ndent on parameters and outputs computed from parameters: C(D|hi) = C(D|(w, b)i). Assuming\nindependence of parameters, we have:\n\f\f∆C(hi)\n\f\f =\n\f\fC(D, hi = 0) −C(D, hi)\n\f\f,\n(3)\nwhere C(D, hi = 0) is a cost value if output hi is pruned, while C(D, hi) is the cost if it is not pruned.\nWhile parameters are in reality inter-dependent, we already make an independence assumption at\neach gradient step during training.\nTo approximate ∆C(hi), we use the ﬁrst-degree Taylor polynomial. For a function f(x), the Taylor\nexpansion at point x = a is\nf(x) =\nP\nX\np=0\nf (p)(a)\np!\n(x −a)p + Rp(x),\n(4)",
            "To approximate ∆C(hi), we use the ﬁrst-degree Taylor polynomial. For a function f(x), the Taylor\nexpansion at point x = a is\nf(x) =\nP\nX\np=0\nf (p)(a)\np!\n(x −a)p + Rp(x),\n(4)\nwhere f (p)(a) is the p-th derivative of f evaluated at point a, and Rp(x) is the p-th order remainder.\nApproximating C(D, hi = 0) with a ﬁrst-order Taylor polynomial near hi = 0, we have:\nC(D, hi = 0) = C(D, hi) −δC\nδhi\nhi + R1(hi = 0).\n(5)\nThe remainder R1(hi = 0) can be calculated through the Lagrange form:\nR1(hi = 0) =\nδ2C\nδ(h2\ni = ξ)\nh2\ni\n2 ,\n(6)\nwhere ξ is a real number between 0 and hi. However, we neglect this ﬁrst-order remainder, largely\ndue to the signiﬁcant calculation required, but also in part because the widely-used ReLU activation\nfunction encourages a smaller second order term.\nFinally, by substituting Eq. (5) into Eq. (3) and ignoring the remainder, we have ΘT E\n:\nRHl×Wl×Cl →R+, with\nΘT E(hi) =\n\f\f∆C(hi)\n\f\f =\n\f\fC(D, hi) −δC\nδhi\nhi −C(D, hi)\n\f\f =\n\f\f\f\f\nδC\nδhi\nhi\n\f\f\f\f.\n(7)",
            "Finally, by substituting Eq. (5) into Eq. (3) and ignoring the remainder, we have ΘT E\n:\nRHl×Wl×Cl →R+, with\nΘT E(hi) =\n\f\f∆C(hi)\n\f\f =\n\f\fC(D, hi) −δC\nδhi\nhi −C(D, hi)\n\f\f =\n\f\f\f\f\nδC\nδhi\nhi\n\f\f\f\f.\n(7)\nIntuitively, this criterion prunes parameters that have an almost ﬂat gradient of the cost function w.r.t.\nfeature map hi. This approach requires accumulation of the product of the activation and the gradient\nof the cost function w.r.t. to the activation, which is easily computed from the same computations for\nback-propagation. ΘT E is computed for a multi-variate output, such as a feature map, by\nΘT E(z(k)\nl\n) =\n\f\f\f\f\n1\nM\nX\nm\nδC\nδz(k)\nl,m\nz(k)\nl,m\n\f\f\f\f,\n(8)\nwhere M is length of vectorized feature map. For a minibatch with T > 1 examples, the criterion is\ncomputed for each example separately and averaged over T.\nIndependently of our work, Figurnov et al. (2016) came up with similar metric based on the Taylor",
            "computed for each example separately and averaged over T.\nIndependently of our work, Figurnov et al. (2016) came up with similar metric based on the Taylor\nexpansion, called impact, to evaluate importance of spatial cells in a convolutional layer. It shows\nthat the same metric can be applied to evaluate importance of different groups of parameters.\n4",
            "Published as a conference paper at ICLR 2017\nRelation to Optimal Brain Damage.\nThe Taylor criterion proposed above relies on approximating\nthe change in loss caused by removing a feature map. The core idea is the same as in Optimal Brain\nDamage (OBD) (LeCun et al., 1990). Here we consider the differences more carefully.\nThe primary difference is the treatment of the ﬁrst-order term of the Taylor expansion, in our notation\ny = δC\nδhh for cost function C and hidden layer activation h. After sufﬁcient training epochs, the\ngradient term tends to zero: δC\nδh →0 and E(y) = 0. At face value y offers little useful information,\nhence OBD regards the term as zero and focuses on the second-order term.\nHowever, the variance of y is non-zero and correlates with the stability of the local function w.r.t.\nactivation h. By considering the absolute change in the cost3 induced by pruning (as in Eq. 3), we use",
            "activation h. By considering the absolute change in the cost3 induced by pruning (as in Eq. 3), we use\nthe absolute value of the ﬁrst-order term, |y|. Under assumption that samples come from independent\nand identical distribution, E(|y|) = σ\n√\n2/√π where σ is the standard deviation of y, known as the\nexpected value of the half-normal distribution. So, while y tends to zero, the expectation of |y| is\nproportional to the variance of y, a value which is empirically more informative as a pruning criterion.\nAs an additional beneﬁt, we avoid the computation of the second-order Taylor expansion term, or its\nsimpliﬁcation - diagonal of the Hessian, as required in OBD.\nWe found important to compare proposed Taylor criteria to OBD. As described in the original\npapers (LeCun et al., 1990; 1998), OBD can be efﬁciently implemented similarly to standard back\npropagation algorithm doubling backward propagation time and memory usage when used together",
            "papers (LeCun et al., 1990; 1998), OBD can be efﬁciently implemented similarly to standard back\npropagation algorithm doubling backward propagation time and memory usage when used together\nwith standard ﬁne-tuning. Efﬁcient implementation of the original OBD algorithm might require\nsigniﬁcant changes to the framework based on automatic differentiation like Theano to efﬁciently\ncompute only diagonal of the Hessian instead of the full matrix. Several researchers tried to tackle this\nproblem with approximation techniques (Martens, 2010; Martens et al., 2012). In our implementation,\nwe use efﬁcient way of computing Hessian-vector product (Pearlmutter, 1994) and matrix diagonal\napproximation proposed by (Bekas et al., 2007), please refer to more details in appendix. With\ncurrent implementation, OBD is 30 times slower than Taylor technique for saliency estimation, and 3\ntimes slower for iterative pruning, however with different implementation can only be 50% slower as",
            "current implementation, OBD is 30 times slower than Taylor technique for saliency estimation, and 3\ntimes slower for iterative pruning, however with different implementation can only be 50% slower as\nmentioned in the original paper.\nAverage Percentage of Zeros (APoZ).\nHu et al. (2016) proposed to explore sparsity in activations\nfor network pruning. ReLU activation function imposes sparsity during inference, and average\npercentage of positive activations at the output can determine importance of the neuron. Intuitively,\nit is a good criteria, however feature maps at the ﬁrst layers have similar APoZ regardless of the\nnetwork’s target as they learn to be Gabor like ﬁlters. We will use APoZ to estimate saliency of\nfeature maps.\n2.3\nNORMALIZATION\nSome criteria return “raw” values, whose scale varies with the depth of the parameter’s layer in the\nnetwork. A simple layer-wise ℓ2-normalization can achieve adequate rescaling across layers:\nˆΘ(z(k)\nl\n)=\nΘ(z(k)\nl\n)\nqP\nj\n\u0000Θ(z(j)\nl )\n\u00012 .\n2.4",
            "network. A simple layer-wise ℓ2-normalization can achieve adequate rescaling across layers:\nˆΘ(z(k)\nl\n)=\nΘ(z(k)\nl\n)\nqP\nj\n\u0000Θ(z(j)\nl )\n\u00012 .\n2.4\nFLOPS REGULARIZED PRUNING\nOne of the main reasons to apply pruning is to reduce number of operations in the network. Feature\nmaps from different layers require different amounts of computation due the number and sizes of input\nfeature maps and convolution kernels. To take this into account we introduce FLOPs regularization:\nΘ(z(k)\nl\n) = Θ(z(k)\nl\n) −λΘflops\nl\n,\n(9)\nwhere λ controls the amount of regularization. For our experiments, we use λ = 10−3. Θflops is\ncomputed under the assumption that convolution is implemented as a sliding window (see Appendix).\nOther regularization conditions may be applied, e.g. storage size, kernel sizes, or memory footprint.\n3OBD approximates the signed difference in loss, while our method approximates absolute difference in loss.\nWe ﬁnd in our results that pruning based on absolute difference yields better accuracy.",
            "3OBD approximates the signed difference in loss, while our method approximates absolute difference in loss.\nWe ﬁnd in our results that pruning based on absolute difference yields better accuracy.\n5",
            "Published as a conference paper at ICLR 2017\nFigure 2: Global statistics of oracle ranking,\nshown by layer for Birds-200 transfer learning.\nFigure 3: Pruning without ﬁne-tuning using\noracle ranking for Birds-200 transfer learning.\n3\nRESULTS\nWe empirically study the pruning criteria and procedure detailed in the previous section for a variety of\nproblems. We focus many experiments on transfer learning problems, a setting where pruning seems\nto excel. We also present results for pruning large networks on their original tasks for more direct\ncomparison with the existing pruning literature. Experiments are performed within Theano (Theano\nDevelopment Team, 2016). Training and pruning are performed on the respective training sets for\neach problem, while results are reported on appropriate holdout sets, unless otherwise indicated. For\nall experiments we prune a single feature map at every pruning iteration, allowing ﬁne-tuning and",
            "each problem, while results are reported on appropriate holdout sets, unless otherwise indicated. For\nall experiments we prune a single feature map at every pruning iteration, allowing ﬁne-tuning and\nre-evaluation of the criterion to account for dependency between parameters.\n3.1\nCHARACTERIZING THE ORACLE RANKING\nWe begin by explicitly computing the oracle for a single pruning iteration of a visual transfer learning\nproblem. We ﬁne-tune the VGG-16 network (Simonyan & Zisserman, 2014) for classiﬁcation of bird\nspecies using the Caltech-UCSD Birds 200-2011 dataset (Wah et al., 2011). The dataset consists of\nnearly 6000 training images and 5700 test images, covering 200 species. We ﬁne-tune VGG-16 for\n60 epochs with learning rate 0.0001 to achieve a test accuracy of 72.2% using uncropped images.\nTo compute the oracle, we evaluate the change in loss caused by removing each individual feature\nmap from the ﬁne-tuned VGG-16 network. (See Appendix A.3 for additional analysis.) We rank",
            "To compute the oracle, we evaluate the change in loss caused by removing each individual feature\nmap from the ﬁne-tuned VGG-16 network. (See Appendix A.3 for additional analysis.) We rank\nfeature maps by their contributions to the loss, where rank 1 indicates the most important feature\nmap—removing it results in the highest increase in loss—and rank 4224 indicates the least important.\nStatistics of global ranks are shown in Fig. 2 grouped by convolutional layer. We observe: (1)\nMedian global importance tends to decrease with depth. (2) Layers with max-pooling tend to be\nmore important than those without. (VGG-16 has pooling after layers 2, 4, 7, 10, and 13.) However,\n(3) maximum and minimum ranks show that every layer has some feature maps that are globally\nimportant and others that are globally less important. Taken together with the results of subsequent\nexperiments, we opt for encouraging a balanced pruning that distributes selection across all layers.",
            "experiments, we opt for encouraging a balanced pruning that distributes selection across all layers.\nNext, we iteratively prune the network using pre-computed oracle ranking. In this experiment, we do\nnot update the parameters of the network or the oracle ranking between iterations. Training accuracy\nis illustrated in Fig. 3 over many pruning iterations. Surprisingly, pruning by smallest absolute\nchange in loss (Oracle-abs) yields higher accuracy than pruning by the net effect on loss (Oracle-loss).\nEven though the oracle indicates that removing some feature maps individually may decrease loss,\ninstability accumulates due the large absolute changes that are induced. These results support pruning\nby absolute difference in cost, as constructed in Eq. 1.\n3.2\nEVALUATING PROPOSED CRITERIA VERSUS THE ORACLE\nTo evaluate computationally efﬁcient criteria as substitutes for the oracle, we compute Spearman’s",
            "3.2\nEVALUATING PROPOSED CRITERIA VERSUS THE ORACLE\nTo evaluate computationally efﬁcient criteria as substitutes for the oracle, we compute Spearman’s\nrank correlation, an estimate of how well two predictors provide monotonically related outputs,\n6",
            "Published as a conference paper at ICLR 2017\nAlexNet / Flowers-102\nVGG-16 / Birds-200\nWeight\nActivation\nOBD\nTaylor\nWeight\nActivation\nOBD\nTaylor\nMutual\nMean\nS.d.\nAPoZ\nMean\nS.d.\nAPoZ\nInfo.\nPer layer\n0.17\n0.65\n0.67\n0.54\n0.64\n0.77\n0.27\n0.56\n0.57\n0.35\n0.59\n0.73\n0.28\nAll layers\n0.28\n0.51\n0.53\n0.41\n0.68\n0.37\n0.34\n0.35\n0.30\n0.43\n0.65\n0.14\n0.35\n(w/ ℓ2-norm)\n0.13\n0.63\n0.61\n0.60\n-\n0.75\n0.33\n0.64\n0.66\n0.51\n-\n0.73\n0.47\nAlexNet / Birds-200\nVGG-16 / Flowers-102\nPer layer\n0.36\n0.57\n0.65\n0.42\n0.54\n0.81\n0.19\n0.51\n0.47\n0.36\n0.21\n0.6\nAll layers\n0.32\n0.37\n0.51\n0.28\n0.61\n0.37\n0.35\n0.53\n0.45\n0.61\n0.28\n0.02\n(w/ ℓ2-norm)\n0.23\n0.54\n0.57\n0.49\n-\n0.78\n0.28\n0.66\n0.65\n0.61\n-\n0.7\nAlexNet / ImageNet\nPer layer\n0.57\n0.09\n0.19\n−0.06\n0.58\n0.58\nAll layers\n0.67\n0.00\n0.13\n−0.08\n0.72\n0.11\n(w/ ℓ2-norm)\n0.44\n0.10\n0.19\n0.19\n-\n0.55\nTable 1: Spearman’s rank correlation of criteria vs. oracle for convolutional feature maps of VGG-16\nand AlexNet ﬁne-tuned on Birds-200 and Flowers-102 datasets, and AlexNet trained on ImageNet.",
            "0.55\nTable 1: Spearman’s rank correlation of criteria vs. oracle for convolutional feature maps of VGG-16\nand AlexNet ﬁne-tuned on Birds-200 and Flowers-102 datasets, and AlexNet trained on ImageNet.\nFigure 4: Pruning of feature maps in VGG-16 ﬁne-tuned on the Birds-200 dataset.\neven if their relationship is not linear. Given the difference between oracle4 and criterion ranks\ndi = rank(Θoracle(i))−rank(Θcriterion(i)) for each parameter i, the rank correlation is computed:\nS = 1 −\n6\nN(N 2 −1)\nN\nX\ni=1\ndi\n2,\n(10)\nwhere N is the number of parameters (and the highest rank). This correlation coefﬁcient takes values\nin [−1, 1], where −1 implies full negative correlation, 0 no correlation, and 1 full positive correlation.\nWe show Spearman’s correlation in Table 1 to compare the oracle-abs ranking to rankings by different\ncriteria on a set of networks/datasets some of which are going to be introduced later. Data-dependent",
            "criteria on a set of networks/datasets some of which are going to be introduced later. Data-dependent\ncriteria (all except weight magnitude) are computed on training data during the ﬁne-tuning before\nor between pruning iterations. As a sanity check, we evaluate random ranking and observe 0.0\ncorrelation across all layers. “Per layer” analysis shows ranking within each convolutional layer,\nwhile “All layers” describes ranking across layers. While several criteria do not scale well across\nlayers with raw values, a layer-wise ℓ2-normalization signiﬁcantly improves performance. The Taylor\ncriterion has the highest correlation among the criteria, both within layers and across layers (with ℓ2\nnormalization). OBD shows the best correlation across layers when no normalization used; it also\nshows best results for correlation on ImageNet dataset. (See Appendix A.2 for further analysis.)\n3.3\nPRUNING FINE-TUNED IMAGENET NETWORKS",
            "shows best results for correlation on ImageNet dataset. (See Appendix A.2 for further analysis.)\n3.3\nPRUNING FINE-TUNED IMAGENET NETWORKS\nWe now evaluate the full iterative pruning procedure on two transfer learning problems. We focus on\nreducing the number of convolutional feature maps and the total estimated ﬂoating point operations\n(FLOPs). Fine-grained recognition is difﬁcult for relatively small datasets without relying on transfer\n4We use Oracle-abs because of better performance in previous experiment\n7",
            "Published as a conference paper at ICLR 2017\nFigure 5: Pruning of feature maps in AlexNet on ﬁne-tuned on Flowers-102.\nlearning. Branson et al. (2014) show that training CNN from scratch on the Birds-200 dataset achieves\ntest accuracy of only 10.9%. We compare results to training a randomly initialized CNN with half\nthe number of parameters per layer, denoted \"from scratch\".\nFig. 4 shows pruning of VGG-16 after ﬁne-tuning on the Birds-200 dataset (as described previously).\nAt each pruning iteration, we remove a single feature map and then perform 30 minibatch SGD\nupdates with batch-size 32, momentum 0.9, learning rate 10−4, and weight decay 10−4. The ﬁgure\ndepicts accuracy relative to the pruning rate (left) and estimated GFLOPs (right). The Taylor criterion\nshows the highest accuracy for nearly the entire range of pruning ratios, and with FLOPs regularization\ndemonstrates the best performance relative to the number of operations. OBD shows slightly worse",
            "demonstrates the best performance relative to the number of operations. OBD shows slightly worse\nperformance of pruning in terms of parameters, however signiﬁcantly worse in terms of FLOPs.\nIn Fig. 5, we show pruning of the CaffeNet implementation of AlexNet (Krizhevsky et al., 2012) after\nadapting to the Oxford Flowers 102 dataset (Nilsback & Zisserman, 2008), with 2040 training and\n6129 test images from 102 species of ﬂowers. Criteria correlation with oracle-abs is summarized in\nTable 1. We initially ﬁne-tune the network for 20 epochs using a learning rate of 0.001, achieving a\nﬁnal test accuracy of 80.1%. Then pruning procedes as previously described for Birds-200, except\nwith only 10 mini-batch updates between pruning iterations. We observe the superior performance of\nthe Taylor and OBD criteria in both number of parameters and GFLOPs.\nWe observed that Taylor criterion shows the best performance which is closely followed by OBD with",
            "the Taylor and OBD criteria in both number of parameters and GFLOPs.\nWe observed that Taylor criterion shows the best performance which is closely followed by OBD with\na bit lower Spearman’s rank correlation coefﬁcient. Implementing OBD takes more effort because of\ncomputation of diagonal of the Hessian and it is 50% to 300% slower than Taylor criteria that relies\non ﬁrst order gradient only.\nFig. 6 shows pruning with the Taylor technique and a varying number of ﬁne-tuning updates between\npruning iterations. Increasing the number of updates results in higher accuracy, but at the cost of\nadditional runtime of the pruning procedure.\nDuring pruning we observe a small drop in accuracy. One of the reasons is ﬁne-tuning between\npruning iterations. Accuracy of the initial network can be improved with longer ﬁne tunning and\nsearch of better optimization parameters. For example accuracy of unpruned VGG16 network on",
            "pruning iterations. Accuracy of the initial network can be improved with longer ﬁne tunning and\nsearch of better optimization parameters. For example accuracy of unpruned VGG16 network on\nBirds-200 goes up to 75% after extra 128k updates. And AlexNet on Flowers-102 goes up to 82.9%\nafter 130k updates. It should be noted that with farther ﬁne-tuning of pruned networks we can achieve\nhigher accuracy as well, therefore the one-to-one comparison of accuracies is rough.\n3.4\nPRUNING A RECURRENT 3D-CNN NETWORK FOR HAND GESTURE RECOGNITION\nMolchanov et al. (2016) learn to recognize 25 dynamic hand gestures in streaming video with a large\nrecurrent neural network. The network is constructed by adding recurrent connections to a 3D-CNN\npretrained on the Sports-1M video dataset (Karpathy et al., 2014) and ﬁne tuning on a gesture dataset.\nThe full network achieves an accuracy of 80.7% when trained on the depth modality, but a single",
            "pretrained on the Sports-1M video dataset (Karpathy et al., 2014) and ﬁne tuning on a gesture dataset.\nThe full network achieves an accuracy of 80.7% when trained on the depth modality, but a single\ninference requires an estimated 37.8 GFLOPs, too much for deployment on an embedded GPU. After\nseveral iterations of pruning with the Taylor criterion with learning rate 0.0003, momentum 0.9,\nFLOPs regularization 10−3, we reduce inference to 3.0 GFLOPs, as shown in Fig. 7. While pruning\n8",
            "Published as a conference paper at ICLR 2017\nFigure 6: Varying the number of minibatch\nupdates between pruning iterations with\nAlexNet/Flowers-102 and the Taylor criterion.\nFigure 7: Pruning of a recurrent 3D-CNN for\ndynamic hand gesture recognition\n(Molchanov et al., 2016).\nFigure 8: Pruning of AlexNet on Imagenet with varying number of updates between pruning iterations.\nincreases classiﬁcation error by nearly 6%, additional ﬁne-tuning restores much of the lost accuracy,\nyielding a ﬁnal pruned network with a 12.6× reduction in GFLOPs and only a 2.5% loss in accuracy.\n3.5\nPRUNING NETWORKS FOR IMAGENET\nFigure 9: Pruning of the VGG-16 network on\nImageNet, with additional following ﬁne-tuning at\n11.5 and 8 GFLOPs.\nWe also test our pruning scheme on the large-\nscale ImageNet classiﬁcation task. In the ﬁrst\nexperiment, we begin with a trained CaffeNet\nimplementation of AlexNet with 79.2% top-5\nvalidation accuracy. Between pruning iterations,\nwe ﬁne-tune with learning rate 10−4, momen-",
            "experiment, we begin with a trained CaffeNet\nimplementation of AlexNet with 79.2% top-5\nvalidation accuracy. Between pruning iterations,\nwe ﬁne-tune with learning rate 10−4, momen-\ntum 0.9, weight decay 10−4, batch size 32, and\ndrop-out 50%. Using a subset of 5000 training\nimages, we compute oracle-abs and Spearman’s\nrank correlation with the criteria, as shown in\nTable 1. Pruning traces are illustrated in Fig. 8.\nWe observe: 1) Taylor performs better than ran-\ndom or minimum weight pruning when 100 up-\ndates are used between pruning iterations. When\nresults are displayed w.r.t. FLOPs, the differ-\nence with random pruning is only 0%−4%, but\nthe difference is higher, 1%−10%, when plot-\nted with the number of feature maps pruned. 2)\nIncreasing the number of updates from 100 to\n1000 improves performance of pruning signiﬁ-\ncantly for both the Taylor criterion and random\npruning.\n9",
            "Published as a conference paper at ICLR 2017\nHardware\nBatch\nAccuracy\nTime, ms\nAccuracy\nTime (speed up)\nAccuracy\nTime (speed up)\nAlexNet / Flowers-102, 1.46 GFLOPs\n41% feature maps, 0.4 GFLOPs\n19.5% feature maps, 0.2 GFLOPs\nCPU: Intel Core i7-5930K\n16\n80.1%\n226.4\n79.8%(-0.3%)\n121.4 (1.9x)\n74.1%(-6.0%)\n87.0 (2.6x)\nGPU: GeForce GTX TITAN X (Pascal)\n16\n4.8\n2.4 (2.0x)\n1.9 (2.5x)\nGPU: GeForce GTX TITAN X (Pascal)\n512\n88.3\n36.6 (2.4x)\n27.4 (3.2x)\nGPU: NVIDIA Jetson TX1\n32\n169.2\n73.6 (2.3x)\n58.6 (2.9x)\nVGG-16 / ImageNet, 30.96 GFLOPs\n66% feature maps, 11.5 GFLOPs\n52% feature maps, 8.0 GFLOPs\nCPU: Intel Core i7-5930K\n16\n89.3%\n2564.7\n87.0% (-2.3%)\n1483.3 (1.7x)\n84.5% (-4.8%)\n1218.4 (2.1x)\nGPU: GeForce GTX TITAN X (Pascal)\n16\n68.3\n31.0 (2.2x)\n20.2 (3.4x)\nGPU: NVIDIA Jetson TX1\n4\n456.6\n182.5 (2.5x)\n138.2 (3.3x)\nR3DCNN / nvGesture, 37.8 GFLOPs\n25% feature maps, 3 GFLOPs\nGPU: GeForce GT 730M\n1\n80.7%\n438.0\n78.2% (-2.5%)\n85.0 (5.2x)",
            "20.2 (3.4x)\nGPU: NVIDIA Jetson TX1\n4\n456.6\n182.5 (2.5x)\n138.2 (3.3x)\nR3DCNN / nvGesture, 37.8 GFLOPs\n25% feature maps, 3 GFLOPs\nGPU: GeForce GT 730M\n1\n80.7%\n438.0\n78.2% (-2.5%)\n85.0 (5.2x)\nTable 2: Actual speed up of networks pruned by Taylor criterion for various hardware setup. All\nmeasurements were performed with PyTorch with cuDNN v5.1.0, except R3DCNN which was\nimplemented in C++ with cuDNN v4.0.4). Results for ImageNet dataset are reported as top-5\naccuracy on validation set. Results on AlexNet / Flowers-102 are reported for pruning with 1000\nupdates between iterations and no ﬁne-tuning after pruning.\nFor a second experiment, we prune a trained VGG-16 network with the same parameters as before,\nexcept enabling FLOPs regularization. We stop pruning at two points, 11.5 and 8.0 GFLOPs, and\nﬁne-tune both models for an additional ﬁve epochs with learning rate 10−4. Fine-tuning after pruning",
            "except enabling FLOPs regularization. We stop pruning at two points, 11.5 and 8.0 GFLOPs, and\nﬁne-tune both models for an additional ﬁve epochs with learning rate 10−4. Fine-tuning after pruning\nsigniﬁcantly improves results: the network pruned to 11.5 GFLOPs improves from 83% to 87% top-5\nvalidation accuracy, and the network pruned to 8.0 GFLOPs improves from 77.8% to 84.5%.\n3.6\nSPEED UP MEASUREMENTS\nDuring pruning we were measuring reduction in computations by FLOPs, which is a common practice\n(Han et al., 2015; Lavin, 2015a;b). Improvements in FLOPs result in monotonically decreasing\ninference time of the networks because of removing entire feature map from the layer. However,\ntime consumed by inference dependents on particular implementation of convolution operator,\nparallelization algorithm, hardware, scheduling, memory transfer rate etc. Therefore we measure\nimprovement in the inference time for selected networks to see real speed up compared to unpruned",
            "parallelization algorithm, hardware, scheduling, memory transfer rate etc. Therefore we measure\nimprovement in the inference time for selected networks to see real speed up compared to unpruned\nnetworks in Table 2. We observe signiﬁcant speed ups by proposed pruning scheme.\n4\nCONCLUSIONS\nWe propose a new scheme for iteratively pruning deep convolutional neural networks. We ﬁnd: 1)\nCNNs may be successfully pruned by iteratively removing the least important parameters—feature\nmaps in this case—according to heuristic selection criteria; 2) a Taylor expansion-based criterion\ndemonstrates signiﬁcant improvement over other criteria; 3) per-layer normalization of the criterion\nis important to obtain global scaling.\nREFERENCES\nJose M Alvarez and Mathieu Salzmann. Learning the Number of Neurons in Deep Networks. In\nD. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural\nInformation Processing Systems 29, pp. 2262–2270. Curran Associates, Inc., 2016.",
            "D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural\nInformation Processing Systems 29, pp. 2262–2270. Curran Associates, Inc., 2016.\nSajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural\nnetworks. arXiv preprint arXiv:1512.08571, 2015. URL http://arxiv.org/abs/1512.\n08571.\nCostas Bekas, Effrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.\nApplied numerical mathematics, 57(11):1214–1229, 2007.\nSteve Branson, Grant Van Horn, Serge Belongie, and Pietro Perona. Bird species categorization using\npose normalized deep convolutional nets. arXiv preprint arXiv:1406.2952, 2014.\nYann Dauphin, Harm de Vries, and Yoshua Bengio. Equilibrated adaptive learning rates for non-\nconvex optimization. In Advances in Neural Information Processing Systems, pp. 1504–1512,\n2015.\n10",
            "Published as a conference paper at ICLR 2017\nMikhail Figurnov, Aizhan Ibraimova, Dmitry P Vetrov, and Pushmeet Kohli. PerforatedCNNs:\nAcceleration through elimination of redundant convolutions. In Advances in Neural Information\nProcessing Systems, pp. 947–955, 2016.\nSuyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with\nlimited numerical precision. CoRR, abs/1502.02551, 392, 2015. URL http://arxiv.org/\nabs/1502.025513.\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\nefﬁcient neural network. In Advances in Neural Information Processing Systems, pp. 1135–1143,\n2015.\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J.\nDally. EIE: Efﬁcient inference engine on compressed deep neural network. In Proceedings of the\n43rd International Symposium on Computer Architecture, ISCA ’16, pp. 243–254, Piscataway, NJ,\nUSA, 2016. IEEE Press.",
            "43rd International Symposium on Computer Architecture, ISCA ’16, pp. 243–254, Piscataway, NJ,\nUSA, 2016. IEEE Press.\nBabak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain\nsurgeon. In Advances in Neural Information Processing Systems (NIPS), pp. 164–171, 1993.\nHengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network trimming: A data-driven\nneuron pruning approach towards efﬁcient deep architectures. arXiv preprint arXiv:1607.03250,\n2016.\nAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei.\nLarge-scale video classiﬁcation with convolutional neural networks. In CVPR, 2014.\nYong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-\npression of deep convolutional neural networks for fast and low power mobile applications. In\nProceedings of the International Conference on Learning Representations (ICLR), 2015.",
            "pression of deep convolutional neural networks for fast and low power mobile applications. In\nProceedings of the International Conference on Learning Representations (ICLR), 2015.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, pp. 1097–1105,\n2012.\nAndrew Lavin. maxDNN: An Efﬁcient Convolution Kernel for Deep Learning with Maxwell GPUs.\nCoRR, abs/1501.06633, 2015a. URL http://arxiv.org/abs/1501.06633.\nAndrew Lavin. Fast algorithms for convolutional neural networks. arXiv preprint arXiv:1509.09308,\n2015b.\nVadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2554–2564, 2016.\nYann LeCun, J. S. Denker, S. Solla, R. E. Howard, and L. D. Jackel. Optimal brain damage. In\nAdvances in Neural Information Processing Systems (NIPS), 1990.",
            "Yann LeCun, J. S. Denker, S. Solla, R. E. Howard, and L. D. Jackel. Optimal brain damage. In\nAdvances in Neural Information Processing Systems (NIPS), 1990.\nYann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus Robert Müller. Efﬁcient BackProp, pp. 9–50.\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 1998.\nJames Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International\nConference on Machine Learning (ICML-10), pp. 735–742, 2010.\nJames Martens, Ilya Sutskever, and Kevin Swersky. Estimating the Hessian by back-propagating\ncurvature. arXiv preprint arXiv:1206.6464, 2012.\nPavlo Molchanov, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, and Jan Kautz.\nOnline detection and classiﬁcation of dynamic hand gestures with recurrent 3d convolutional\nneural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2016.\nM-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In",
            "neural network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\nJune 2016.\nM-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec\n2008.\n11",
            "Published as a conference paper at ICLR 2017\nBarak A. Pearlmutter. Fast Exact Multiplication by the Hessian. Neural Computation, 6:147–160,\n1994.\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet\nClassiﬁcation Using Binary Convolutional Neural Networks. CoRR, abs/1603.05279, 2016. URL\nhttp://arxiv.org/abs/1603.05279.\nRussell Reed. Pruning algorithms-a survey. IEEE transactions on Neural Networks, 4(5):740–747,\n1993.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet\nLarge Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115\n(3):211–252, 2015.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.\nCoRR, abs/1409.1556, 2014.\nSuraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks. In",
            "CoRR, abs/1409.1556, 2014.\nSuraj Srinivas and R. Venkatesh Babu. Data-free parameter pruning for deep neural networks. In\nMark W. Jones Xianghua Xie and Gary K. L. Tam (eds.), Proceedings of the British Machine\nVision Conference (BMVC), pp. 31.1–31.12. BMVA Press, September 2015.\nTheano Development Team. Theano: A Python framework for fast computation of mathematical\nexpressions. arXiv e-prints, abs/1605.02688, May 2016. URL http://arxiv.org/abs/\n1605.02688.\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\ndeep neural networks. In Advances in Neural Information Processing Systems, pp. 2074–2082,\n2016.\nHao Zhou, Jose M. Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In European\nConference on Computer Vision, pp. 662–677, Amsterdam, the Netherlands, October 2016.\n12",
            "Published as a conference paper at ICLR 2017\nA\nAPPENDIX\nA.1\nFLOPS COMPUTATION\nTo compute the number of ﬂoating-point operations (FLOPs), we assume convolution is implemented\nas a sliding window and that the nonlinearity function is computed for free. For convolutional kernels\nwe have:\nFLOPs = 2HW(CinK2 + 1)Cout,\n(11)\nwhere H, W and Cin are height, width and number of channels of the input feature map, K is the\nkernel width (assumed to be symmetric), and Cout is the number of output channels.\nFor fully connected layers we compute FLOPs as:\nFLOPs = (2I −1)O,\n(12)\nwhere I is the input dimensionality and O is the output dimensionality.\nWe apply FLOPs regularization during pruning to prune neurons with higher FLOPs ﬁrst. FLOPs per\nconvolutional neuron in every layer:\nVGG16: Θflops = [3.1, 57.8, 14.1, 28.9, 7.0, 14.5, 14.5, 3.5, 7.2, 7.2, 1.8, 1.8, 1.8, 1.8]\nAlexNet: Θflops = [2.3, 1.7, 0.8, 0.6, 0.6]\nR3DCNN: Θflops = [5.6, 86.9, 21.7, 43.4, 5.4, 10.8, 1.4, 1.4]\nA.2",
            "AlexNet: Θflops = [2.3, 1.7, 0.8, 0.6, 0.6]\nR3DCNN: Θflops = [5.6, 86.9, 21.7, 43.4, 5.4, 10.8, 1.4, 1.4]\nA.2\nNORMALIZATION ACROSS LAYERS\nScaling a criterion across layers is very important for pruning. If the criterion is not properly scaled,\nthen a hand-tuned multiplier would need to be selected for each layer. Statistics of feature map\nranking by different criteria are shown in Fig. 10. Without normalization (Fig. 14a–14d), the weight\nmagnitude criterion tends to rank feature maps from the ﬁrst layers more important than last layers;\nthe activation criterion ranks middle layers more important; and Taylor ranks ﬁrst layers higher. After\nℓ2 normalization (Fig. 10d–10f), all criteria have a shape more similar to the oracle, where each layer\nhas some feature maps which are highly important and others which are unimportant.\n(a) Weight\n(b) Activation (mean)\n(c) Taylor\n(d) Weight + ℓ2\n(e) Activation (mean) + ℓ2\n(f) Taylor + ℓ2",
            "has some feature maps which are highly important and others which are unimportant.\n(a) Weight\n(b) Activation (mean)\n(c) Taylor\n(d) Weight + ℓ2\n(e) Activation (mean) + ℓ2\n(f) Taylor + ℓ2\nFigure 10: Statistics of feature map ranking by raw criteria values (top) and by criteria values after ℓ2\nnormalization (bottom).\n13",
            "Published as a conference paper at ICLR 2017\nMI\nWeight\nActivation\nOBD\nTaylor\nMean\nS.d.\nAPoZ\nPer layer\nLayer 1\n0.41\n0.40\n0.65\n0.78\n0.36\n0.54\n0.95\nLayer 2\n0.23\n0.57\n0.56\n0.59\n0.33\n0.78\n0.90\nLayer 3\n0.14\n0.55\n0.48\n0.45\n0.51\n0.66\n0.74\nLayer 4\n0.26\n0.23\n0.58\n0.42\n0.10\n0.36\n0.80\nLayer 5\n0.17\n0.28\n0.49\n0.52\n0.15\n0.54\n0.69\nLayer 6\n0.21\n0.18\n0.41\n0.48\n0.16\n0.49\n0.63\nLayer 7\n0.12\n0.19\n0.54\n0.49\n0.38\n0.55\n0.71\nLayer 8\n0.18\n0.23\n0.43\n0.42\n0.30\n0.50\n0.54\nLayer 9\n0.21\n0.18\n0.50\n0.55\n0.35\n0.53\n0.61\nLayer 10\n0.26\n0.15\n0.59\n0.60\n0.45\n0.61\n0.66\nLayer 11\n0.41\n0.12\n0.61\n0.65\n0.45\n0.64\n0.72\nLayer 12\n0.47\n0.15\n0.60\n0.66\n0.39\n0.66\n0.72\nLayer 13\n0.61\n0.21\n0.77\n0.76\n0.65\n0.76\n0.77\nMean\n0.28\n0.27\n0.56\n0.57\n0.35\n0.59\n0.73\nAll layers\nNo normalization\n0.35\n0.34\n0.35\n0.30\n0.43\n0.65\n0.14\nℓ1 normalization\n0.47\n0.37\n0.63\n0.63\n0.52\n0.65\n0.71\nℓ2 normalization\n0.47\n0.33\n0.64\n0.66\n0.51\n0.60\n0.73\nMin-max normalization\n0.27\n0.17\n0.52\n0.57\n0.42\n0.54\n0.67",
            "0.35\n0.34\n0.35\n0.30\n0.43\n0.65\n0.14\nℓ1 normalization\n0.47\n0.37\n0.63\n0.63\n0.52\n0.65\n0.71\nℓ2 normalization\n0.47\n0.33\n0.64\n0.66\n0.51\n0.60\n0.73\nMin-max normalization\n0.27\n0.17\n0.52\n0.57\n0.42\n0.54\n0.67\nTable 3: Spearman’s rank correlation of criteria vs oracle-abs in VGG-16 ﬁne-tuned on Birds 200.\nA.3\nORACLE COMPUTATION FOR VGG-16 ON BIRDS-200\nWe compute the change in the loss caused by removing individual feature maps from the VGG-16\nnetwork, after ﬁne-tuning on the Birds-200 dataset. Results are illustrated in Fig. 11a-11b for each\nfeature map in layers 1 and 13, respectively. To compute the oracle estimate for a feature map, we\nremove the feature map and compute the network prediction for each image in the training set using\nthe central crop with no data augmentation or dropout. We draw the following conclusions:\n• The contribution of feature maps range from positive (above the red line) to slightly negative",
            "the central crop with no data augmentation or dropout. We draw the following conclusions:\n• The contribution of feature maps range from positive (above the red line) to slightly negative\n(below the red line), implying the existence of some feature maps which decrease the training\ncost when removed.\n• There are many feature maps with little contribution to the network output, indicated by\nalmost zero change in loss when removed.\n• Both layers contain a small number of feature maps which induce a signiﬁcant increase in\nthe loss when removed.\n(a) Layer 1\n(b) Layer 13\nFigure 11: Change in training loss as a function of the removal of a single feature map from the\nVGG-16 network after ﬁne-tuning on Birds-200. Results are plotted for two convolutional layers w.r.t.\nthe index of the removed feature map index. The loss with all feature maps, 0.00461, is indicated\nwith a red horizontal line.\n14",
            "Published as a conference paper at ICLR 2017\nFigure 12: Comparison of our iterative pruning with pruning by regularization\nTable 3 contains a layer-by-layer listing of Spearman’s rank correlation of several criteria with the\nranking of oracle-abs. In this more detailed comparison, we see the Taylor criterion shows higher\ncorrelation for all individual layers. For several methods including Taylor, the worst correlations\nare observed for the middle of the network, layers 5-10. We also evaluate several techniques for\nnormalization of the raw criteria values for comparison across layers. The table shows the best\nperformance is obtained by ℓ2 normalization, hence we select it for our method.\nA.4\nCOMPARISON WITH WEIGHT REGULARIZATION\nHan et al. (2015) ﬁnd that ﬁne-tuning with high ℓ1 or ℓ2 regularization causes unimportant connections\nto be suppressed. Connections with energy lower than some threshold can be removed on the",
            "Han et al. (2015) ﬁnd that ﬁne-tuning with high ℓ1 or ℓ2 regularization causes unimportant connections\nto be suppressed. Connections with energy lower than some threshold can be removed on the\nassumption that they do not contribute much to subsequent layers. The same work also ﬁnds that\nthresholds must be set separately for each layer depending on its sensitivity to pruning. The procedure\nto evaluate sensitivity is time-consuming as it requires pruning layers independently during evaluation.\nThe idea of pruning with high regularization can be extended to removing the kernels for an entire\nfeature map if the ℓ2 norm of those kernels is below a predeﬁned threshold. We compare our approach\nwith this regularization-based pruning for the task of pruning the last convolutional layer of VGG-16\nﬁne-tuned for Birds-200. By considering only a single layer, we avoid the need to compute layerwise\nsensitivity. Parameters for optimization during ﬁne-tuning are the same as other experiments with the",
            "sensitivity. Parameters for optimization during ﬁne-tuning are the same as other experiments with the\nBirds-200 dataset. For the regularization technique, the pruning threshold is set to σ = 10−5 while\nwe vary the regularization coefﬁcient γ of the ℓ2 norm on each feature map kernel.5 We prune only\nkernel weights, while keeping the bias to maintain the same expected output.\nA comparison between pruning based on regularization and our greedy scheme is illustrated in\nFig. 12. We observe that our approach has higher test accuracy for the same number of remaining\nunpruned feature maps, when pruning 85% or more of the feature maps. We observe that with high\nregularization all weights tend to zero, not only unimportant weights as Han et al. (2015) observe in\nthe case of ImageNet networks. The intuition here is that with regularization we push all weights\ndown and potentially can affect important connections for transfer learning, whereas in our iterative",
            "the case of ImageNet networks. The intuition here is that with regularization we push all weights\ndown and potentially can affect important connections for transfer learning, whereas in our iterative\nprocedure we only remove unimportant parameters leaving others untouched.\nA.5\nCOMBINATION OF CRITERIA\nOne of the possibilities to improve saliency estimation is to combine several criteria together. One of\nthe straight forward combinations is Taylor and mean activation of the neuron. We compute the joint\ncriteria as Θjoint(z(k)\nl\n) = (1 −λ)ˆΘT aylor(z(k)\nl\n) + λˆΘActivation(z(k)\nl\n) and perform a grid search of\nparameter λ in Fig.13. The highest correlation value for each dataset is marked with with vertical bar\nwith λ and gain. We observe that the gain of linearly combining criteria is negligibly small (see ∆’s\nin the ﬁgure).\n5In our implementation, the regularization coefﬁcient is multiplied by the learning rate equal to 10−4.\n15",
            "Published as a conference paper at ICLR 2017\nFigure 13: Spearman rank correlation for linear combination of criteria. The per layer metric is used.\nEach ∆indicates the gain in correlation for one experiment.\nA.6\nOPTIMAL BRAIN DAMAGE IMPLEMENTATION\nOBD computes saliency of a parameter by computing a product of the squared magnitude of the\nparameter and the corresponding element on the diagonal of the Hessian. For many deep learning\nframeworks, an efﬁcient implementation of the diagonal evaluation is not straightforward and\napproximation techniques must be applied. Our implementation of Hessian diagonal computation\nwas inspired by Dauphin et al. (2015) work, where the technique proposed by Bekas et al. (2007) was\nused to evaluate SGD preconditioned with the Jacobi preconditioner. It was shown that diagonal of\nthe Hessian can be approximated as:\ndiag(H) = E[v ⊙Hv] = E[v ⊙∇(∇C · v)],\n(13)\nwhere ⊙is the element-wise product, v are random vectors with entries ±1, and ∇is the gradient",
            "the Hessian can be approximated as:\ndiag(H) = E[v ⊙Hv] = E[v ⊙∇(∇C · v)],\n(13)\nwhere ⊙is the element-wise product, v are random vectors with entries ±1, and ∇is the gradient\noperator. To compute saliency with OBD, we randomly draw v and compute the diagonal over 10\niterations for a single minibatch for 1000 mini batches. We found that this number of mini batches is\nrequired to compute close approximation of the Hessian’s diagonal (which we veriﬁed). Computing\nsaliency this way is computationally expensive for iterative pruning, and we use a slightly different\nbut more efﬁcient procedure. Before the ﬁrst pruning iteration, saliency is initialized from values\ncomputed off-line with 1000 minibatches and 10 iterations, as described above. Then, at every\nminibatch we compute the OBD criteria with only one iteration and apply an exponential moving\naveraging with a coefﬁcient of 0.99. We veriﬁed that this computes a close approximation to the\nHessian’s diagonal.\nA.7",
            "averaging with a coefﬁcient of 0.99. We veriﬁed that this computes a close approximation to the\nHessian’s diagonal.\nA.7\nCORRELATION OF TAYLOR CRITERION WITH GRADIENT AND ACTIVATION\nThe Taylor criterion is composed of both an activation term and a gradient term. In Figure 14, we\ndepict the correlation between the Taylor criterion and each constituent part. We consider expected\nabsolute value of the gradient instead of the mean, because otherwise it tends to zero. The plots are\ncomputed from pruning criteria for an unpruned VGG network ﬁne-tuned for the Birds-200 dataset.\n(Values are shown after layer-wise normalization). Figure 14(a-b) depict the Taylor criterion in the\ny-axis for all neurons w.r.t. the gradient and activation components, respectively. The bottom 10% of\nneurons (lowest Taylor criterion, most likely to be pruned) are depicted in red, while the top 10% are\nshown in green. Considering all neurons, both gradient and activation components demonstrate a",
            "neurons (lowest Taylor criterion, most likely to be pruned) are depicted in red, while the top 10% are\nshown in green. Considering all neurons, both gradient and activation components demonstrate a\nlinear trend with the Taylor criterion. However, for the bottom 10% of neurons, as shown in Figure\n14(c-d), the activation criterion shows much stronger correlation, with lower activations indicating\nlower Taylor scores.\n16",
            "Published as a conference paper at ICLR 2017\n(a)\n(b)\n(c)\n(d)\nFigure 14: Correlation of Taylor criterion with gradient and activation (after layer-wise ℓ2 normaliza-\ntion) for all neurons (a-b) and bottom 10% of neurons (c-d) for unpruned VGG after ﬁne-tuning on\nBirds-200.\n17"
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 1,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 1,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 1,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 1,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 2,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 2,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 2,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 2,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 2,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 3,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 4,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 4,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 4,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 4,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 4,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 5,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 6,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 6,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 6,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 6,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 6,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 7,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 7,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 7,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 7,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 8,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 8,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 8,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 8,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 8,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 9,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 9,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 10,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 10,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 10,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 10,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 10,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 11,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 11,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 11,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 11,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 11,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 12,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 12,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 13,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 13,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 13,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 14,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 14,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 14,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 15,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 15,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 15,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 15,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 16,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 16,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 16,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 16,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            },
            {
                "page": 17,
                "source": "PRUNING CONVOLUTIONAL NEURAL NETWORKS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz"
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25",
            "chunk_26",
            "chunk_27",
            "chunk_28",
            "chunk_29",
            "chunk_30",
            "chunk_31",
            "chunk_32",
            "chunk_33",
            "chunk_34",
            "chunk_35",
            "chunk_36",
            "chunk_37",
            "chunk_38",
            "chunk_39",
            "chunk_40",
            "chunk_41",
            "chunk_42",
            "chunk_43",
            "chunk_44",
            "chunk_45",
            "chunk_46",
            "chunk_47",
            "chunk_48",
            "chunk_49",
            "chunk_50",
            "chunk_51",
            "chunk_52",
            "chunk_53",
            "chunk_54",
            "chunk_55",
            "chunk_56",
            "chunk_57",
            "chunk_58",
            "chunk_59",
            "chunk_60",
            "chunk_61",
            "chunk_62",
            "chunk_63",
            "chunk_64",
            "chunk_65",
            "chunk_66",
            "chunk_67",
            "chunk_68",
            "chunk_69"
        ]
    },
    "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf": {
        "chunks": [
            "Learning Efﬁcient Convolutional Networks through Network Slimming\nZhuang Liu1∗\nJianguo Li2\nZhiqiang Shen3\nGao Huang4\nShoumeng Yan2\nChangshui Zhang1\n1CSAI, TNList, Tsinghua University\n2Intel Labs China\n3Fudan University\n4Cornell University\n{liuzhuangthu, zhiqiangshen0214}@gmail.com, {jianguo.li, shoumeng.yan}@intel.com,\ngh349@cornell.edu, zcs@mail.tsinghua.edu.cn\nAbstract\nThe deployment of deep convolutional neural networks\n(CNNs) in many real world applications is largely hindered\nby their high computational cost. In this paper, we propose\na novel learning scheme for CNNs to simultaneously 1) re-\nduce the model size; 2) decrease the run-time memory foot-\nprint; and 3) lower the number of computing operations,\nwithout compromising accuracy. This is achieved by en-\nforcing channel-level sparsity in the network in a simple but\neffective way. Different from many existing approaches, the\nproposed method directly applies to modern CNN architec-",
            "forcing channel-level sparsity in the network in a simple but\neffective way. Different from many existing approaches, the\nproposed method directly applies to modern CNN architec-\ntures, introduces minimum overhead to the training process,\nand requires no special software/hardware accelerators for\nthe resulting models. We call our approach network slim-\nming, which takes wide and large networks as input mod-\nels, but during training insigniﬁcant channels are automat-\nically identiﬁed and pruned afterwards, yielding thin and\ncompact models with comparable accuracy. We empirically\ndemonstrate the effectiveness of our approach with several\nstate-of-the-art CNN models, including VGGNet, ResNet\nand DenseNet, on various image classiﬁcation datasets. For\nVGGNet, a multi-pass version of network slimming gives a\n20× reduction in model size and a 5× reduction in comput-\ning operations.\n1. Introduction\nIn recent years, convolutional neural networks (CNNs)",
            "20× reduction in model size and a 5× reduction in comput-\ning operations.\n1. Introduction\nIn recent years, convolutional neural networks (CNNs)\nhave become the dominant approach for a variety of com-\nputer vision tasks, e.g., image classiﬁcation [22], object\ndetection [8], semantic segmentation [26].\nLarge-scale\ndatasets, high-end modern GPUs and new network architec-\ntures allow the development of unprecedented large CNN\nmodels. For instance, from AlexNet [22], VGGNet [31] and\nGoogleNet [34] to ResNets [14], the ImageNet Classiﬁca-\ntion Challenge winner models have evolved from 8 layers\nto more than 100 layers.\n∗This work was done when Zhuang Liu and Zhiqiang Shen were interns\nat Intel Labs China. Jianguo Li is the corresponding author.\nHowever, larger CNNs, although with stronger represen-\ntation power, are more resource-hungry.\nFor instance, a\n152-layer ResNet [14] has more than 60 million parame-\nters and requires more than 20 Giga ﬂoat-point-operations",
            "tation power, are more resource-hungry.\nFor instance, a\n152-layer ResNet [14] has more than 60 million parame-\nters and requires more than 20 Giga ﬂoat-point-operations\n(FLOPs) when inferencing an image with resolution 224×\n224.\nThis is unlikely to be affordable on resource con-\nstrained platforms such as mobile devices, wearables or In-\nternet of Things (IoT) devices.\nThe deployment of CNNs in real world applications are\nmostly constrained by 1) Model size: CNNs’ strong repre-\nsentation power comes from their millions of trainable pa-\nrameters. Those parameters, along with network structure\ninformation, need to be stored on disk and loaded into mem-\nory during inference time. As an example, storing a typi-\ncal CNN trained on ImageNet consumes more than 300MB\nspace, which is a big resource burden to embedded devices.\n2) Run-time memory: During inference time, the interme-\ndiate activations/responses of CNNs could even take more\nmemory space than storing the model parameters, even with",
            "2) Run-time memory: During inference time, the interme-\ndiate activations/responses of CNNs could even take more\nmemory space than storing the model parameters, even with\nbatch size 1. This is not a problem for high-end GPUs, but\nunaffordable for many applications with low computational\npower. 3) Number of computing operations: The convolu-\ntion operations are computationally intensive on high reso-\nlution images. A large CNN may take several minutes to\nprocess one single image on a mobile device, making it un-\nrealistic to be adopted for real applications.\nMany works have been proposed to compress large\nCNNs or directly learn more efﬁcient CNN models for fast\ninference. These include low-rank approximation [7], net-\nwork quantization [3, 12] and binarization [28, 6], weight\npruning [12], dynamic inference [16], etc. However, most\nof these methods can only address one or two challenges\nmentioned above. Moreover, some of the techniques require",
            "pruning [12], dynamic inference [16], etc. However, most\nof these methods can only address one or two challenges\nmentioned above. Moreover, some of the techniques require\nspecially designed software/hardware accelerators for exe-\ncution speedup [28, 6, 12].\nAnother direction to reduce the resource consumption of\nlarge CNNs is to sparsify the network. Sparsity can be im-\nposed on different level of structures [2, 37, 35, 29, 25],\nwhich yields considerable model-size compression and in-\nference speedup. However, these approaches generally re-\n2736",
            "…\ni-th conv-layer\ninitial network\n…\n(i+1)=j-th\nconv-layer\nchannel scaling \nfactors\n…\n1.170\n0.001\n0.820\n0.290\n0.003\nCi1\nCin\nCi2\nCi3\nCi4\nCj1\nCj2\n…\ni-th conv-layer\ncompact network\n…\n(i+1)=j-th\nconv-layer\nchannel scaling \nfactors\n…\n1.170\n0.820\n0.290\nCi1\nCin\nCi3\nCj1\nCj2\npruning\nFigure 1: We associate a scaling factor (reused from a batch normalization layer) with each channel in convolutional layers. Sparsity\nregularization is imposed on these scaling factors during training to automatically identify unimportant channels. The channels with small\nscaling factor values (in orange color) will be pruned (left side). After pruning, we obtain compact models (right side), which are then\nﬁne-tuned to achieve comparable (or even higher) accuracy as normally trained full network.\nquire special software/hardware accelerators to harvest the\ngain in memory or time savings, though it is easier than\nnon-structured sparse weight matrix as in [12].\nIn this paper, we propose network slimming, a simple",
            "gain in memory or time savings, though it is easier than\nnon-structured sparse weight matrix as in [12].\nIn this paper, we propose network slimming, a simple\nyet effective network training scheme, which addresses all\nthe aforementioned challenges when deploying large CNNs\nunder limited resources. Our approach imposes L1 regular-\nization on the scaling factors in batch normalization (BN)\nlayers, thus it is easy to implement without introducing any\nchange to existing CNN architectures.\nPushing the val-\nues of BN scaling factors towards zero with L1 regulariza-\ntion enables us to identify insigniﬁcant channels (or neu-\nrons), as each scaling factor corresponds to a speciﬁc con-\nvolutional channel (or a neuron in a fully-connected layer).\nThis facilitates the channel-level pruning at the followed\nstep. The additional regularization term rarely hurt the per-\nformance. In fact, in some cases it leads to higher gen-\neralization accuracy.\nPruning unimportant channels may",
            "step. The additional regularization term rarely hurt the per-\nformance. In fact, in some cases it leads to higher gen-\neralization accuracy.\nPruning unimportant channels may\nsometimes temporarily degrade the performance, but this\neffect can be compensated by the followed ﬁne-tuning of\nthe pruned network. After pruning, the resulting narrower\nnetwork is much more compact in terms of model size, run-\ntime memory, and computing operations compared to the\ninitial wide network. The above process can be repeated\nfor several times, yielding a multi-pass network slimming\nscheme which leads to even more compact network.\nExperiments on several benchmark datasets and different\nnetwork architectures show that we can obtain CNN models\nwith up to 20x mode-size compression and 5x reduction in\ncomputing operations of the original ones, while achieving\nthe same or even higher accuracy. Moreover, our method\nachieves model compression and inference speedup with",
            "computing operations of the original ones, while achieving\nthe same or even higher accuracy. Moreover, our method\nachieves model compression and inference speedup with\nconventional hardware and deep learning software pack-\nages, since the resulting narrower model is free of any\nsparse storing format or computing operations.\n2. Related Work\nIn this section, we discuss related work from ﬁve aspects.\nLow-rank Decomposition approximates weight matrix in\nneural networks with low-rank matrix using techniques like\nSingular Value Decomposition (SVD) [7].\nThis method\nworks especially well on fully-connected layers, yield-\ning ∼3x model-size compression however without notable\nspeed acceleration, since computing operations in CNN\nmainly come from convolutional layers.\nWeight Quantization. HashNet [3] proposes to quantize\nthe network weights. Before training, network weights are\nhashed to different groups and within each group weight\nthe value is shared. In this way only the shared weights and",
            "the network weights. Before training, network weights are\nhashed to different groups and within each group weight\nthe value is shared. In this way only the shared weights and\nhash indices need to be stored, thus a large amount of stor-\nage space could be saved. [12] uses a improved quantization\ntechnique in a deep compression pipeline and achieves 35x\nto 49x compression rates on AlexNet and VGGNet. How-\never, these techniques can neither save run-time memory\nnor inference time, since during inference shared weights\nneed to be restored to their original positions.\n[28, 6] quantize real-valued weights into binary/ternary\nweights (weight values restricted to {−1, 1} or {−1, 0, 1}).\nThis yields a large amount of model-size saving, and signiﬁ-\ncant speedup could also be obtained given bitwise operation\nlibraries. However, this aggressive low-bit approximation\nmethod usually comes with a moderate accuracy loss.\nWeight Pruning / Sparsifying. [12] proposes to prune the",
            "libraries. However, this aggressive low-bit approximation\nmethod usually comes with a moderate accuracy loss.\nWeight Pruning / Sparsifying. [12] proposes to prune the\nunimportant connections with small weights in trained neu-\nral networks. The resulting network’s weights are mostly\nzeros thus the storage space can be reduced by storing the\nmodel in a sparse format. However, these methods can only\nachieve speedup with dedicated sparse matrix operation li-\nbraries and/or hardware. The run-time memory saving is\nalso very limited since most memory space is consumed by\nthe activation maps (still dense) instead of the weights.\nIn [12], there is no guidance for sparsity during training.\n[32] overcomes this limitation by explicitly imposing sparse\nconstraint over each weight with additional gate variables,\nand achieve high compression rates by pruning connections\nwith zero gate values. This method achieves better com-\n2737",
            "pression rate than [12], but suffers from the same drawback.\nStructured Pruning / Sparsifying.\nRecently, [23] pro-\nposes to prune channels with small incoming weights in\ntrained CNNs, and then ﬁne-tune the network to regain\naccuracy.\n[2] introduces sparsity by random deactivat-\ning input-output channel-wise connections in convolutional\nlayers before training, which also yields smaller networks\nwith moderate accuracy loss. Compared with these works,\nwe explicitly impose channel-wise sparsity in the optimiza-\ntion objective during training, leading to smoother channel\npruning process and little accuracy loss.\n[37] imposes neuron-level sparsity during training thus\nsome neurons could be pruned to obtain compact networks.\n[35] proposes a Structured Sparsity Learning (SSL) method\nto sparsify different level of structures (e.g. ﬁlters, channels\nor layers) in CNNs. Both methods utilize group sparsity\nregualarization during training to obtain structured spar-\nsity.",
            "to sparsify different level of structures (e.g. ﬁlters, channels\nor layers) in CNNs. Both methods utilize group sparsity\nregualarization during training to obtain structured spar-\nsity.\nInstead of resorting to group sparsity on convolu-\ntional weights, our approach imposes simple L1 sparsity on\nchannel-wise scaling factors, thus the optimization objec-\ntive is much simpler.\nSince these methods prune or sparsify part of the net-\nwork structures (e.g., neurons, channels) instead of individ-\nual weights, they usually require less specialized libraries\n(e.g. for sparse computing operation) to achieve inference\nspeedup and run-time memory saving. Our network slim-\nming also falls into this category, with absolutely no special\nlibraries needed to obtain the beneﬁts.\nNeural Architecture Learning.\nWhile state-of-the-art\nCNNs are typically designed by experts [22, 31, 14], there\nare also some explorations on automatically learning net-\nwork architectures.\n[20] introduces sub-modular/super-",
            "While state-of-the-art\nCNNs are typically designed by experts [22, 31, 14], there\nare also some explorations on automatically learning net-\nwork architectures.\n[20] introduces sub-modular/super-\nmodular optimization for network architecture search with\na given resource budget. Some recent works [38, 1] propose\nto learn neural architecture automatically with reinforce-\nment learning. The searching space of these methods are\nextremely large, thus one needs to train hundreds of mod-\nels to distinguish good from bad ones. Network slimming\ncan also be treated as an approach for architecture learning,\ndespite the choices are limited to the width of each layer.\nHowever, in contrast to the aforementioned methods, net-\nwork slimming learns network architecture through only a\nsingle training process, which is in line with our goal of\nefﬁciency.\n3. Network slimming\nWe aim to provide a simple scheme to achieve channel-\nlevel sparsity in deep CNNs. In this section, we ﬁrst dis-",
            "efﬁciency.\n3. Network slimming\nWe aim to provide a simple scheme to achieve channel-\nlevel sparsity in deep CNNs. In this section, we ﬁrst dis-\ncuss the advantages and challenges of channel-level spar-\nsity, and introduce how we leverage the scaling layers in\nbatch normalization to effectively identify and prune unim-\nportant channels in the network.\nAdvantages of Channel-level Sparsity. As discussed in\nprior works [35, 23, 11], sparsity can be realized at differ-\nent levels, e.g., weight-level, kernel-level, channel-level or\nlayer-level. Fine-grained level (e.g., weight-level) sparsity\ngives the highest ﬂexibility and generality leads to higher\ncompression rate, but it usually requires special software or\nhardware accelerators to do fast inference on the sparsiﬁed\nmodel [11]. On the contrary, the coarsest layer-level spar-\nsity does not require special packages to harvest the infer-\nence speedup, while it is less ﬂexible as some whole layers",
            "model [11]. On the contrary, the coarsest layer-level spar-\nsity does not require special packages to harvest the infer-\nence speedup, while it is less ﬂexible as some whole layers\nneed to be pruned. In fact, removing layers is only effec-\ntive when the depth is sufﬁciently large, e.g., more than 50\nlayers [35, 18]. In comparison, channel-level sparsity pro-\nvides a nice tradeoff between ﬂexibility and ease of imple-\nmentation. It can be applied to any typical CNNs or fully-\nconnected networks (treat each neuron as a channel), and\nthe resulting network is essentially a “thinned” version of\nthe unpruned network, which can be efﬁciently inferenced\non conventional CNN platforms.\nChallenges.\nAchieving channel-level sparsity requires\npruning all the incoming and outgoing connections asso-\nciated with a channel. This renders the method of directly\npruning weights on a pre-trained model ineffective, as it is\nunlikely that all the weights at the input or output end of",
            "ciated with a channel. This renders the method of directly\npruning weights on a pre-trained model ineffective, as it is\nunlikely that all the weights at the input or output end of\na channel happen to have near zero values. As reported in\n[23], pruning channels on pre-trained ResNets can only lead\nto a reduction of ∼10% in the number of parameters without\nsuffering from accuracy loss. [35] addresses this problem\nby enforcing sparsity regularization into the training objec-\ntive. Speciﬁcally, they adopt group LASSO to push all the\nﬁlter weights corresponds to the same channel towards zero\nsimultaneously during training. However, this approach re-\nquires computing the gradients of the additional regulariza-\ntion term with respect to all the ﬁlter weights, which is non-\ntrivial. We introduce a simple idea to address the above\nchallenges, and the details are presented below.\nScaling Factors and Sparsity-induced Penalty. Our idea\nis introducing a scaling factor γ for each channel, which is",
            "challenges, and the details are presented below.\nScaling Factors and Sparsity-induced Penalty. Our idea\nis introducing a scaling factor γ for each channel, which is\nmultiplied to the output of that channel. Then we jointly\ntrain the network weights and these scaling factors, with\nsparsity regularization imposed on the latter. Finally we\nprune those channels with small factors, and ﬁne-tune the\npruned network. Speciﬁcally, the training objective of our\napproach is given by\nL =\nX\n(x,y)\nl(f(x, W), y) + λ\nX\nγ∈Γ\ng(γ)\n(1)\nwhere (x, y) denote the train input and target, W denotes\nthe trainable weights, the ﬁrst sum-term corresponds to the\nnormal training loss of a CNN, g(·) is a sparsity-induced\npenalty on the scaling factors, and λ balances the two terms.\nIn our experiment, we choose g(s) = |s|, which is known as\n2738",
            "Initial  \nnetwork\nCompact  \nnetwork\nTrain with  \nchannel sparsity  \nregularization\nFine-tune the  \npruned network\nPrune channels \nwith small \nscaling factors\nFigure 2: Flow-chart of network slimming procedure. The dotted-\nline is for the multi-pass/iterative scheme.\nL1-norm and widely used to achieve sparsity. Subgradient\ndescent is adopted as the optimization method for the non-\nsmooth L1 penalty term. An alternative option is to replace\nthe L1 penalty with the smooth-L1 penalty [30] to avoid\nusing sub-gradient at non-smooth point.\nAs pruning a channel essentially corresponds to remov-\ning all the incoming and outgoing connections of that chan-\nnel, we can directly obtain a narrow network (see Figure 1)\nwithout resorting to any special sparse computation pack-\nages. The scaling factors act as the agents for channel se-\nlection.\nAs they are jointly optimized with the network\nweights, the network can automatically identity insigniﬁ-",
            "ages. The scaling factors act as the agents for channel se-\nlection.\nAs they are jointly optimized with the network\nweights, the network can automatically identity insigniﬁ-\ncant channels, which can be safely removed without greatly\naffecting the generalization performance.\nLeveraging the Scaling Factors in BN Layers. Batch nor-\nmalization [19] has been adopted by most modern CNNs\nas a standard approach to achieve fast convergence and bet-\nter generalization performance. The way BN normalizes\nthe activations motivates us to design a simple and efﬁ-\ncient method to incorporates the channel-wise scaling fac-\ntors. Particularly, BN layer normalizes the internal activa-\ntions using mini-batch statistics. Let zin and zout be the\ninput and output of a BN layer, B denotes the current mini-\nbatch, BN layer performs the following transformation:\nˆz = zin −µB\np\nσ2\nB + ǫ\n; zout = γˆz + β\n(2)\nwhere µB and σB are the mean and standard deviation val-",
            "batch, BN layer performs the following transformation:\nˆz = zin −µB\np\nσ2\nB + ǫ\n; zout = γˆz + β\n(2)\nwhere µB and σB are the mean and standard deviation val-\nues of input activations over B, γ and β are trainable afﬁne\ntransformation parameters (scale and shift) which provides\nthe possibility of linearly transforming normalized activa-\ntions back to any scales.\nIt is common practice to insert a BN layer after a convo-\nlutional layer, with channel-wise scaling/shifting parame-\nters. Therefore, we can directly leverage the γ parameters in\nBN layers as the scaling factors we need for network slim-\nming. It has the great advantage of introducing no overhead\nto the network. In fact, this is perhaps also the most effec-\ntive way we can learn meaningful scaling factors for chan-\nnel pruning. 1), if we add scaling layers to a CNN without\nBN layer, the value of the scaling factors are not meaning-\nful for evaluating the importance of a channel, because both",
            "nel pruning. 1), if we add scaling layers to a CNN without\nBN layer, the value of the scaling factors are not meaning-\nful for evaluating the importance of a channel, because both\nconvolution layers and scaling layers are linear transforma-\ntions. One can obtain the same results by decreasing the\nscaling factor values while amplifying the weights in the\nconvolution layers. 2), if we insert a scaling layer before\na BN layer, the scaling effect of the scaling layer will be\ncompletely canceled by the normalization process in BN.\n3), if we insert scaling layer after BN layer, there are two\nconsecutive scaling factors for each channel.\nChannel Pruning and Fine-tuning. After training under\nchannel-level sparsity-induced regularization, we obtain a\nmodel in which many scaling factors are near zero (see Fig-\nure 1). Then we can prune channels with near-zero scaling\nfactors, by removing all their incoming and outgoing con-\nnections and corresponding weights. We prune channels",
            "ure 1). Then we can prune channels with near-zero scaling\nfactors, by removing all their incoming and outgoing con-\nnections and corresponding weights. We prune channels\nwith a global threshold across all layers, which is deﬁned\nas a certain percentile of all the scaling factor values. For\ninstance, we prune 70% channels with lower scaling factors\nby choosing the percentile threshold as 70%. By doing so,\nwe obtain a more compact network with less parameters and\nrun-time memory, as well as less computing operations.\nPruning may temporarily lead to some accuracy loss,\nwhen the pruning ratio is high. But this can be largely com-\npensated by the followed ﬁne-tuning process on the pruned\nnetwork. In our experiments, the ﬁne-tuned narrow network\ncan even achieve higher accuracy than the original unpruned\nnetwork in many cases.\nMulti-pass Scheme.\nWe can also extend the proposed\nmethod from single-pass learning scheme (training with",
            "can even achieve higher accuracy than the original unpruned\nnetwork in many cases.\nMulti-pass Scheme.\nWe can also extend the proposed\nmethod from single-pass learning scheme (training with\nsparsity regularization, pruning, and ﬁne-tuning) to a multi-\npass scheme. Speciﬁcally, a network slimming procedure\nresults in a narrow network, on which we could again apply\nthe whole training procedure to learn an even more compact\nmodel. This is illustrated by the dotted-line in Figure 2. Ex-\nperimental results show that this multi-pass scheme can lead\nto even better results in terms of compression rate.\nHandling Cross Layer Connections and Pre-activation\nStructure.\nThe network slimming process introduced\nabove can be directly applied to most plain CNN architec-\ntures such as AlexNet [22] and VGGNet [31]. While some\nadaptations are required when it is applied to modern net-\nworks with cross layer connections and the pre-activation\ndesign such as ResNet [15] and DenseNet [17]. For these",
            "adaptations are required when it is applied to modern net-\nworks with cross layer connections and the pre-activation\ndesign such as ResNet [15] and DenseNet [17]. For these\nnetworks, the output of a layer may be treated as the input\nof multiple subsequent layers, in which a BN layer is placed\nbefore the convolutional layer. In this case, the sparsity is\nachieved at the incoming end of a layer, i.e., the layer selec-\ntively uses a subset of channels it received. To harvest the\nparameter and computation savings at test time, we need\nto place a channel selection layer to mask out insigniﬁcant\nchannels we have identiﬁed.\n4. Experiments\nWe empirically demonstrate the effectiveness of network\nslimming on several benchmark datasets. We implement\n2739",
            "(a) Test Errors on CIFAR-10\nModel\nTest error (%)\nParameters\nPruned\nFLOPs\nPruned\nVGGNet (Baseline)\n6.34\n20.04M\n-\n7.97×108\n-\nVGGNet (70% Pruned)\n6.20\n2.30M\n88.5%\n3.91×108\n51.0%\nDenseNet-40 (Baseline)\n6.11\n1.02M\n-\n5.33×108\n-\nDenseNet-40 (40% Pruned)\n5.19\n0.66M\n35.7%\n3.81×108\n28.4%\nDenseNet-40 (70% Pruned)\n5.65\n0.35M\n65.2%\n2.40×108\n55.0%\nResNet-164 (Baseline)\n5.42\n1.70M\n-\n4.99×108\n-\nResNet-164 (40% Pruned)\n5.08\n1.44M\n14.9%\n3.81×108\n23.7%\nResNet-164 (60% Pruned)\n5.27\n1.10M\n35.2%\n2.75×108\n44.9%\n(b) Test Errors on CIFAR-100\nModel\nTest error (%)\nParameters\nPruned\nFLOPs\nPruned\nVGGNet (Baseline)\n26.74\n20.08M\n-\n7.97×108\n-\nVGGNet (50% Pruned)\n26.52\n5.00M\n75.1%\n5.01×108\n37.1%\nDenseNet-40 (Baseline)\n25.36\n1.06M\n-\n5.33×108\n-\nDenseNet-40 (40% Pruned)\n25.28\n0.66M\n37.5%\n3.71×108\n30.3%\nDenseNet-40 (60% Pruned)\n25.72\n0.46M\n54.6%\n2.81×108\n47.1%\nResNet-164 (Baseline)\n23.37\n1.73M\n-\n5.00×108\n-\nResNet-164 (40% Pruned)\n22.87\n1.46M\n15.5%\n3.33×108\n33.3%\nResNet-164 (60% Pruned)\n23.91\n1.21M\n29.7%\n2.47×108\n50.6%",
            "25.72\n0.46M\n54.6%\n2.81×108\n47.1%\nResNet-164 (Baseline)\n23.37\n1.73M\n-\n5.00×108\n-\nResNet-164 (40% Pruned)\n22.87\n1.46M\n15.5%\n3.33×108\n33.3%\nResNet-164 (60% Pruned)\n23.91\n1.21M\n29.7%\n2.47×108\n50.6%\n(c) Test Errors on SVHN\nModel\nTest Error (%)\nParameters\nPruned\nFLOPs\nPruned\nVGGNet (Baseline)\n2.17\n20.04M\n-\n7.97×108\n-\nVGGNet (60% Pruned)\n2.06\n3.04M\n84.8%\n3.98×108\n50.1%\nDenseNet-40 (Baseline)\n1.89\n1.02M\n-\n5.33×108\n-\nDenseNet-40 (40% Pruned)\n1.79\n0.65M\n36.3%\n3.69×108\n30.8%\nDenseNet-40 (60% Pruned)\n1.81\n0.44M\n56.6%\n2.67×108\n49.8%\nResNet-164 (Baseline)\n1.78\n1.70M\n-\n4.99×108\n-\nResNet-164 (40% Pruned)\n1.85\n1.46M\n14.5%\n3.44×108\n31.1%\nResNet-164 (60% Pruned)\n1.81\n1.12M\n34.3%\n2.25×108\n54.9%\nTable 1: Results on CIFAR and SVHN datasets. “Baseline” denotes normal training without sparsity regularization. In column-1, “60%\npruned” denotes the ﬁne-tuned model with 60% channels pruned from the model trained with sparsity, etc. The pruned ratio of parameters",
            "pruned” denotes the ﬁne-tuned model with 60% channels pruned from the model trained with sparsity, etc. The pruned ratio of parameters\nand FLOPs are also shown in column-4&6. Pruning a moderate amount (40%) of channels can mostly lower the test errors. The accuracy\ncould typically be maintained with ≥60% channels pruned.\nour method based on the publicly available Torch [5] im-\nplementation for ResNets by [10]. The code is available at\nhttps://github.com/liuzhuang13/slimming.\n4.1. Datasets\nCIFAR. The two CIFAR datasets [21] consist of natural im-\nages with resolution 32×32. CIFAR-10 is drawn from 10\nand CIFAR-100 from 100 classes. The train and test sets\ncontain 50,000 and 10,000 images respectively. On CIFAR-\n10, a validation set of 5,000 images is split from the training\nset for the search of λ (in Equation 1) on each model. We\nreport the ﬁnal test errors after training or ﬁne-tuning on\nall training images. A standard data augmentation scheme",
            "set for the search of λ (in Equation 1) on each model. We\nreport the ﬁnal test errors after training or ﬁne-tuning on\nall training images. A standard data augmentation scheme\n(shifting/mirroring) [14, 18, 24] is adopted. The input data\nis normalized using channel means and standard deviations.\nWe also compare our method with [23] on CIFAR datasets.\nSVHN. The Street View House Number (SVHN) dataset\n[27] consists of 32x32 colored digit images.\nFollowing\ncommon practice [9, 18, 24] we use all the 604,388 training\nimages, from which we split a validation set of 6,000 im-\nages for model selection during training. The test set con-\ntains 26,032 images. During training, we select the model\nwith the lowest validation error as the model to be pruned\n(or the baseline model). We also report the test errors of the\nmodels with lowest validation errors during ﬁne-tuning.\nImageNet.\nThe ImageNet dataset contains 1.2 million\ntraining images and 50,000 validation images of 1000",
            "models with lowest validation errors during ﬁne-tuning.\nImageNet.\nThe ImageNet dataset contains 1.2 million\ntraining images and 50,000 validation images of 1000\nclasses. We adopt the data augmentation scheme as in [10].\nWe report the single-center-crop validation error of the ﬁnal\nmodel.\nMNIST. MNIST is a handwritten digit dataset containing\n60,000 training images and 10,000 test images. To test the\neffectiveness of our method on a fully-connected network\n(treating each neuron as a channel with 1×1 spatial size),\nwe compare our method with [35] on this dataset.\n2740",
            "4.2. Network Models\nOn CIFAR and SVHN dataset, we evaluate our method\non three popular network architectures:\nVGGNet[31],\nResNet [14] and DenseNet [17]. The VGGNet is originally\ndesigned for ImageNet classiﬁcation. For our experiment a\nvariation of the original VGGNet for CIFAR dataset is taken\nfrom [36]. For ResNet, a 164-layer pre-activation ResNet\nwith bottleneck structure (ResNet-164) [15] is used. For\nDenseNet, we use a 40-layer DenseNet with growth rate 12\n(DenseNet-40).\nOn ImageNet dataset, we adopt the 11-layer (8-conv +\n3 FC) “VGG-A” network [31] model with batch normaliza-\ntion from [4]. We remove the dropout layers since we use\nrelatively heavy data augmentation. To prune the neurons\nin fully-connected layers, we treat them as convolutional\nchannels with 1×1 spatial size.\nOn MNIST dataset, we evaluate our method on the same\n3-layer fully-connected network as in [35].\n4.3. Training, Pruning and Fine-tuning\nNormal Training. We train all the networks normally from",
            "On MNIST dataset, we evaluate our method on the same\n3-layer fully-connected network as in [35].\n4.3. Training, Pruning and Fine-tuning\nNormal Training. We train all the networks normally from\nscratch as baselines.\nAll the networks are trained using\nSGD. On CIFAR and SVHN datasets we train using mini-\nbatch size 64 for 160 and 20 epochs, respectively. The ini-\ntial learning rate is set to 0.1, and is divided by 10 at 50%\nand 75% of the total number of training epochs. On Im-\nageNet and MNIST datasets, we train our models for 60\nand 30 epochs respectively, with a batch size of 256, and an\ninitial learning rate of 0.1 which is divided by 10 after 1/3\nand 2/3 fraction of training epochs. We use a weight de-\ncay of 10−4 and a Nesterov momentum [33] of 0.9 without\ndampening. The weight initialization introduced by [13] is\nadopted. Our optimization settings closely follow the orig-\ninal implementation at [10]. In all our experiments, we ini-",
            "dampening. The weight initialization introduced by [13] is\nadopted. Our optimization settings closely follow the orig-\ninal implementation at [10]. In all our experiments, we ini-\ntialize all channel scaling factors to be 0.5, since this gives\nhigher accuracy for the baseline models compared with de-\nfault setting (all initialized to be 1) from [10].\nTraining with Sparsity. For CIFAR and SVHN datasets,\nwhen training with channel sparse regularization, the hyper-\nparameteer λ, which controls the tradeoff between empiri-\ncal loss and sparsity, is determined by a grid search over\n10−3, 10−4, 10−5 on CIFAR-10 validation set. For VG-\nGNet we choose λ=10−4 and for ResNet and DenseNet\nλ=10−5. For VGG-A on ImageNet, we set λ=10−5. All\nother settings are kept the same as in normal training.\nPruning. When we prune the channels of models trained\nwith sparsity, a pruning threshold on the scaling factors\nneeds to be determined. Unlike in [23] where different lay-",
            "Pruning. When we prune the channels of models trained\nwith sparsity, a pruning threshold on the scaling factors\nneeds to be determined. Unlike in [23] where different lay-\ners are pruned by different ratios, we use a global pruning\nthreshold for simplicity. The pruning threshold is deter-\nmined by a percentile among all scaling factors , e.g., 40%\nor 60% channels are pruned. The pruning process is imple-\nVGGNet\nDenseNet-40\nResNet-164\n0\n20\n40\n60\n80\n100\nRatio (%)\n100.0%\n100.0%\n100.0%\n11.5%\n34.8%\n64.8%\n49.0%\n45.0%\n55.1%\nModel Parameter and FLOP Savings\nOriginal\nParameter Ratio\nFLOPs Ratio\nFigure 3: Comparison of pruned models with lower test errors on\nCIFAR-10 than the original models. The blue and green bars are\nparameter and FLOP ratios between pruned and original models.\nmented by building a new narrower model and copying the\ncorresponding weights from the model trained with sparsity.\nFine-tuning. After the pruning we obtain a narrower and",
            "mented by building a new narrower model and copying the\ncorresponding weights from the model trained with sparsity.\nFine-tuning. After the pruning we obtain a narrower and\nmore compact model, which is then ﬁne-tuned. On CIFAR,\nSVHN and MNIST datasets, the ﬁne-tuning uses the same\noptimization setting as in training. For ImageNet dataset,\ndue to time constraint, we ﬁne-tune the pruned VGG-A with\na learning rate of 10−3 for only 5 epochs.\n4.4. Results\nCIFAR and SVHN The results on CIFAR and SVHN are\nshown in Table 1. We mark all lowest test errors of a model\nin boldface.\nParameter and FLOP reductions. The purpose of net-\nwork slimming is to reduce the amount of computing re-\nsources needed. The last row of each model has ≥60%\nchannels pruned while still maintaining similar accuracy to\nthe baseline. The parameter saving can be up to 10×. The\nFLOP reductions are typically around 50%. To highlight\nnetwork slimming’s efﬁciency, we plot the resource sav-",
            "the baseline. The parameter saving can be up to 10×. The\nFLOP reductions are typically around 50%. To highlight\nnetwork slimming’s efﬁciency, we plot the resource sav-\nings in Figure 3. It can be observed that VGGNet has a\nlarge amount of redundant parameters that can be pruned.\nOn ResNet-164 the parameter and FLOP savings are rel-\natively insigniﬁcant, we conjecture this is due to its “bot-\ntleneck” structure has already functioned as selecting chan-\nnels. Also, on CIFAR-100 the reduction rate is typically\nslightly lower than CIFAR-10 and SVHN, which is possi-\nbly due to the fact that CIFAR-100 contains more classes.\nRegularization Effect. From Table 1, we can observe that,\non ResNet and DenseNet, typically when 40% channels are\npruned, the ﬁne-tuned network can achieve a lower test er-\nror than the original models. For example, DenseNet-40\nwith 40% channels pruned achieve a test error of 5.19%\non CIFAR-10, which is almost 1% lower than the original",
            "ror than the original models. For example, DenseNet-40\nwith 40% channels pruned achieve a test error of 5.19%\non CIFAR-10, which is almost 1% lower than the original\nmodel. We hypothesize this is due to the regularization ef-\nfect of L1 sparsity on channels, which naturally provides\nfeature selection in intermediate layers of a network. We\nwill analyze this effect in the next section.\n2741",
            "VGG-A\nBaseline\n50% Pruned\nParams\n132.9M\n23.2M\nParams Pruned\n-\n82.5%\nFLOPs\n4.57×1010\n3.18×1010\nFLOPs Pruned\n-\n30.4%\nValidation Error (%)\n36.69\n36.66\nTable 2: Results on ImageNet.\nModel\nTest Error (%) Params Pruned\n#Neurons\nBaseline\n1.43\n-\n784-500-300-10\nPruned [35]\n1.53\n83.5%\n434-174-78-10\nPruned (ours)\n1.49\n84.4%\n784-100-60-10\nTable 3: Results on MNIST.\nImageNet. The results for ImageNet dataset are summa-\nrized in Table 2. When 50% channels are pruned, the pa-\nrameter saving is more than 5×, while the FLOP saving\nis only 30.4%. This is due to the fact that only 378 (out\nof 2752) channels from all the computation-intensive con-\nvolutional layers are pruned, while 5094 neurons (out of\n8192) from the parameter-intensive fully-connected layers\nare pruned. It is worth noting that our method can achieve\nthe savings with no accuracy loss on the 1000-class Im-\nageNet dataset, where other methods for efﬁcient CNNs\n[2, 23, 35, 28] mostly report accuracy loss.",
            "the savings with no accuracy loss on the 1000-class Im-\nageNet dataset, where other methods for efﬁcient CNNs\n[2, 23, 35, 28] mostly report accuracy loss.\nMNIST. On MNIST dataset, we compare our method with\nthe Structured Sparsity Learning (SSL) method [35] in Ta-\nble 3.\nDespite our method is mainly designed to prune\nchannels in convolutional layers, it also works well in prun-\ning neurons in fully-connected layers. In this experiment,\nwe observe that pruning with a global threshold sometimes\ncompletely removes a layer, thus we prune 80% of the neu-\nrons in each of the two intermediate layers. Our method\nslightly outperforms [35], in that a slightly lower test error\nis achieved while pruning more parameters.\nWe provide some additional experimental results in the\nsupplementary materials, including (1) detailed structure of\na compact VGGNet on CIFAR-10; (2) wall-clock time and\nrun-time memory savings in practice. (3) comparison with\na previous channel pruning method [23];",
            "a compact VGGNet on CIFAR-10; (2) wall-clock time and\nrun-time memory savings in practice. (3) comparison with\na previous channel pruning method [23];\n4.5. Results for Multi-pass Scheme\nWe employ the multi-pass scheme on CIFAR datasets\nusing VGGNet. Since there are no skip-connections, prun-\ning away a whole layer will completely destroy the mod-\nels. Thus, besides setting the percentile threshold as 50%,\nwe also put a constraint that at each layer, at most 50% of\nchannels can be pruned.\nThe test errors of models in each iteration are shown in\nTable 4. As the pruning process goes, we obtain more and\n(a) Multi-pass Scheme on CIFAR-10\nIter Trained Fine-tuned Params Pruned FLOPs Pruned\n1\n6.38\n6.51\n66.7%\n38.6%\n2\n6.23\n6.11\n84.7%\n52.7%\n3\n5.87\n6.10\n91.4%\n63.1%\n4\n6.19\n6.59\n95.6%\n77.2%\n5\n5.96\n7.73\n98.3%\n88.7%\n6\n7.79\n9.70\n99.4%\n95.7%\n(b) Multi-pass Scheme on CIFAR-100\nIter Trained Fine-tuned Params Pruned FLOPs Pruned\n1\n27.72\n26.52\n59.1%\n30.9%\n2\n26.03\n26.52\n79.2%\n46.1%\n3\n26.49\n29.08\n89.8%\n67.3%",
            "98.3%\n88.7%\n6\n7.79\n9.70\n99.4%\n95.7%\n(b) Multi-pass Scheme on CIFAR-100\nIter Trained Fine-tuned Params Pruned FLOPs Pruned\n1\n27.72\n26.52\n59.1%\n30.9%\n2\n26.03\n26.52\n79.2%\n46.1%\n3\n26.49\n29.08\n89.8%\n67.3%\n4\n28.17\n30.59\n95.3%\n83.0%\n5\n30.04\n36.35\n98.3%\n93.5%\n6\n35.91\n46.73\n99.4%\n97.7%\nTable 4: Results for multi-pass scheme on CIFAR-10 and CIFAR-\n100 datasets, using VGGNet. The baseline model has test errors of\n6.34% and 26.74%. “Trained” and “Fine-tuned” columns denote\nthe test errors (%) of the model trained with sparsity, and the ﬁne-\ntuned model after channel pruning, respectively. The parameter\nand FLOP pruned ratios correspond to the ﬁne-tuned model in that\nrow and the trained model in the next row.\nmore compact models. On CIFAR-10, the trained model\nachieves the lowest test error in iteration 5. This model\nachieves 20× parameter reduction and 5× FLOP reduction,\nwhile still achieving lower test error. On CIFAR-100, after\niteration 3, the test error begins to increase. This is pos-",
            "achieves 20× parameter reduction and 5× FLOP reduction,\nwhile still achieving lower test error. On CIFAR-100, after\niteration 3, the test error begins to increase. This is pos-\nsibly due to that it contains more classes than CIFAR-10,\nso pruning channels too agressively will inevitably hurt the\nperformance. However, we can still prune near 90% param-\neters and near 70% FLOPs without notable accuracy loss.\n5. Analysis\nThere are two crucial hyper-parameters in network slim-\nming, the pruned percentage t and the coefﬁcient of the\nsparsity regularization term λ (see Equation 1). In this sec-\ntion, we analyze their effects in more detail.\nEffect of Pruned Percentage. Once we obtain a model\ntrained with sparsity regularization, we need to decide what\npercentage of channels to prune from the model.\nIf we\nprune too few channels, the resource saving can be very\nlimited. However, it could be destructive to the model if\nwe prune too many channels, and it may not be possible to",
            "If we\nprune too few channels, the resource saving can be very\nlimited. However, it could be destructive to the model if\nwe prune too many channels, and it may not be possible to\nrecover the accuracy by ﬁne-tuning. We train a DenseNet-\n40 model with λ=10−5 on CIFAR-10 to show the effect of\npruning a varying percentage of channels. The results are\nsummarized in Figure 5.\nFrom Figure 5, it can be concluded that the classiﬁcation\nperformance of the pruned or ﬁne-tuned models degrade\nonly when the pruning ratio surpasses a threshold. The ﬁne-\n2742",
            "0.0\n0.2\n0.4\n0.6\n0.8\n0\n50\n100\n150\n200\n250\n300\n350\n400\nCount\nλ = 0\n0.0\n0.2\n0.4\n0.6\n0.8\nScaling factor value\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nλ = 10−5\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n500\n1000\n1500\n2000\nλ = 10−4\nFigure 4: Distributions of scaling factors in a trained VGGNet under various degree of sparsity regularization (controlled by the parameter\nλ). With the increase of λ, scaling factors become sparser.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPruned channels (%)\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\nTest error (%)\nBaseline\nTrained with Sparsity\nPruned\nFine-tuned\nFigure 5: The effect of pruning varying percentages of channels,\nfrom DenseNet-40 trained on CIFAR-10 with λ=10−5.\ntuning process can typically compensate the possible accu-\nracy loss caused by pruning. Only when the threshold goes\nbeyond 80%, the test error of ﬁne-tuned model falls behind\nthe baseline model. Notably, when trained with sparsity,\neven without ﬁne-tuning, the model performs better than the",
            "beyond 80%, the test error of ﬁne-tuned model falls behind\nthe baseline model. Notably, when trained with sparsity,\neven without ﬁne-tuning, the model performs better than the\noriginal model. This is possibly due the the regularization\neffect of L1 sparsity on channel scaling factors.\nChannel Sparsity Regularization. The purpose of the L1\nsparsity term is to force many of the scaling factors to be\nnear zero. The parameter λ in Equation 1 controls its signif-\nicance compared with the normal training loss. In Figure 4\nwe plot the distributions of scaling factors in the whole net-\nwork with different λ values. For this experiment we use a\nVGGNet trained on CIFAR-10 dataset.\nIt can be observed that with the increase of λ, the scaling\nfactors are more and more concentrated near zero. When\nλ=0, i.e., there’s no sparsity regularization, the distribution\nis relatively ﬂat. When λ=10−4, almost all scaling factors\nfall into a small region near zero. This process can be seen",
            "λ=0, i.e., there’s no sparsity regularization, the distribution\nis relatively ﬂat. When λ=10−4, almost all scaling factors\nfall into a small region near zero. This process can be seen\nas a feature selection happening in intermediate layers of\ndeep networks, where only channels with non-negligible\nscaling factors are chosen. We further visualize this pro-\ncess by a heatmap. Figure 6 shows the magnitude of scaling\nfactors from one layer in VGGNet, along the training pro-\ncess. Each channel starts with equal weights; as the training\n0\n20\n40\n60\n80\nEpoch\n0\n10\n20\n30\n40\n50\nChannel Index\nFigure 6: Visulization of channel scaling factors’ change in scale\nalong the training process, taken from the 11th conv-layer in VG-\nGNet trained on CIFAR-10. Brighter color corresponds to larger\nvalue. The bright lines indicate the “selected” channels, the dark\nlines indicate channels that can be pruned.\nprogresses, some channels’ scaling factors become larger\n(brighter) while others become smaller (darker).",
            "lines indicate channels that can be pruned.\nprogresses, some channels’ scaling factors become larger\n(brighter) while others become smaller (darker).\n6. Conclusion\nWe proposed the network slimming technique to learn\nmore compact CNNs. It directly imposes sparsity-induced\nregularization on the scaling factors in batch normalization\nlayers, and unimportant channels can thus be automatically\nidentiﬁed during training and then pruned.\nOn multiple\ndatasets, we have shown that the proposed method is able to\nsigniﬁcantly decrease the computational cost (up to 20×) of\nstate-of-the-art networks, with no accuracy loss. More im-\nportantly, the proposed method simultaneously reduces the\nmodel size, run-time memory, computing operations while\nintroducing minimum overhead to the training process, and\nthe resulting models require no special libraries/hardware\nfor efﬁcient inference.\nAcknowledgements. Gao Huang is supported by the In-\nternational Postdoctoral Exchange Fellowship Program of",
            "the resulting models require no special libraries/hardware\nfor efﬁcient inference.\nAcknowledgements. Gao Huang is supported by the In-\nternational Postdoctoral Exchange Fellowship Program of\nChina Postdoctoral Council (No.20150015).\nChangshui\nZhang is supported by NSFC and DFG joint project NSFC\n61621136008/DFG TRR-169.\n2743",
            "References\n[1] B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neu-\nral network architectures using reinforcement learning. In\nICLR, 2017.\n[2] S. Changpinyo, M. Sandler, and A. Zhmoginov. The power\nof sparsity in convolutional neural networks. arXiv preprint\narXiv:1702.06257, 2017.\n[3] W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and\nY. Chen.\nCompressing neural networks with the hashing\ntrick. In ICML, 2015.\n[4] S. Chintala.\nTraining an object classiﬁer in torch-7 on\nmultiple gpus over imagenet. https://github.com/\nsoumith/imagenet-multiGPU.torch.\n[5] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A\nmatlab-like environment for machine learning. In BigLearn,\nNIPS Workshop, number EPFL-CONF-192376, 2011.\n[6] M. Courbariaux and Y. Bengio. Binarynet: Training deep\nneural networks with weights and activations constrained to+\n1 or-1. arXiv preprint arXiv:1602.02830, 2016.\n[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-\ngus.",
            "neural networks with weights and activations constrained to+\n1 or-1. arXiv preprint arXiv:1602.02830, 2016.\n[7] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-\ngus.\nExploiting linear structure within convolutional net-\nworks for efﬁcient evaluation. In NIPS, 2014.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR, pages 580–587, 2014.\n[9] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. In ICML, 2013.\n[10] S. Gross and M. Wilber. Training and investigating residual\nnets. https://github.com/szagoruyko/cifar.\ntorch.\n[11] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-\npressing deep neural network with pruning, trained quanti-\nzation and huffman coding. In ICLR, 2016.\n[12] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights\nand connections for efﬁcient neural network. In NIPS, pages\n1135–1143, 2015.",
            "zation and huffman coding. In ICLR, 2016.\n[12] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights\nand connections for efﬁcient neural network. In NIPS, pages\n1135–1143, 2015.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into\nrectiﬁers: Surpassing human-level performance on imagenet\nclassiﬁcation. In ICCV, 2015.\n[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\nfor image recognition. In CVPR, 2016.\n[15] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in\ndeep residual networks. In ECCV, pages 630–645. Springer,\n2016.\n[16] G. Huang, D. Chen, T. Li, F. Wu, L. van der Maaten, and\nK. Q. Weinberger. Multi-scale dense convolutional networks\nfor efﬁcient prediction.\narXiv preprint arXiv:1703.09844,\n2017.\n[17] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.\nDensely connected convolutional networks. In CVPR, 2017.\n[18] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.\nDeep networks with stochastic depth. In ECCV, 2016.",
            "Densely connected convolutional networks. In CVPR, 2017.\n[18] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.\nDeep networks with stochastic depth. In ECCV, 2016.\n[19] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\narXiv preprint arXiv:1502.03167, 2015.\n[20] J. Jin, Z. Yan, K. Fu, N. Jiang, and C. Zhang. Neural network\narchitecture optimization through submodularity and super-\nmodularity. arXiv preprint arXiv:1609.00074, 2016.\n[21] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images. In Tech Report, 2009.\n[22] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, pages 1097–1105, 2012.\n[23] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P.\nGraf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint\narXiv:1608.08710, 2016.\n[24] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,\n2014.",
            "Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint\narXiv:1608.08710, 2016.\n[24] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,\n2014.\n[25] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.\nSparse convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 806–814, 2015.\n[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, pages 3431–\n3440, 2015.\n[27] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.\nNg. Reading digits in natural images with unsupervised fea-\nture learning, 2011. In NIPS Workshop on Deep Learning\nand Unsupervised Feature Learning, 2011.\n[28] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. Xnor-\nnet: Imagenet classiﬁcation using binary convolutional neu-\nral networks. In ECCV, 2016.\n[29] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini.\nGroup sparse regularization for deep neural networks. arXiv",
            "ral networks. In ECCV, 2016.\n[29] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini.\nGroup sparse regularization for deep neural networks. arXiv\npreprint arXiv:1607.00485, 2016.\n[30] M. Schmidt, G. Fung, and R. Rosales.\nFast optimization\nmethods for l1 regularization: A comparative study and two\nnew approaches. In ECML, pages 286–297, 2007.\n[31] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n[32] S. Srinivas, A. Subramanya, and R. V. Babu. Training sparse\nneural networks. CoRR, abs/1611.06694, 2016.\n[33] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the\nimportance of initialization and momentum in deep learning.\nIn ICML, 2013.\n[34] C. Szegedy,\nW. Liu,\nY. Jia,\nP. Sermanet,\nS. Reed,\nD. Anguelov, D. Erhan, et al. Going deeper with convolu-\ntions. In CVPR, pages 1–9, 2015.\n[35] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning\nstructured sparsity in deep neural networks. In NIPS, 2016.\n[36] S. Zagoruyko.",
            "tions. In CVPR, pages 1–9, 2015.\n[35] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning\nstructured sparsity in deep neural networks. In NIPS, 2016.\n[36] S. Zagoruyko.\n92.5% on cifar-10 in torch.\nhttps://\ngithub.com/szagoruyko/cifar.torch.\n[37] H. Zhou, J. M. Alvarez, and F. Porikli. Less is more: Towards\ncompact cnns. In ECCV, 2016.\n[38] B. Zoph and Q. V. Le. Neural architecture search with rein-\nforcement learning. In ICLR, 2017.\n2744"
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 1,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 2,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 3,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 4,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 5,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 5,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 5,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 5,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 5,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 6,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 7,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 8,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 8,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 8,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 8,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 8,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            },
            {
                "page": 9,
                "source": "Liu_Learning_Efficient_Convolutional_ICCV_2017_paper.pdf",
                "title": "Learning Efficient Convolutional Networks Through Network Slimming",
                "authors": "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang"
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25",
            "chunk_26",
            "chunk_27",
            "chunk_28",
            "chunk_29",
            "chunk_30",
            "chunk_31",
            "chunk_32",
            "chunk_33",
            "chunk_34",
            "chunk_35",
            "chunk_36",
            "chunk_37",
            "chunk_38",
            "chunk_39",
            "chunk_40",
            "chunk_41",
            "chunk_42",
            "chunk_43",
            "chunk_44",
            "chunk_45",
            "chunk_46",
            "chunk_47",
            "chunk_48",
            "chunk_49",
            "chunk_50",
            "chunk_51",
            "chunk_52",
            "chunk_53",
            "chunk_54",
            "chunk_55"
        ]
    },
    "NIPS-1989-optimal-brain-damage-Paper.pdf": {
        "chunks": [
            "598 \nLe Cun, Denker and Solla \nOptimal Brain Damage \nYann Le Cun, John S. Denker and Sara A. Sol1a \nAT&T Bell Laboratories, Holmdel, N. J. 07733 \nABSTRACT \nWe have used information-theoretic ideas to derive a class of prac-\ntical and nearly optimal schemes for adapting the size of a neural \nnetwork. By removing unimportant weights from a network, sev-\neral improvements can be expected: better generalization, fewer \ntraining examples required, and improved speed of learning and/or \nclassification. The basic idea is to use second-derivative informa-\ntion to make a tradeoff between network complexity and training \nset error. Experiments confirm the usefulness of the methods on a \nreal-world application. \n1 \nINTRODUCTION \nMost successful applications of neural network learning to real-world problems have \nbeen achieved using highly structured networks of rather large size [for example \n(Waibel, 1989; Le Cun et al., 1990a)]. As applications become more complex, the",
            "been achieved using highly structured networks of rather large size [for example \n(Waibel, 1989; Le Cun et al., 1990a)]. As applications become more complex, the \nnetworks will presumably become even larger and more structured. Design tools \nand techniques for comparing different architectures and minimizing the network \nsize will be needed. More importantly, as the number of parameters in the systems \nincreases, overfitting problems may arise, with devastating effects on the general-\nization performance. We introduce a new technique called Optimal Brain Damage \n(OBD) for reducing the size of a learning network by selectively deleting weights. \nWe show that OBD can be used both as an automatic network minimization pro-\ncedure and as an interactive tool to suggest better architectures. \nThe basic idea of OBD is that it is possible to take a perfectly reasonable network, \ndelete half (or more) of the weights and wind up with a network that works just as",
            "The basic idea of OBD is that it is possible to take a perfectly reasonable network, \ndelete half (or more) of the weights and wind up with a network that works just as \nwell, or better. It can be applied in situations where a complicated problem must",
            "Optimal Brain Damage \n599 \nbe solved, and the system must make optimal use of a limited amount of training \ndata. It is known from theory (Denker et al., 1987; Baum and Haussler, 1989; Solla \net al., 1990) and experience (Le Cun, 1989) that, for a fixed amount of training \ndata, networks with too many weights do not generalize well. On the other hand. \nnetworks with too few weights will not have enough power to represent the data \naccurately. The best generalization is obtained by trading off the training error and \nthe network complexity. \nOne technique to reach this tradeoff is to minimize a cost function composed of two \nterms: the ordinary training error, plus some measure of the network complexity. \nSeveral such schemes have been proposed in the statistical inference literature [see \n(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in \nthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer \nand Smolensky, 1989).",
            "(Akaike, 1986; Rissanen, 1989; Vapnik, 1989) and references therein] as well as in \nthe NN literature (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989; Mozer \nand Smolensky, 1989). \nVarious complexity measures have been proposed, including Vapnik-Chervonenkis \ndimensionality (Vapnik and Chervonenkis, 1971) and description length (Rissanen, \n1989). A time-honored (albeit inexact) measure of complexity is simply the number \nof non-zero free parameters, which is the measure we choose to use in this paper \n[but see (Denker, Le Cun and Solla, 1990)]. Free parameters are used rather than \nconnections, since in constrained networks, several connections can be controlled by \na single parameter. \nIn most cases in the statistical inference literature, there is some a priori or heuristic \ninformation that dictates the order in which parameters should be deleted; for \nexample, in a family of polynomials, a smoothness heuristic may require high-order",
            "information that dictates the order in which parameters should be deleted; for \nexample, in a family of polynomials, a smoothness heuristic may require high-order \nterms to be deleted first. In a neural network, however, it is not at all obvious in \nwhich order the parameters should be deleted. \nA simple strategy consists in deleting parameters with small \"saliency\", i.e. those \nwhose deletion will have the least effect on the training error. Other things be-\ning equal, small-magnitude parameters will have the least saliency, so a reasonable \ninitial strategy is to train the network and delete small-magnitude parameters in \norder. After deletion, the network should be retrained. Of course this procedure \ncan be iterated; in the limit it reduces to continuous weight-decay during training \n(using disproportionately rapid decay of small-magnitude parameters). In fact, sev-\neral network minimization schemes have been implemented using non-proportional",
            "(using disproportionately rapid decay of small-magnitude parameters). In fact, sev-\neral network minimization schemes have been implemented using non-proportional \nweight decay (Rumelhart, 1988; Chauvin, 1989; Hanson and Pratt, 1989), or \"gat-\ning coefficients\" (Mozer and Smolensky, 1989). Generalization performance has \nbeen reported to increase significantly on the somewhat small problems examined. \nTwo drawbacks of these techniques are that they require fine-tuning of the \"prun-\ning\" coefficients to avoid catastrophic effects, and also that the learning process \nis significantly slowed down. Such methods include the implicit hypothesis that \nthe appropriate measure of network complexity is the number of parameters (or \nsometimes the number of units) in the network. \nOne of the main points of this paper is to move beyond the approximation that \n\"magnitude equals saliency\" , and propose a theoretically justified saliency measure.",
            "600 \nLe Cun, Denker and Solla \nOur technique uses the second derivative of the objective function with respect to \nthe parameters to compute the saliencies. The method was ,,-alidated using our \nhandwritten digit recognition network trained with backpropagation (Le Cun et aI., \n1990b). \n2 \nOPTIMAL BRAIN DAMAGE \nObjective functions playa central role in this field; therefore it is more than rea-\nsonable to define the saliency of a parameter to be the change in the objective \nfunction caused by deleting that parameter. It would be prohibiti,-ely laborious to \nevaluate the saliency directly from this definition, i.e. by temporarily deleting each \nparameter and reevaluating the objective function. \nFortunately, it is possible to construct a local model of the error function and \nanalytically predict the effect of perturbing the parameter vector. \"'e approximate \nthe objective function E by a Taylor series. A perturbation lL~ of the parameter \nvector will change the objective function by",
            "the objective function E by a Taylor series. A perturbation lL~ of the parameter \nvector will change the objective function by \n(1) \nHere, the 6ui'S are the components of flJ, the gi's are the components of the gradient \nG of E with respect to U, and the hi;'S are the elements of the Hessian matrix H \nof E with respect to U: \n8E \ngi= -8 \nUi \nand \n(2) \nThe goal is to find a set of parameters whose deletion will cause the least increase \nof E . This problem is practically insoluble in the general case. One reason is \nthat the matrix H is enormous (6.5 x 106 terms for our 2600 parameter network), \nand is very difficult to compute. Therefore we must introduce some simplifying \napproximations. The \"diagonal\" approximation assumes that the 6E caused by \ndeleting several parameters is the sum of the 6E's caused by delet~ each parameter \nindividually; cross terms are neglected, so third term of the npt hand side of \nequation 1 is discarded. The \"extremal\" approximation assumes that parameter",
            "individually; cross terms are neglected, so third term of the npt hand side of \nequation 1 is discarded. The \"extremal\" approximation assumes that parameter \ndeletion will be performed after training has converged. The parameter vector is \nthen at a (local) minimum of E and the first term of the right hand side of equation 1 \ncan be neglected. Furthermore, at a local minimum, all the hii's are non-negative, \nso any perturbation of the parameters will cause E to increase or stay the same. \nThirdly, the \"quadratic\" approximation assumes that the cost fundion is nearly \nquadratic 80 that the last term in the equation can be neglected. Equation 1 then \nreduces to \n6E=~~h\"6u~ \n2L.i \" • \ni \n(3)",
            "Optimal Brain Damage \n601 \n2.1 \nCOMPUTING THE SECOND DERIVATIVES \nNow we need an efficient way of computing the diagonal second derivatives hii . \nSuch a procedure was derived in (Le Cun, 1987), and was the basis of a fast back-\npropagation method used extensively in \\1lrious applications (Becker and Le Cun, \n1989; Le Cun, 1989; Le Cun et al., 1990a). The procedure is very similar to the \nback-propagation algorithm used for computing the first derivatives. We will only \noutline the proced ure; details can be found in the references. \nWe assume the objective function is the usual mean-squared error (MSE); general-\nization to other additive error measures is straightforward. The following expres-\nsions apply to a single input pattern; afterward E and H must be averaged over \nthe training set. The network state is computed using the standard formulae \nand \nai = L WijZj \nj \n( 4) \nwhere Zi is the state of unit i, ai its total input (weighted sum), ! the squashing",
            "the training set. The network state is computed using the standard formulae \nand \nai = L WijZj \nj \n( 4) \nwhere Zi is the state of unit i, ai its total input (weighted sum), ! the squashing \nfunction and Wij is the connection going from unit j to unit i. In a shared-weight \nnetwork like ours, a single parameter Uk can control one or more connections: Wij = \nUk for all (i, j) E Vk, where Vk is a set of index pairs. By the chain rule, the diagonal \nterms of H are given by \n{)2E \nhu = L \n{)w~, \n(i,j)EV. \n., \nThe summand can be expanded (using the basic network equations 4) as: \n{J2E \nlP E \n2 \n--=-z· \n{Jw~. \n{Ja~' \n., . \nThe second derivatives are back-propagated from layer to layer: \n(5) \n(6) \n(7) \nWe also need the boundary condition at the output layer, specifying the second \nderivative of E with respect to the last-layer weighted BUms: \n{J{J2 ~ = 2!'(ai)2 - 2(di - Zi)!\"(ai) \nai \nfor all units i in the output layer. \n(8)",
            "derivative of E with respect to the last-layer weighted BUms: \n{J{J2 ~ = 2!'(ai)2 - 2(di - Zi)!\"(ai) \nai \nfor all units i in the output layer. \n(8) \nAs can be seen, computing the diagonal Hessian is of the same order of complexity \nas computing the gradient. In some cases, the second term of the right hand side of \nthe last two equations (involving the second derivative of I) can be neglected. This \ncorresponds to the well-known Levenberg-Marquardt approximation, and has the \ninteresting property of giving guaranteed positive estimates of the second derivative.",
            "602 \nLe Cun, Denker and Solla \n2.2 \nTHE RECIPE \nThe OBO procedure can be carried out as follows: \n1. Choose a reasonable network architecture \n2. Train the network until a reasonable solution is obtained \n3. Compute the second derivatives hu for each parameter \n4. Compute the saliencies for each parameter: Sk = huu~/2 \n5. Sort the parameters by saliency and delete some low-saliency parameters \n6. Iterate to step 2 \nDeleting a parameter is defined as setting it to 0 and freezing it there. Several \nvariants of the procedure can be devised, such as decreasing the ... 41ues of the low-\nsaliency parameters instead of simply setting them to 0, or allowing the deleted \nparameters to adapt again after they have been set to o. \n2.3 \nEXPERIMENTS \nThe simulation results given in this section were obtained using back-propagation \napplied to handwritten digit recognition. The initial network was highly constrained \nand sparsely connected, having 105 connections controlled by 2578 free parameters.",
            "applied to handwritten digit recognition. The initial network was highly constrained \nand sparsely connected, having 105 connections controlled by 2578 free parameters. \nIt was trained on a database of segmented handwritten zip code digits and printed \ndigits containing approximately 9300 training examples and 3350 t.est examples. \nMore details can be obtained from the companion paper (Le Cun et al., 1990b). \n16 \n14 \n1 \n10 \npJ 8 \n~6 \nb04 \no - o \n<a> \nMagnitude \nOBD \n~~--~--~---+--~----~ \no \n500 \n1000 1500 2000 2SOO \nParameters \n16 \n14 \n1 \n10 \npJ 8 \n~6 \nb04 \n.9 \no \n(b) \n-2~ __ ~ \n__ ~ \n__ -+ ________ ~ \no \nSOO \n1000 1500 laX) 2SOO \nParameters \nFigure 1: (a) Objective function (in dB) versus number of paramet.ers for OBn \n(lower curve) and magnitude-based parameter deletion (upper curve). (b) Predicted \nand actual objective function versus number of parameters. The predicted value \n(lower curve) is the sum of the saliencies of the deleted parameters.",
            "and actual objective function versus number of parameters. The predicted value \n(lower curve) is the sum of the saliencies of the deleted parameters. \nFigure la shows how the objective function increases (from right to left) as the \nnumber of remaining parameters decreases. It is clear that deletin~ parameters by",
            "Optimal Brain Damage \n603 \norder of saliency causes a significantly smaller increase of the objective function than \ndeleting them according to their magnitude. Random deletions were also tested for \nthe sake of comparison, but the performance was so bad that the curves cannot be \nshown on the same scale. \nFigure 1b shows how the objective function increases (from right to left) as the num-\nber of remaining parameters decreases, compared to the increase predicted by the \nQuadratic-Extremum-Diagonal approximation. Good agrement is obtained for up \nto approximately 800 deleted parameters (approximately 30% of the parameters). \nBeyond that point, the curves begin to split, for several reasons: the off-diagonal \nterms in equation 1 become disproportionately more important as the number of \ndeleted parameters increases, and higher-than-quadratic terms become more impor-\ntant when larger-valued parameters are deleted. \n' \n16 \n14 \n1 \n10 \nUJ 8 \n~ 6 \n~4 \no -\no \n<a) \n-2~--4----+----~--~--~ \no",
            "tant when larger-valued parameters are deleted. \n' \n16 \n14 \n1 \n10 \nUJ 8 \n~ 6 \n~4 \no -\no \n<a) \n-2~--4----+----~--~--~ \no \nSOO \n1000 1500 2000 2500 \nParameters \n16 \n14 \n1 \n10 \nUJ 8 \n~ 6 \n~ 4. \n(b) \n~~ \n-2~I--~,~ __ +, ____ ~, __ ~I ____ ~I \no \n500 \n1000 1500 2000 2500 \nParameters \nFigure 2: Objective function (in dB) versus number of parameters, without re-\ntraining (upper curve), and after retraining (lower curve). Curves are given for the \ntraining set (a) and the test set (b). \nFigure 2 shows the log-MSE on the training set and the on the test set before and \nafter retraining. The performance on the training set and on the test set (after \nretraining) stays almost the same when up to 1500 parameters (60% of the total) \nare deleted. \nWe have also used OBn as an interactive tool for network design and analysis. \nThis contrasts with the usual view of weight deletion as a more-or-Iess automatic \nprocedure. Specifically, we prepared charts depicting the saliency of the 10,000",
            "This contrasts with the usual view of weight deletion as a more-or-Iess automatic \nprocedure. Specifically, we prepared charts depicting the saliency of the 10,000 \nparameters in the digit recognition network reported last year (Le Cun et aI., 1990b). \nTo our surprise, several large groups of parameters were expendable. We were \nable to excise the second-to-Iast layer, thereby reducing the number of parameters \nby a factor of two. The training set MSE increased by a factor of 10, and the \ngeneralization MSE increased by only 50%. The 10-category classification error \non the test set actually decreased (which indicates that MSE is not the optimal",
            "604 \nLe Cun, Denker and Solla \nobjective function for this task). OBD motivated other architectural changes, as \ncan be seen by comparing the 2600-parameter network in (Le Cun et aI., 1990a) to \nthe 1O,OOO-parameter network in (Le Cun et aI., 1990b). \n3 \nCONCLUSIONS AND OUTLOOK \nWe have used Optimal Brain Damage interactively to reduce the number of param-\neters in a practical neural network by a factor of four. We obtained an additional \nfactor of more than two by using OBD to delete parameters automatically. The net-\nwork's speed improved significantly, and its recognition accuracy increased slightly. \nWe emphasize that the starting point was a state-of-the-art network. It would be \ntoo easy to start with a foolish network and make large improvements: a technique \nthat can help improve an already-good network is particularly valuable. \nWe believe that the techniques presented here only scratch the surface of the appli-",
            "that can help improve an already-good network is particularly valuable. \nWe believe that the techniques presented here only scratch the surface of the appli-\ncations where second-derivative information can and should be used. In particular, \nwe have also been able to move beyond the approximation that \"complexity equals \nnumber of free parameters\" by using second-derivative information. In (Denker, Le \nCun and Solla, 1990), we use it to to derive an improved measure of the network's \ninformation content, or complexity. This allows us to compare network architec-\ntures on a given task, and makes contact with the notion of Minimum Description \nLength (MDL) (Rissanen, 1989). The main idea is that a \"simple\" network whose \ndescription needs a small number of bits is more likely to generalize correctly than \na more complex network, because it presumably has extracted the essence of the \ndata and removed the redundancy from it. \nAcknowledgments",
            "a more complex network, because it presumably has extracted the essence of the \ndata and removed the redundancy from it. \nAcknowledgments \nWe thank the US Postal Service and its contractors for providing us with the \ndatabase. We also thank Rich Howard and Larry Jackel for their helpful comments \nand encouragements. We especially thank David Rumelhart for sharing unpublished \nideas. \nReferences \nAkaike, H. (1986). Use of Statistical Models for Time Series Analysis. In Proceedings \nICASSP 86, pages 3147-3155, Tokyo. IEEE. \nBaum, E. B. and Haussler, D. (1989). What Size Net Gives Valid Generaliztion? \nNeural Computation, 1:151-160. \nBecker, S. and Le Cun, Y. (1989). Improving the Convergence of Back-Propagation \nLearning with Second-Order Methods. In Touretzky, D., Hinton, G., and Se-\njnowski, T., editors, Proc. of the 1988 Connectionist Model& S.mmer School, \npages 29-37, San Mateo. Morgan Kaufman. \nChauvin, Y. (1989). A Back-Propagation Algorithm with Optimal Use of Hid-",
            "jnowski, T., editors, Proc. of the 1988 Connectionist Model& S.mmer School, \npages 29-37, San Mateo. Morgan Kaufman. \nChauvin, Y. (1989). A Back-Propagation Algorithm with Optimal Use of Hid-\nden Units. In Touretzky, D., editor, Neural Information Proce$$ing S,&tems, \nvolume 1, Denver, 1988. Morgan Kaufmann.",
            "Optimal Brain Damage \n605 \nDenker, J., Schwartz, D., Wittner, B., Solla, S. A., Howard, R., Jackel, L., and \nHopfield, J. (1987). Large Automatic Learning, Rule Extraction and General-\nization. Complex Systems, 1:877-922. \nDenker, J. S., Le Cun, Y., and Solla, S. A. (1990). Optimal Brain Damage. To \nappear in Computer and System Sciences. \nHanson, S. J. and Pratt, L. Y. (1989). Some Comparisons of Constraints for Min-\nimal Network Construction with Back-Propagation. In Touretzky, D., editor, \nNeural Information Processing Systems, volume 1, Denver, 1988. Morgan Kauf-\nmann. \nLe Cun, Y. (1987). Modeles Connexionnistes de l'Apprentissage. PhD thesis, Uni-\nversite Pierre et Marie Curie, Paris, France. \nLe Cun, Y. (1989). Generalization and Network Design Strategies. In Pfeifer, R., \nSchreter, Z., Fogelman, F., and Steels, L., editors, Connectionism in Perspec-\ntive, Zurich, Switzerland. Elsevier. \nLe Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,",
            "Schreter, Z., Fogelman, F., and Steels, L., editors, Connectionism in Perspec-\ntive, Zurich, Switzerland. Elsevier. \nLe Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, \nW., and Jackel, L. D. (1990a). Handwritten Digit Recognition with a Back-\nPropagation Network. In Touretzky, D., editor, Neural Information Processing \nSystems, volume 2, Denver, 1989. Morgan Kaufman. \nLe Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., \nand Jackel, L. D. (1990b). Back-Propagation Applied to Handwritten Zipcode \nRecognition. Neural Computation, 1{ 4). \nMozer, M. C. and Smolensky, P. (1989). Skeletonization: A Technique for Trim-\nming the Fat from a Network via Relevance Assessment. In Touretzky, D., \neditor, Neural Information Processing Systefn$, volume 1, Denver, 1988. Mor-\ngan Kaufmann. \nRissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scientific, \nSingapore. \nRumeihart, D. E. (1988). personal communication.",
            "gan Kaufmann. \nRissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scientific, \nSingapore. \nRumeihart, D. E. (1988). personal communication. \nSolla, S. A., Schwartz, D. B., Tishby, N., and Levin, E. (1990). Supervised Learn-\ning: a Theoretical Framework. In Touretzky, D., editor, Neural Information \nProcessing Systems, volume 2, Denver, 1989. Morgan Kaufman. \nVapnik, V. N. (1989). Inductive Principles of the Search for Empirical Dependences. \nIn Proceedings of the second annual Workshop on Computational Learning The-\nory, pages 3-21. Morgan Kaufmann. \nVapnik, V. N. and Chervonenkis, A. Y. (1971). On the Uniform Convergence of \nRelative Frequencies of Events to Their Probabilities. Th. Pro6. and its Appli-\ncations, 17(2):264-280. \nWaibel, A. (1989). Consonant Recognition by Modular Construction of Large \nPhonemic Time-Delay Neural Networks. In Touretzky, D., editor, Neural In-\nformation Processing Systems, volume 1, pages 215-223, Denver, 1988. Morgan \nKaufmann."
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 1,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 1,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 2,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 2,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 2,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 2,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 3,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 3,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 3,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 4,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 4,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 4,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 5,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 5,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 5,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 6,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 6,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 6,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 7,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 7,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 7,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 7,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 8,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 8,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            },
            {
                "page": 8,
                "source": "NIPS-1989-optimal-brain-damage-Paper.pdf",
                "title": "Optimal Brain Damage",
                "authors": "Yann LeCun, John S. Denker, Sara A. Solla"
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25"
        ]
    },
    "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf": {
        "chunks": [
            "Published as a conference paper at ICLR 2017\nPRUNING FILTERS FOR EFFICIENT CONVNETS\nHao Li∗\nUniversity of Maryland\nhaoli@cs.umd.edu\nAsim Kadav\nNEC Labs America\nasim@nec-labs.com\nIgor Durdanovic\nNEC Labs America\nigord@nec-labs.com\nHanan Samet†\nUniversity of Maryland\nhjs@cs.umd.edu\nHans Peter Graf\nNEC Labs America\nhpg@nec-labs.com\nABSTRACT\nThe success of CNNs in various applications is accompanied by a signiﬁcant\nincrease in the computation and parameter storage costs. Recent efforts toward\nreducing these overheads involve pruning and compressing the weights of various\nlayers without hurting original accuracy. However, magnitude-based pruning of\nweights reduces a signiﬁcant number of parameters from the fully connected layers\nand may not adequately reduce the computation costs in the convolutional layers\ndue to irregular sparsity in the pruned networks. We present an acceleration method\nfor CNNs, where we prune ﬁlters from CNNs that are identiﬁed as having a small",
            "due to irregular sparsity in the pruned networks. We present an acceleration method\nfor CNNs, where we prune ﬁlters from CNNs that are identiﬁed as having a small\neffect on the output accuracy. By removing whole ﬁlters in the network together\nwith their connecting feature maps, the computation costs are reduced signiﬁcantly.\nIn contrast to pruning weights, this approach does not result in sparse connectivity\npatterns. Hence, it does not need the support of sparse convolution libraries and\ncan work with existing efﬁcient BLAS libraries for dense matrix multiplications.\nWe show that even simple ﬁlter pruning techniques can reduce inference costs for\nVGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining\nclose to the original accuracy by retraining the networks.\n1\nINTRODUCTION\nThe ImageNet challenge has led to signiﬁcant advancements in exploring various architectural\nchoices in CNNs (Russakovsky et al. (2015); Krizhevsky et al. (2012); Simonyan & Zisserman",
            "1\nINTRODUCTION\nThe ImageNet challenge has led to signiﬁcant advancements in exploring various architectural\nchoices in CNNs (Russakovsky et al. (2015); Krizhevsky et al. (2012); Simonyan & Zisserman\n(2015); Szegedy et al. (2015a); He et al. (2016)). The general trend since the past few years has\nbeen that the networks have grown deeper, with an overall increase in the number of parameters and\nconvolution operations. These high capacity networks have signiﬁcant inference costs especially\nwhen used with embedded sensors or mobile devices where computational and power resources\nmay be limited. For these applications, in addition to accuracy, computational efﬁciency and small\nnetwork sizes are crucial enabling factors (Szegedy et al. (2015b)). In addition, for web services\nthat provide image search and image classiﬁcation APIs that operate on a time budget often serving\nhundreds of thousands of images per second, beneﬁt signiﬁcantly from lower inference times.",
            "that provide image search and image classiﬁcation APIs that operate on a time budget often serving\nhundreds of thousands of images per second, beneﬁt signiﬁcantly from lower inference times.\nThere has been a signiﬁcant amount of work on reducing the storage and computation costs by model\ncompression (Le Cun et al. (1989); Hassibi & Stork (1993); Srinivas & Babu (2015); Han et al.\n(2015); Mariet & Sra (2016)). Recently Han et al. (2015; 2016b) report impressive compression rates\non AlexNet (Krizhevsky et al. (2012)) and VGGNet (Simonyan & Zisserman (2015)) by pruning\nweights with small magnitudes and then retraining without hurting the overall accuracy. However,\npruning parameters does not necessarily reduce the computation time since the majority of the\nparameters removed are from the fully connected layers where the computation cost is low, e.g., the\nfully connected layers of VGG-16 occupy 90% of the total parameters but only contribute less than",
            "parameters removed are from the fully connected layers where the computation cost is low, e.g., the\nfully connected layers of VGG-16 occupy 90% of the total parameters but only contribute less than\n1% of the overall ﬂoating point operations (FLOP). They also demonstrate that the convolutional\nlayers can be compressed and accelerated (Iandola et al. (2016)), but additionally require sparse\n∗Work done at NEC Labs\n†Supported in part by the NSF under Grant IIS-13-2079\n1\narXiv:1608.08710v3  [cs.CV]  10 Mar 2017",
            "Published as a conference paper at ICLR 2017\nBLAS libraries or even specialized hardware (Han et al. (2016a)). Modern libraries that provide\nspeedup using sparse operations over CNNs are often limited (Szegedy et al. (2015a); Liu et al.\n(2015)) and maintaining sparse data structures also creates an additional storage overhead which can\nbe signiﬁcant for low-precision weights.\nRecent work on CNNs have yielded deep architectures with more efﬁcient design (Szegedy et al.\n(2015a;b); He & Sun (2015); He et al. (2016)), in which the fully connected layers are replaced with\naverage pooling layers (Lin et al. (2013); He et al. (2016)), which reduces the number of parameters\nsigniﬁcantly. The computation cost is also reduced by downsampling the image at an early stage\nto reduce the size of feature maps (He & Sun (2015)). Nevertheless, as the networks continue to\nbecome deeper, the computation costs of convolutional layers continue to dominate.",
            "to reduce the size of feature maps (He & Sun (2015)). Nevertheless, as the networks continue to\nbecome deeper, the computation costs of convolutional layers continue to dominate.\nCNNs with large capacity usually have signiﬁcant redundancy among different ﬁlters and feature\nchannels. In this work, we focus on reducing the computation cost of well-trained CNNs by pruning\nﬁlters. Compared to pruning weights across the network, ﬁlter pruning is a naturally structured way\nof pruning without introducing sparsity and therefore does not require using sparse libraries or any\nspecialized hardware. The number of pruned ﬁlters correlates directly with acceleration by reducing\nthe number of matrix multiplications, which is easy to tune for a target speedup. In addition, instead\nof layer-wise iterative ﬁne-tuning (retraining), we adopt a one-shot pruning and retraining strategy to\nsave retraining time for pruning ﬁlters across multiple layers, which is critical for pruning very deep",
            "save retraining time for pruning ﬁlters across multiple layers, which is critical for pruning very deep\nnetworks. Finally, we observe that even for ResNets, which have signiﬁcantly fewer parameters and\ninference costs than AlexNet or VGGNet, still have about 30% of FLOP reduction without sacriﬁcing\ntoo much accuracy. We conduct sensitivity analysis for convolutional layers in ResNets that improves\nthe understanding of ResNets.\n2\nRELATED WORK\nThe early work by Le Cun et al. (1989) introduces Optimal Brain Damage, which prunes weights\nwith a theoretically justiﬁed saliency measure. Later, Hassibi & Stork (1993) propose Optimal Brain\nSurgeon to remove unimportant weights determined by the second-order derivative information.\nMariet & Sra (2016) reduce the network redundancy by identifying a subset of diverse neurons that\ndoes not require retraining. However, this method only operates on the fully-connected layers and\nintroduce sparse connections.",
            "does not require retraining. However, this method only operates on the fully-connected layers and\nintroduce sparse connections.\nTo reduce the computation costs of the convolutional layers, past work have proposed to approximate\nconvolutional operations by representing the weight matrix as a low rank product of two smaller\nmatrices without changing the original number of ﬁlters (Denil et al. (2013); Jaderberg et al. (2014);\nZhang et al. (2015b;a); Tai et al. (2016); Ioannou et al. (2016)). Other approaches to reduce the\nconvolutional overheads include using FFT based convolutions (Mathieu et al. (2013)) and fast\nconvolution using the Winograd algorithm (Lavin & Gray (2016)). Additionally, quantization (Han\net al. (2016b)) and binarization (Rastegari et al. (2016); Courbariaux & Bengio (2016)) can be used\nto reduce the model size and lower the computation overheads. Our method can be used in addition\nto these techniques to reduce computation costs without incurring additional overheads.",
            "to reduce the model size and lower the computation overheads. Our method can be used in addition\nto these techniques to reduce computation costs without incurring additional overheads.\nSeveral work have studied removing redundant feature maps from a well trained network (Anwar et al.\n(2015); Polyak & Wolf (2015)). Anwar et al. (2015) introduce a three-level pruning of the weights\nand locate the pruning candidates using particle ﬁltering, which selects the best combination from\na number of random generated masks. Polyak & Wolf (2015) detect the less frequently activated\nfeature maps with sample input data for face detection applications. We choose to analyze the\nﬁlter weights and prune ﬁlters with their corresponding feature maps using a simple magnitude\nbased measure, without examining possible combinations. We also introduce network-wide holistic\napproaches to prune ﬁlters for simple and complex convolutional network architectures.",
            "based measure, without examining possible combinations. We also introduce network-wide holistic\napproaches to prune ﬁlters for simple and complex convolutional network architectures.\nConcurrently with our work, there is a growing interest in training compact CNNs with sparse\nconstraints (Lebedev & Lempitsky (2016); Zhou et al. (2016); Wen et al. (2016)).\nLebedev &\nLempitsky (2016) leverage group-sparsity on the convolutional ﬁlters to achieve structured brain\ndamage, i.e., prune the entries of the convolution kernel in a group-wise fashion. Zhou et al. (2016)\nadd group-sparse regularization on neurons during training to learn compact CNNs with reduced\nﬁlters. Wen et al. (2016) add structured sparsity regularizer on each layer to reduce trivial ﬁlters,\nchannels or even layers. In the ﬁlter-level pruning, all above work use ℓ2,1-norm as a regularizer.\n2",
            "Published as a conference paper at ICLR 2017\nSimilar to the above work, we use ℓ1-norm to select unimportant ﬁlters and physically prune them.\nOur ﬁne-tuning process is the same as the conventional training procedure, without introducing\nadditional regularization. Our approach does not introduce extra layer-wise meta-parameters for the\nregularizer except for the percentage of ﬁlters to be pruned, which is directly related to the desired\nspeedup. By employing stage-wise pruning, we can set a single pruning rate for all layers in one\nstage.\n3\nPRUNING FILTERS AND FEATURE MAPS\nLet ni denote the number of input channels for the ith convolutional layer and hi/wi be the\nheight/width of the input feature maps. The convolutional layer transforms the input feature maps\nxi ∈Rni×hi×wi into the output feature maps xi+1 ∈Rni+1×hi+1×wi+1, which are used as in-\nput feature maps for the next convolutional layer. This is achieved by applying ni+1 3D ﬁlters",
            "xi ∈Rni×hi×wi into the output feature maps xi+1 ∈Rni+1×hi+1×wi+1, which are used as in-\nput feature maps for the next convolutional layer. This is achieved by applying ni+1 3D ﬁlters\nFi,j ∈Rni×k×k on the ni input channels, in which one ﬁlter generates one feature map. Each\nﬁlter is composed by ni 2D kernels K ∈Rk×k (e.g., 3 × 3). All the ﬁlters, together, constitute\nthe kernel matrix Fi ∈Rni×ni+1×k×k. The number of operations of the convolutional layer is\nni+1nik2hi+1wi+1. As shown in Figure 1, when a ﬁlter Fi,j is pruned, its corresponding feature\nmap xi+1,j is removed, which reduces nik2hi+1wi+1 operations. The kernels that apply on the\nremoved feature maps from the ﬁlters of the next convolutional layer are also removed, which saves\nan additional ni+2k2hi+2wi+2 operations. Pruning m ﬁlters of layer i will reduce m/ni+1 of the\ncomputation cost for both layers i and i + 1.\nFigure 1: Pruning a ﬁlter results in removal of its corresponding feature map and related kernels in",
            "computation cost for both layers i and i + 1.\nFigure 1: Pruning a ﬁlter results in removal of its corresponding feature map and related kernels in\nthe next layer.\n3.1\nDETERMINING WHICH FILTERS TO PRUNE WITHIN A SINGLE LAYER\nOur method prunes the less useful ﬁlters from a well-trained model for computational efﬁciency\nwhile minimizing the accuracy drop. We measure the relative importance of a ﬁlter in each layer\nby calculating the sum of its absolute weights P |Fi,j|, i.e., its ℓ1-norm ∥Fi,j∥1. Since the number\nof input channels, ni, is the same across ﬁlters, P |Fi,j| also represents the average magnitude\nof its kernel weights. This value gives an expectation of the magnitude of the output feature map.\nFilters with smaller kernel weights tend to produce feature maps with weak activations as compared\nto the other ﬁlters in that layer. Figure 2(a) illustrates the distribution of ﬁlters’ absolute weights",
            "to the other ﬁlters in that layer. Figure 2(a) illustrates the distribution of ﬁlters’ absolute weights\nsum for each convolutional layer in a VGG-16 network trained on the CIFAR-10 dataset, where the\ndistribution varies signiﬁcantly across layers. We ﬁnd that pruning the smallest ﬁlters works better\nin comparison with pruning the same number of random or largest ﬁlters (Section 4.4). Compared\nto other criteria for activation-based feature map pruning (Section 4.5), we ﬁnd ℓ1-norm is a good\ncriterion for data-free ﬁlter selection.\nThe procedure of pruning m ﬁlters from the ith convolutional layer is as follows:\n1. For each ﬁlter Fi,j, calculate the sum of its absolute kernel weights sj = Pni\nl=1\nP |Kl|.\n2. Sort the ﬁlters by sj.\n3. Prune m ﬁlters with the smallest sum values and their corresponding feature maps. The\nkernels in the next convolutional layer corresponding to the pruned feature maps are also\nremoved.",
            "3. Prune m ﬁlters with the smallest sum values and their corresponding feature maps. The\nkernels in the next convolutional layer corresponding to the pruned feature maps are also\nremoved.\n4. A new kernel matrix is created for both the ith and i + 1th layers, and the remaining kernel\nweights are copied to the new model.\n3",
            "Published as a conference paper at ICLR 2017\n(a) Filters are ranked by sj\n(b) Prune the smallest ﬁlters\n(c) Prune and retrain\nFigure 2: (a) Sorting ﬁlters by absolute weights sum for each layer of VGG-16 on CIFAR-10. The\nx-axis is the ﬁlter index divided by the total number of ﬁlters. The y-axis is the ﬁlter weight sum\ndivided by the max sum value among ﬁlters in that layer. (b) Pruning ﬁlters with the lowest absolute\nweights sum and their corresponding test accuracies on CIFAR-10. (c) Prune and retrain for each\nsingle layer of VGG-16 on CIFAR-10. Some layers are sensitive and it can be harder to recover\naccuracy after pruning them.\nRelationship to pruning weights Pruning ﬁlters with low absolute weights sum is similar to pruning\nlow magnitude weights (Han et al. (2015)). Magnitude-based weight pruning may prune away whole\nﬁlters when all the kernel weights of a ﬁlter are lower than a given threshold. However, it requires",
            "low magnitude weights (Han et al. (2015)). Magnitude-based weight pruning may prune away whole\nﬁlters when all the kernel weights of a ﬁlter are lower than a given threshold. However, it requires\na careful tuning of the threshold and it is difﬁcult to predict the exact number of ﬁlters that will\neventually be pruned. Furthermore, it generates sparse convolutional kernels which can be hard to\naccelerate given the lack of efﬁcient sparse libraries, especially for the case of low-sparsity.\nRelationship to group-sparse regularization on ﬁlters\nRecent work (Zhou et al. (2016); Wen\net al. (2016)) apply group-sparse regularization (Pni\nj=1 ∥Fi,j∥2 or ℓ2,1-norm) on convolutional ﬁlters,\nwhich also favor to zero-out ﬁlters with small l2-norms, i.e. Fi,j = 0. In practice, we do not observe\nnoticeable difference between the ℓ2-norm and the ℓ1-norm for ﬁlter selection, as the important\nﬁlters tend to have large values for both measures (Appendix 6.1). Zeroing out weights of multiple",
            "noticeable difference between the ℓ2-norm and the ℓ1-norm for ﬁlter selection, as the important\nﬁlters tend to have large values for both measures (Appendix 6.1). Zeroing out weights of multiple\nﬁlters during training has a similar effect to pruning ﬁlters with the strategy of iterative pruning and\nretraining as introduced in Section 3.4.\n3.2\nDETERMINING SINGLE LAYER’S SENSITIVITY TO PRUNING\nTo understand the sensitivity of each layer, we prune each layer independently and evaluate the\nresulting pruned network’s accuracy on the validation set. Figure 2(b) shows that layers that maintain\ntheir accuracy as ﬁlters are pruned away correspond to layers with larger slopes in Figure 2(a). On\nthe contrary, layers with relatively ﬂat slopes are more sensitive to pruning. We empirically determine\nthe number of ﬁlters to prune for each layer based on their sensitivity to pruning. For deep networks\nsuch as VGG-16 or ResNets, we observe that layers in the same stage (with the same feature map",
            "the number of ﬁlters to prune for each layer based on their sensitivity to pruning. For deep networks\nsuch as VGG-16 or ResNets, we observe that layers in the same stage (with the same feature map\nsize) have a similar sensitivity to pruning. To avoid introducing layer-wise meta-parameters, we use\nthe same pruning ratio for all layers in the same stage. For layers that are sensitive to pruning, we\nprune a smaller percentage of these layers or completely skip pruning them.\n3.3\nPRUNING FILTERS ACROSS MULTIPLE LAYERS\nWe now discuss how to prune ﬁlters across the network. Previous work prunes the weights on a layer\nby layer basis, followed by iteratively retraining and compensating for any loss of accuracy (Han et al.\n(2015)). However, understanding how to prune ﬁlters of multiple layers at once can be useful: 1) For\ndeep networks, pruning and retraining on a layer by layer basis can be extremely time-consuming 2)",
            "(2015)). However, understanding how to prune ﬁlters of multiple layers at once can be useful: 1) For\ndeep networks, pruning and retraining on a layer by layer basis can be extremely time-consuming 2)\nPruning layers across the network gives a holistic view of the robustness of the network resulting in a\nsmaller network 3) For complex networks, a holistic approach may be necessary. For example, for\nthe ResNet, pruning the identity feature maps or the second layer of each residual block results in\nadditional pruning of other layers.\nTo prune ﬁlters across multiple layers, we consider two strategies for layer-wise ﬁlter selection:\n4",
            "Published as a conference paper at ICLR 2017\n• Independent pruning determines which ﬁlters should be pruned at each layer independent of\nother layers.\n• Greedy pruning accounts for the ﬁlters that have been removed in the previous layers.\nThis strategy does not consider the kernels for the previously pruned feature maps while\ncalculating the sum of absolute weights.\nFigure 3 illustrates the difference between two approaches in calculating the sum of absolute weights.\nThe greedy approach, though not globally optimal, is holistic and results in pruned networks with\nhigher accuracy especially when many ﬁlters are pruned.\nFigure 3: Pruning ﬁlters across consecutive layers. The independent pruning strategy calculates\nthe ﬁlter sum (columns marked in green) without considering feature maps removed in previous\nlayer (shown in blue), so the kernel weights marked in yellow are still included. The greedy pruning",
            "the ﬁlter sum (columns marked in green) without considering feature maps removed in previous\nlayer (shown in blue), so the kernel weights marked in yellow are still included. The greedy pruning\nstrategy does not count kernels for the already pruned feature maps. Both approaches result in a\n(ni+1 −1) × (ni+2 −1) kernel matrix.\nFigure 4: Pruning residual blocks with the projection shortcut. The ﬁlters to be pruned for the second\nlayer of the residual block (marked as green) are determined by the pruning result of the shortcut\nprojection. The ﬁrst layer of the residual block can be pruned without restrictions.\nFor simpler CNNs like VGGNet or AlexNet, we can easily prune any of the ﬁlters in any convolutional\nlayer. However, for complex network architectures such as Residual networks (He et al. (2016)),\npruning ﬁlters may not be straightforward. The architecture of ResNet imposes restrictions and the",
            "layer. However, for complex network architectures such as Residual networks (He et al. (2016)),\npruning ﬁlters may not be straightforward. The architecture of ResNet imposes restrictions and the\nﬁlters need to be pruned carefully. We show the ﬁlter pruning for residual blocks with projection\nmapping in Figure 4. Here, the ﬁlters of the ﬁrst layer in the residual block can be arbitrarily pruned,\nas it does not change the number of output feature maps of the block. However, the correspondence\nbetween the output feature maps of the second convolutional layer and the identity feature maps\nmakes it difﬁcult to prune. Hence, to prune the second convolutional layer of the residual block, the\ncorresponding projected feature maps must also be pruned. Since the identical feature maps are more\nimportant than the added residual maps, the feature maps to be pruned should be determined by the\npruning results of the shortcut layer. To determine which identity feature maps are to be pruned, we",
            "important than the added residual maps, the feature maps to be pruned should be determined by the\npruning results of the shortcut layer. To determine which identity feature maps are to be pruned, we\nuse the same selection criterion based on the ﬁlters of the shortcut convolutional layers (with 1 × 1\nkernels). The second layer of the residual block is pruned with the same ﬁlter index as selected by\nthe pruning of the shortcut layer.\n3.4\nRETRAINING PRUNED NETWORKS TO REGAIN ACCURACY\nAfter pruning the ﬁlters, the performance degradation should be compensated by retraining the\nnetwork. There are two strategies to prune the ﬁlters across multiple layers:\n5",
            "Published as a conference paper at ICLR 2017\n1. Prune once and retrain: Prune ﬁlters of multiple layers at once and retrain them until the original\naccuracy is restored.\n2. Prune and retrain iteratively: Prune ﬁlters layer by layer or ﬁlter by ﬁlter and then retrain iteratively.\nThe model is retrained before pruning the next layer for the weights to adapt to the changes from the\npruning process.\nWe ﬁnd that for the layers that are resilient to pruning, the prune and retrain once strategy can be\nused to prune away signiﬁcant portions of the network and any loss in accuracy can be regained by\nretraining for a short period of time (less than the original training time). However, when some ﬁlters\nfrom the sensitive layers are pruned away or large portions of the networks are pruned away, it may\nnot be possible to recover the original accuracy. Iterative pruning and retraining may yield better\nresults, but the iterative process requires many more epochs especially for very deep networks.\n4",
            "not be possible to recover the original accuracy. Iterative pruning and retraining may yield better\nresults, but the iterative process requires many more epochs especially for very deep networks.\n4\nEXPERIMENTS\nWe prune two types of networks: simple CNNs (VGG-16 on CIFAR-10) and Residual networks\n(ResNet-56/110 on CIFAR-10 and ResNet-34 on ImageNet). Unlike AlexNet or VGG (on ImageNet)\nthat are often used to demonstrate model compression, both VGG (on CIFAR-10) and Residual\nnetworks have fewer parameters in the fully connected layers. Hence, pruning a large percentage\nof parameters from these networks is challenging. We implement our ﬁlter pruning method in\nTorch7 (Collobert et al. (2011)). When ﬁlters are pruned, a new model with fewer ﬁlters is created\nand the remaining parameters of the modiﬁed layers as well as the unaffected layers are copied into\nthe new model. Furthermore, if a convolutional layer is pruned, the weights of the subsequent batch",
            "and the remaining parameters of the modiﬁed layers as well as the unaffected layers are copied into\nthe new model. Furthermore, if a convolutional layer is pruned, the weights of the subsequent batch\nnormalization layer are also removed. To get the baseline accuracies for each network, we train each\nmodel from scratch and follow the same pre-processing and hyper-parameters as ResNet (He et al.\n(2016)). For retraining, we use a constant learning rate 0.001 and retrain 40 epochs for CIFAR-10\nand 20 epochs for ImageNet, which represents one-fourth of the original training epochs. Past work\nhas reported up to 3× original training times to retrain pruned networks (Han et al. (2015)).\nTable 1: Overall results. The best test/validation accuracy during the retraining process is reported.\nTraining a pruned model from scratch performs worse than retraining a pruned model, which may\nindicate the difﬁculty of training a network with a small capacity.\nModel\nError(%)\nFLOP\nPruned %\nParameters",
            "indicate the difﬁculty of training a network with a small capacity.\nModel\nError(%)\nFLOP\nPruned %\nParameters\nPruned %\nVGG-16\n6.75\n3.13 × 108\n1.5 × 107\nVGG-16-pruned-A\n6.60\n2.06 × 108\n34.2%\n5.4 × 106\n64.0%\nVGG-16-pruned-A scratch-train\n6.88\nResNet-56\n6.96\n1.25 × 108\n8.5 × 105\nResNet-56-pruned-A\n6.90\n1.12 × 108\n10.4%\n7.7 × 105\n9.4%\nResNet-56-pruned-B\n6.94\n9.09 × 107\n27.6%\n7.3 × 105\n13.7%\nResNet-56-pruned-B scratch-train\n8.69\nResNet-110\n6.47\n2.53 × 108\n1.72 × 106\nResNet-110-pruned-A\n6.45\n2.13 × 108\n15.9%\n1.68 × 106\n2.3%\nResNet-110-pruned-B\n6.70\n1.55 × 108\n38.6%\n1.16 × 106\n32.4%\nResNet-110-pruned-B scratch-train\n7.06\nResNet-34\n26.77\n3.64 × 109\n2.16 × 107\nResNet-34-pruned-A\n27.44\n3.08 × 109\n15.5%\n1.99 × 107\n7.6%\nResNet-34-pruned-B\n27.83\n2.76 × 109\n24.2%\n1.93 × 107\n10.8%\nResNet-34-pruned-C\n27.52\n3.37 × 109\n7.5%\n2.01 × 107\n7.2%\n4.1\nVGG-16 ON CIFAR-10\nVGG-16 is a high-capacity network originally designed for the ImageNet dataset (Simonyan &",
            "24.2%\n1.93 × 107\n10.8%\nResNet-34-pruned-C\n27.52\n3.37 × 109\n7.5%\n2.01 × 107\n7.2%\n4.1\nVGG-16 ON CIFAR-10\nVGG-16 is a high-capacity network originally designed for the ImageNet dataset (Simonyan &\nZisserman (2015)). Recently, Zagoruyko (2015) applies a slightly modiﬁed version of the model\non CIFAR-10 and achieves state of the art results. As shown in Table 2, VGG-16 on CIFAR-10\nconsists of 13 convolutional layers and 2 fully connected layers, in which the fully connected layers\ndo not occupy large portions of parameters due to the small input size and less hidden units. We use\nthe model described in Zagoruyko (2015) but add Batch Normalization (Ioffe & Szegedy (2015))\n6",
            "Published as a conference paper at ICLR 2017\nTable 2: VGG-16 on CIFAR-10 and the pruned model. The last two columns show the number of\nfeature maps and the reduced percentage of FLOP from the pruned model.\nlayer type\nwi × hi\n#Maps\nFLOP\n#Params\n#Maps\nFLOP%\nConv 1\n32 × 32\n64\n1.8E+06\n1.7E+03\n32\n50%\nConv 2\n32 × 32\n64\n3.8E+07\n3.7E+04\n64\n50%\nConv 3\n16 × 16\n128\n1.9E+07\n7.4E+04\n128\n0%\nConv 4\n16 × 16\n128\n3.8E+07\n1.5E+05\n128\n0%\nConv 5\n8 × 8\n256\n1.9E+07\n2.9E+05\n256\n0%\nConv 6\n8 × 8\n256\n3.8E+07\n5.9E+05\n256\n0%\nConv 7\n8 × 8\n256\n3.8E+07\n5.9E+05\n256\n0%\nConv 8\n4 × 4\n512\n1.9E+07\n1.2E+06\n256\n50%\nConv 9\n4 × 4\n512\n3.8E+07\n2.4E+06\n256\n75%\nConv 10\n4 × 4\n512\n3.8E+07\n2.4E+06\n256\n75%\nConv 11\n2 × 2\n512\n9.4E+06\n2.4E+06\n256\n75%\nConv 12\n2 × 2\n512\n9.4E+06\n2.4E+06\n256\n75%\nConv 13\n2 × 2\n512\n9.4E+06\n2.4E+06\n256\n75%\nLinear\n1\n512\n2.6E+05\n2.6E+05\n512\n50%\nLinear\n1\n10\n5.1E+03\n5.1E+03\n10\n0%\nTotal\n3.1E+08\n1.5E+07\n34%\nlayer after each convolutional layer and the ﬁrst linear layer, without using Dropout (Srivastava et al.",
            "1\n512\n2.6E+05\n2.6E+05\n512\n50%\nLinear\n1\n10\n5.1E+03\n5.1E+03\n10\n0%\nTotal\n3.1E+08\n1.5E+07\n34%\nlayer after each convolutional layer and the ﬁrst linear layer, without using Dropout (Srivastava et al.\n(2014)). Note that when the last convolutional layer is pruned, the input to the linear layer is changed\nand the connections are also removed.\nAs shown in Figure 2(b), each of the convolutional layers with 512 feature maps can drop at least\n60% of ﬁlters without affecting the accuracy. Figure 2(c) shows that with retraining, almost 90%\nof the ﬁlters of these layers can be safely removed. One possible explanation is that these ﬁlters\noperate on 4 × 4 or 2 × 2 feature maps, which may have no meaningful spatial connections in such\nsmall dimensions. For instance, ResNets for CIFAR-10 do not perform any convolutions for feature\nmaps below 8 × 8 dimensions. Unlike previous work (Zeiler & Fergus (2014); Han et al. (2015)), we",
            "small dimensions. For instance, ResNets for CIFAR-10 do not perform any convolutions for feature\nmaps below 8 × 8 dimensions. Unlike previous work (Zeiler & Fergus (2014); Han et al. (2015)), we\nobserve that the ﬁrst layer is robust to pruning as compared to the next few layers. This is possible\nfor a simple dataset like CIFAR-10, on which the model does not learn as much useful ﬁlters as on\nImageNet (as shown in Figure. 5). Even when 80% of the ﬁlters from the ﬁrst layer are pruned, the\nnumber of remaining ﬁlters (12) is still larger than the number of raw input channels. However, when\nremoving 80% ﬁlters from the second layer, the layer corresponds to a 64 to 12 mapping, which\nmay lose signiﬁcant information from previous layers, thereby hurting the accuracy. With 50% of\nthe ﬁlters being pruned in layer 1 and from 8 to 13, we achieve 34% FLOP reduction for the same\naccuracy.\nFigure 5: Visualization of ﬁlters in the ﬁrst convolutional layer of VGG-16 trained on CIFAR-10.",
            "accuracy.\nFigure 5: Visualization of ﬁlters in the ﬁrst convolutional layer of VGG-16 trained on CIFAR-10.\nFilters are ranked by ℓ1-norm.\n4.2\nRESNET-56/110 ON CIFAR-10\nResNets for CIFAR-10 have three stages of residual blocks for feature maps with sizes of 32 × 32,\n16 × 16 and 8 × 8. Each stage has the same number of residual blocks. When the number of feature\nmaps increases, the shortcut layer provides an identity mapping with an additional zero padding for\nthe increased dimensions. Since there is no projection mapping for choosing the identity feature\nmaps, we only consider pruning the ﬁrst layer of the residual block. As shown in Figure 6, most of\nthe layers are robust to pruning. For ResNet-110, pruning some single layers without retraining even\n7",
            "Published as a conference paper at ICLR 2017\nFigure 6: Sensitivity to pruning for the ﬁrst layer of each residual block of ResNet-56/110.\nimproves the performance. In addition, we ﬁnd that layers that are sensitive to pruning (layers 20,\n38 and 54 for ResNet-56, layer 36, 38 and 74 for ResNet-110) lie at the residual blocks close to the\nlayers where the number of feature maps changes, e.g., the ﬁrst and the last residual blocks for each\nstage. We believe this happens because the precise residual errors are necessary for the newly added\nempty feature maps.\nThe retraining performance can be improved by skipping these sensitive layers. As shown in Table 1,\nResNet-56-pruned-A improves the performance by pruning 10% ﬁlters while skipping the sensitive\nlayers 16, 20, 38 and 54. In addition, we ﬁnd that deeper layers are more sensitive to pruning than\nlayers in the earlier stages of the network. Hence, we use a different pruning rate for each stage. We",
            "layers in the earlier stages of the network. Hence, we use a different pruning rate for each stage. We\nuse pi to denote the pruning rate for layers in the ith stage. ResNet-56-pruned-B skips more layers (16,\n18, 20, 34, 38, 54) and prunes layers with p1=60%, p2=30% and p3=10%. For ResNet-110, the ﬁrst\npruned model gets a slightly better result with p1=50% and layer 36 skipped. ResNet-110-pruned-B\nskips layers 36, 38, 74 and prunes with p1=50%, p2=40% and p3=30%. When there are more than\ntwo residual blocks at each stage, the middle residual blocks may be redundant and can be easily\npruned. This might explain why ResNet-110 is easier to prune than ResNet-56.\n4.3\nRESNET-34 ON ILSVRC2012\nResNets for ImageNet have four stages of residual blocks for feature maps with sizes of 56 × 56,\n28 × 28, 14 × 14 and 7 × 7. ResNet-34 uses the projection shortcut when the feature maps are\ndown-sampled. We ﬁrst prune the ﬁrst layer of each residual block. Figure 7 shows the sensitivity of",
            "28 × 28, 14 × 14 and 7 × 7. ResNet-34 uses the projection shortcut when the feature maps are\ndown-sampled. We ﬁrst prune the ﬁrst layer of each residual block. Figure 7 shows the sensitivity of\nthe ﬁrst layer of each residual block. Similar to ResNet-56/110, the ﬁrst and the last residual blocks\nof each stage are more sensitive to pruning than the intermediate blocks (i.e., layers 2, 8, 14, 16, 26,\n28, 30, 32). We skip those layers and prune the remaining layers at each stage equally. In Table 1 we\ncompare two conﬁgurations of pruning percentages for the ﬁrst three stages: (A) p1=30%, p2=30%,\np3=30%; (B) p1=50%, p2=60%, p3=40%. Option-B provides 24% FLOP reduction with about 1%\nloss in accuracy. As seen in the pruning results for ResNet-50/110, we can predict that ResNet-34 is\nrelatively more difﬁcult to prune as compared to deeper ResNets.\nWe also prune the identity shortcuts and the second convolutional layer of the residual blocks. As",
            "relatively more difﬁcult to prune as compared to deeper ResNets.\nWe also prune the identity shortcuts and the second convolutional layer of the residual blocks. As\nthese layers have the same number of ﬁlters, they are pruned equally. As shown in Figure 7(b),\nthese layers are more sensitive to pruning than the ﬁrst layers. With retraining, ResNet-34-pruned-C\nprunes the third stage with p3=20% and results in 7.5% FLOP reduction with 0.75% loss in accuracy.\nTherefore, pruning the ﬁrst layer of the residual block is more effective at reducing the overall FLOP\n8",
            "Published as a conference paper at ICLR 2017\n(a) Pruning the ﬁrst layer of residual blocks\n(b) Pruning the second layer of residual blocks\nFigure 7: Sensitivity to pruning for the residual blocks of ResNet-34.\nthan pruning the second layer. This ﬁnding also correlates with the bottleneck block design for deeper\nResNets, which ﬁrst reduces the dimension of input feature maps for the residual layer and then\nincreases the dimension to match the identity mapping.\n4.4\nCOMPARISON WITH PRUNING RANDOM FILTERS AND LARGEST FILTERS\nWe compare our approach with pruning random ﬁlters and largest ﬁlters. As shown in Figure 8,\npruning the smallest ﬁlters outperforms pruning random ﬁlters for most of the layers at different\npruning ratios. For example, smallest ﬁlter pruning has better accuracy than random ﬁlter pruning for\nall layers with the pruning ratio of 90%. The accuracy of pruning ﬁlters with the largest ℓ1-norms",
            "all layers with the pruning ratio of 90%. The accuracy of pruning ﬁlters with the largest ℓ1-norms\ndrops quickly as the pruning ratio increases, which indicates the importance of ﬁlters with larger\nℓ1-norms.\nFigure 8: Comparison of three pruning methods for VGG-16 on CIFAR-10: pruning the smallest\nﬁlters, pruning random ﬁlters and pruning the largest ﬁlters. In random ﬁlter pruning, the order of\nﬁlters to be pruned is randomly permuted.\n4.5\nCOMPARISON WITH ACTIVATION-BASED FEATURE MAP PRUNING\nThe activation-based feature map pruning method removes the feature maps with weak activation\npatterns and their corresponding ﬁlters and kernels (Polyak & Wolf (2015)), which needs sample\ndata as input to determine which feature maps to prune. A feature map xi+1,j ∈Rwi+1×hi+1 is\ngenerated by applying ﬁlter Fi,j ∈Rni×k×k to feature maps of previous layer xi ∈Rni×wi×hi, i.e.,\nxi+1,j = Fi,j ∗xi. Given N randomly selected images {xn\n1}N\nn=1 from the training set, the statistics",
            "generated by applying ﬁlter Fi,j ∈Rni×k×k to feature maps of previous layer xi ∈Rni×wi×hi, i.e.,\nxi+1,j = Fi,j ∗xi. Given N randomly selected images {xn\n1}N\nn=1 from the training set, the statistics\nof each feature map can be estimated with one epoch forward pass of the N sampled data. Note that\nwe calculate statistics on the feature maps generated from the convolution operations before batch\nnormalization or non-linear activation. We compare our ℓ1-norm based ﬁlter pruning with feature map\npruning using the following criteria: σmean-mean(xi,j) =\n1\nN\nPN\nn=1 mean(xn\ni,j), σmean-std(xi,j) =\n1\nN\nPN\nn=1 std(xn\ni,j), σmean-ℓ1(xi,j) =\n1\nN\nPN\nn=1 ∥xn\ni,j∥1, σmean-ℓ2(xi,j) =\n1\nN\nPN\nn=1 ∥xn\ni,j∥2 and\n9",
            "Published as a conference paper at ICLR 2017\n(a) ∥Fi,j∥1\n(b) σmean-mean\n(c) σmean-std\n(d) σmean-ℓ1\n(e) σmean-ℓ2\n(f) σvar-ℓ2\nFigure 9: Comparison of activation-based feature map pruning for VGG-16 on CIFAR-10.\nσvar-ℓ2(xi,j) = var({∥xn\ni,j∥2}N\nn=1), where mean, std and var are standard statistics (average,\nstandard deviation and variance) of the input. Here, σvar-ℓ2 is the contribution variance of channel\ncriterion proposed in Polyak & Wolf (2015), which is motivated by the intuition that an unimportant\nfeature map has almost similar outputs for the whole training data and acts like an additional bias.\nThe estimation of the criteria becomes more accurate when more sample data is used. Here we use\nthe whole training set (N = 50, 000 for CIFAR-10) to compute the statistics. The performance of\nfeature map pruning with above criteria for each layer is shown in Figure 9. Smallest ﬁlter pruning\noutperforms feature map pruning with the criteria σmean-mean, σmean-ℓ1, σmean-ℓ2 and σvar-ℓ2. The",
            "feature map pruning with above criteria for each layer is shown in Figure 9. Smallest ﬁlter pruning\noutperforms feature map pruning with the criteria σmean-mean, σmean-ℓ1, σmean-ℓ2 and σvar-ℓ2. The\nσmean-std criterion has better or similar performance to ℓ1-norm up to pruning ratio of 60%. However,\nits performance drops quickly after that especially for layers of conv 1, conv 2 and conv 3. We\nﬁnd ℓ1-norm is a good heuristic for ﬁlter selection considering that it is data free.\n5\nCONCLUSIONS\nModern CNNs often have high capacity with large training and inference costs. In this paper we\npresent a method to prune ﬁlters with relatively low weight magnitudes to produce CNNs with\nreduced computation costs without introducing irregular sparsity. It achieves about 30% reduction in\nFLOP for VGGNet (on CIFAR-10) and deep ResNets without signiﬁcant loss in the original accuracy.\nInstead of pruning with speciﬁc layer-wise hayperparameters and time-consuming iterative retraining,",
            "FLOP for VGGNet (on CIFAR-10) and deep ResNets without signiﬁcant loss in the original accuracy.\nInstead of pruning with speciﬁc layer-wise hayperparameters and time-consuming iterative retraining,\nwe use the one-shot pruning and retraining strategy for simplicity and ease of implementation. By\nperforming lesion studies on very deep CNNs, we identify layers that are robust or sensitive to\npruning, which can be useful for further understanding and improving the architectures.\nACKNOWLEDGMENTS\nThe authors would like to thank the anonymous reviewers for their valuable feedback.\nREFERENCES\nSajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured Pruning of Deep Convolutional\nNeural Networks. arXiv preprint arXiv:1512.08571, 2015.\n10",
            "Published as a conference paper at ICLR 2017\nRonan Collobert, Koray Kavukcuoglu, and Cl´ement Farabet. Torch7: A matlab-like environment for\nmachine learning. In BigLearn, NIPS Workshop, 2011.\nMatthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights\nand activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830, 2016.\nMisha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al. Predicting parameters in deep\nlearning. In NIPS, 2013.\nSong Han, Jeff Pool, John Tran, and William Dally. Learning both Weights and Connections for\nEfﬁcient Neural Network. In NIPS, 2015.\nSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J\nDally. EIE: Efﬁcient Inference Engine on Compressed Deep Neural Network. In ISCA, 2016a.\nSong Han, Huizi Mao, and William J Dally. Deep Compression: Compressing Deep Neural Networks\nwith Pruning, Trained Quantization and Huffman Coding. In ICLR, 2016b.",
            "Song Han, Huizi Mao, and William J Dally. Deep Compression: Compressing Deep Neural Networks\nwith Pruning, Trained Quantization and Huffman Coding. In ICLR, 2016b.\nBabak Hassibi and David G Stork. Second Order Derivatives for Network Pruning: Optimal Brain\nSurgeon. In NIPS, 1993.\nKaiming He and Jian Sun. Convolutional Neural Networks at Constrained Time Cost. In CVPR,\n2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\nRecognition. In CVPR, 2016.\nForrest Iandola, Matthew Moskewicz, Khalidand Ashraf, Song Han, William Dally, and Keutzer Kurt.\nSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and ¡ 1MB model size. arXiv\npreprint arXiv:1602.07360, 2016.\nYani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla, and Antonio Criminisi. Training\nCNNs with Low-Rank Filters for Efﬁcient Image Classiﬁcation. In ICLR, 2016.\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by",
            "CNNs with Low-Rank Filters for Efﬁcient Image Classiﬁcation. In ICLR, 2016.\nSergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. 2015.\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\nwith low rank expansions. In BMVC, 2014.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet Classiﬁcation with Deep Convo-\nlutional Neural Networks. In NIPS, 2012.\nAndrew Lavin and Scott Gray. Fast Algorithms for Convolutional Neural Networks. In CVPR, 2016.\nYann Le Cun, John S Denker, and Sara A Solla. Optimal Brain Damage. In NIPS, 1989.\nVadim Lebedev and Victor Lempitsky. Fast Convnets Using Group-wise Brain Damage. In CVPR,\n2016.\nMin Lin, Qiang Chen, and Shuicheng Yan. Network in Network. arXiv preprint arXiv:1312.4400,\n2013.\nBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse Convolu-\ntional Neural Networks. In CVPR, 2015.",
            "2013.\nBaoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse Convolu-\ntional Neural Networks. In CVPR, 2015.\nZelda Mariet and Suvrit Sra. Diversity Networks. In ICLR, 2016.\nMichael Mathieu, Mikael Henaff, and Yann LeCun. Fast Training of Convolutional Networks through\nFFTs. arXiv preprint arXiv:1312.5851, 2013.\nAdam Polyak and Lior Wolf. Channel-Level Acceleration of Deep Face Representations. IEEE\nAccess, 2015.\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet\nClassiﬁcation Using Binary Convolutional Neural Networks. In ECCV, 2016.\n11",
            "Published as a conference paper at ICLR 2017\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet\nLarge Scale Visual Recognition Challenge. IJCV, 2015.\nKaren Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image\nRecognition. In ICLR, 2015.\nSuraj Srinivas and R Venkatesh Babu. Data-free Parameter Pruning for Deep Neural Networks. In\nBMVC, 2015.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A Simple Way to Prevent Neural Networks from Overﬁtting. JMLR, 2014.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. In CVPR,\n2015a.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethink-",
            "Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going Deeper with Convolutions. In CVPR,\n2015a.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethink-\ning the Inception Architecture for Computer Vision. arXiv preprint arXiv:1512.00567, 2015b.\nCheng Tai, Tong Xiao, Xiaogang Wang, and Weinan E. Convolutional neural networks with low-rank\nregularization. In ICLR, 2016.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning Structured Sparsity in\nDeep Learning. In NIPS, 2016.\nSergey Zagoruyko. 92.45% on CIFAR-10 in Torch. http://torch.ch/blog/2015/07/30/\ncifar.html, 2015.\nMatthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In\nECCV, 2014.\nXiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating Very Deep Convolutional\nNetworks for Classiﬁcation and Detection. IEEE T-PAMI, 2015a.\nXiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun. Efﬁcient and accurate",
            "Networks for Classiﬁcation and Detection. IEEE T-PAMI, 2015a.\nXiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, and Jian Sun. Efﬁcient and accurate\napproximations of nonlinear convolutional networks. In CVPR, 2015b.\nHao Zhou, Jose Alvarez, and Fatih Porikli. Less Is More: Towards Compact CNNs. In ECCV, 2016.\n12",
            "Published as a conference paper at ICLR 2017\n6\nAPPENDIX\n6.1\nCOMPARISON WITH ℓ2-NORM BASED FILTER PRUNING\nWe compare ℓ1-norm with ℓ2-norm for ﬁlter pruning. As shown in Figure 10, ℓ1-norm works slightly\nbetter than ℓ2-norm for layer conv 2. There is no signiﬁcant difference between the two norms for\nother layers.\n(a) ∥Fi,j∥1\n(b) ∥Fi,j∥2\nFigure 10: Comparison of ℓ1-norm and ℓ2-norm based ﬁlter pruning for VGG-16 on CIFAR-10.\n6.2\nFLOP AND WALL-CLOCK TIME\nFLOP is a commonly used measure to compare the computation complexities of CNNs. It is easy to\ncompute and can be done statically, which is independent of the underlying hardware and software\nimplementations. Since we physically prune the ﬁlters by creating a smaller model and then copy the\nweights, there are no masks or sparsity introduced to the original dense BLAS operations. Therefore\nthe FLOP and wall-clock time of the pruned model is the same as creating a model with smaller\nnumber of ﬁlters from scratch.",
            "the FLOP and wall-clock time of the pruned model is the same as creating a model with smaller\nnumber of ﬁlters from scratch.\nWe report the inference time of the original model and the pruned model on the test set of CIFAR-10\nand the validation set of ILSVRC 2012, which contains 10,000 32 × 32 images and 50,000 224 × 224\nimages respectively. The ILSVRC 2012 dataset is used only for ResNet-34. The evaluation is\nconducted in Torch7 with Titan X (Pascal) GPU and cuDNN v5.1, using a mini-batch size 128. As\nshown in Table 3, the saved inference time is close to the FLOP reduction. Note that the FLOP\nnumber only considers the operations in the Conv and FC layers, while some calculations such as\nBatch Normalization and other overheads are not accounted.\nTable 3: The reduction of FLOP and wall-clock time for inference.\nModel\nFLOP\nPruned %\nTime (s)\nSaved %\nVGG-16\n3.13 × 108\n1.23\nVGG-16-pruned-A\n2.06 × 108\n34.2%\n0.73\n40.7%\nResNet-56\n1.25 × 108\n1.31\nResNet-56-pruned-B\n9.09 × 107\n27.6%\n0.99\n24.4%",
            "Model\nFLOP\nPruned %\nTime (s)\nSaved %\nVGG-16\n3.13 × 108\n1.23\nVGG-16-pruned-A\n2.06 × 108\n34.2%\n0.73\n40.7%\nResNet-56\n1.25 × 108\n1.31\nResNet-56-pruned-B\n9.09 × 107\n27.6%\n0.99\n24.4%\nResNet-110\n2.53 × 108\n2.38\nResNet-110-pruned-B\n1.55 × 108\n38.6%\n1.86\n21.8%\nResNet-34\n3.64 × 109\n36.02\nResNet-34-pruned-B\n2.76 × 109\n24.2%\n22.93\n28.0%\n13"
        ],
        "metadatas": [
            {
                "page": 1,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 1,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 1,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 1,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 1,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 2,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 3,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 3,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 3,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 3,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 3,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 4,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 4,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 4,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 4,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 4,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 5,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 5,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 5,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 5,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 6,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 6,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 6,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 6,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 6,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 7,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 7,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 7,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 7,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 8,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 8,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 8,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 8,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 9,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 9,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 9,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 10,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 10,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 10,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 11,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 11,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 11,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 11,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 12,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 12,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 12,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 13,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 13,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            },
            {
                "page": 13,
                "source": "PRUNING FILTERS FOR EFFICIENT CONVNETS.pdf",
                "title": "Published as a conference paper at ICLR 2017",
                "authors": ""
            }
        ],
        "ids": [
            "chunk_0",
            "chunk_1",
            "chunk_2",
            "chunk_3",
            "chunk_4",
            "chunk_5",
            "chunk_6",
            "chunk_7",
            "chunk_8",
            "chunk_9",
            "chunk_10",
            "chunk_11",
            "chunk_12",
            "chunk_13",
            "chunk_14",
            "chunk_15",
            "chunk_16",
            "chunk_17",
            "chunk_18",
            "chunk_19",
            "chunk_20",
            "chunk_21",
            "chunk_22",
            "chunk_23",
            "chunk_24",
            "chunk_25",
            "chunk_26",
            "chunk_27",
            "chunk_28",
            "chunk_29",
            "chunk_30",
            "chunk_31",
            "chunk_32",
            "chunk_33",
            "chunk_34",
            "chunk_35",
            "chunk_36",
            "chunk_37",
            "chunk_38",
            "chunk_39",
            "chunk_40",
            "chunk_41",
            "chunk_42",
            "chunk_43",
            "chunk_44",
            "chunk_45",
            "chunk_46",
            "chunk_47",
            "chunk_48",
            "chunk_49",
            "chunk_50",
            "chunk_51",
            "chunk_52",
            "chunk_53"
        ]
    }
}